#!/usr/bin/env python3
"""
å¼‚æ­¥æ•°æ®ç®¡é“ä¼˜åŒ–æ¨¡å—
é€šè¿‡å¼‚æ­¥æ•°æ®æµæ°´çº¿æ¶æ„å®ç°é«˜æ•ˆçš„æ•°æ®è·å–ã€å¤„ç†å’Œåˆ†æ

æ ¸å¿ƒç‰¹æ€§:
1. å¼‚æ­¥æ•°æ®è·å– - å¹¶å‘è·å–å¤šæºæ•°æ®
2. æµå¼å¤„ç† - æ•°æ®æµå¼å¤„ç†å‡å°‘å†…å­˜å ç”¨
3. ç®¡é“ä¼˜åŒ– - å¤šçº§ç®¡é“å¹¶è¡Œå¤„ç†
4. æ™ºèƒ½è°ƒåº¦ - åŠ¨æ€ä»»åŠ¡è°ƒåº¦å’Œè´Ÿè½½å‡è¡¡
"""

import asyncio
import aiohttp
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Any, Tuple, Union, AsyncIterator, Callable
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from enum import Enum
import time
import logging
from asyncio import Queue, Semaphore
from collections import defaultdict
import functools
import json

from ..utils.logging_manager import get_logger

# å¯¼å…¥é›ªçƒæ•°æ®æä¾›å™¨
try:
    from .xueqiu_utils import get_xueqiu_provider
    XUEQIU_AVAILABLE = True
except ImportError:
    XUEQIU_AVAILABLE = False

logger = get_logger('async_data_pipeline')


class PipelineStage(Enum):
    """ç®¡é“é˜¶æ®µæšä¸¾"""
    DATA_FETCH = "data_fetch"          # æ•°æ®è·å–
    DATA_CLEAN = "data_clean"          # æ•°æ®æ¸…æ´—
    DATA_ENRICH = "data_enrich"        # æ•°æ®å¢å¼º
    DATA_ANALYZE = "data_analyze"      # æ•°æ®åˆ†æ
    DATA_AGGREGATE = "data_aggregate"  # æ•°æ®èšåˆ


@dataclass
class PipelineConfig:
    """ç®¡é“é…ç½®"""
    max_concurrent_tasks: int = 50          # æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°
    batch_size: int = 100                   # æ‰¹å¤„ç†å¤§å°
    queue_size: int = 1000                  # é˜Ÿåˆ—å¤§å°
    timeout_seconds: float = 30.0           # è¶…æ—¶æ—¶é—´
    retry_count: int = 3                    # é‡è¯•æ¬¡æ•°
    retry_delay: float = 1.0                # é‡è¯•å»¶è¿Ÿ
    enable_streaming: bool = True           # å¯ç”¨æµå¼å¤„ç†
    enable_caching: bool = True             # å¯ç”¨ç¼“å­˜
    cache_ttl: int = 3600                   # ç¼“å­˜TTL
    enable_metrics: bool = True             # å¯ç”¨åº¦é‡
    checkpoint_interval: int = 100          # æ£€æŸ¥ç‚¹é—´éš”


@dataclass
class DataPacket:
    """æ•°æ®åŒ…"""
    id: str                                 # æ•°æ®åŒ…ID
    symbol: str                             # è‚¡ç¥¨ä»£ç 
    stage: PipelineStage                   # å½“å‰é˜¶æ®µ
    data: Dict[str, Any]                   # æ•°æ®å†…å®¹
    metadata: Dict[str, Any] = field(default_factory=dict)  # å…ƒæ•°æ®
    timestamp: datetime = field(default_factory=datetime.now)  # æ—¶é—´æˆ³
    retry_count: int = 0                   # é‡è¯•æ¬¡æ•°
    error: Optional[str] = None            # é”™è¯¯ä¿¡æ¯


@dataclass
class PipelineMetrics:
    """ç®¡é“åº¦é‡æŒ‡æ ‡"""
    total_packets: int = 0                  # æ€»æ•°æ®åŒ…æ•°
    processed_packets: int = 0              # å·²å¤„ç†æ•°æ®åŒ…æ•°
    failed_packets: int = 0                 # å¤±è´¥æ•°æ®åŒ…æ•°
    total_time: float = 0.0                 # æ€»è€—æ—¶
    stage_times: Dict[str, float] = field(default_factory=dict)  # å„é˜¶æ®µè€—æ—¶
    throughput: float = 0.0                 # ååé‡
    error_rate: float = 0.0                 # é”™è¯¯ç‡
    queue_sizes: Dict[str, int] = field(default_factory=dict)  # é˜Ÿåˆ—å¤§å°


class AsyncDataPipeline:
    """å¼‚æ­¥æ•°æ®ç®¡é“"""
    
    def __init__(self, config: PipelineConfig = None):
        """
        åˆå§‹åŒ–å¼‚æ­¥æ•°æ®ç®¡é“
        
        Args:
            config: ç®¡é“é…ç½®
        """
        self.config = config or PipelineConfig()
        self.data_manager = None  # å»¶è¿Ÿåˆå§‹åŒ–ä»¥é¿å…å¾ªç¯å¯¼å…¥
        
        # åˆå§‹åŒ–é›ªçƒæ•°æ®æä¾›å™¨
        self.xueqiu_provider = None
        if XUEQIU_AVAILABLE:
            try:
                self.xueqiu_provider = get_xueqiu_provider()
                logger.info("âœ… å¼‚æ­¥ç®¡é“ï¼šé›ªçƒæ•°æ®æä¾›å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"âš ï¸ å¼‚æ­¥ç®¡é“ï¼šé›ªçƒæ•°æ®æä¾›å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # åˆ›å»ºé˜¶æ®µé˜Ÿåˆ—
        self.queues = {
            PipelineStage.DATA_FETCH: Queue(maxsize=self.config.queue_size),
            PipelineStage.DATA_CLEAN: Queue(maxsize=self.config.queue_size),
            PipelineStage.DATA_ENRICH: Queue(maxsize=self.config.queue_size),
            PipelineStage.DATA_ANALYZE: Queue(maxsize=self.config.queue_size),
            PipelineStage.DATA_AGGREGATE: Queue(maxsize=self.config.queue_size)
        }
        
        # ä¿¡å·é‡æ§åˆ¶å¹¶å‘
        self.semaphore = Semaphore(self.config.max_concurrent_tasks)
        
        # ç¼“å­˜
        self._cache = {}
        self._cache_timestamps = {}
        
        # åº¦é‡æŒ‡æ ‡
        self.metrics = PipelineMetrics()
        self._stage_timers = defaultdict(list)
        
        # å¤„ç†å™¨æ³¨å†Œè¡¨
        self.processors = {}
        self._register_default_processors()
        
        # HTTPä¼šè¯
        self.session = None
        
        # è¿è¡ŒçŠ¶æ€
        self.is_running = False
        self.workers = []
        
        logger.info("ğŸš€ å¼‚æ­¥æ•°æ®ç®¡é“åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“Š é…ç½®: æœ€å¤§å¹¶å‘={self.config.max_concurrent_tasks}, æ‰¹æ¬¡å¤§å°={self.config.batch_size}")
    
    async def start(self):
        """å¯åŠ¨ç®¡é“"""
        if self.is_running:
            logger.warning("âš ï¸ ç®¡é“å·²åœ¨è¿è¡Œä¸­")
            return
        
        self.is_running = True
        
        # åˆ›å»ºHTTPä¼šè¯
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=self.config.timeout_seconds)
        )
        
        # å¯åŠ¨å·¥ä½œå™¨
        self.workers = [
            asyncio.create_task(self._stage_worker(stage))
            for stage in PipelineStage
        ]
        
        # å¯åŠ¨åº¦é‡æ”¶é›†å™¨
        self.workers.append(
            asyncio.create_task(self._metrics_collector())
        )
        
        logger.info("âœ… å¼‚æ­¥æ•°æ®ç®¡é“å·²å¯åŠ¨")
    
    async def stop(self):
        """åœæ­¢ç®¡é“"""
        if not self.is_running:
            return
        
        self.is_running = False
        
        # ç­‰å¾…é˜Ÿåˆ—æ¸…ç©º
        for queue in self.queues.values():
            await queue.join()
        
        # å–æ¶ˆå·¥ä½œå™¨
        for worker in self.workers:
            worker.cancel()
        
        # å…³é—­HTTPä¼šè¯
        if self.session:
            await self.session.close()
        
        logger.info("ğŸ›‘ å¼‚æ­¥æ•°æ®ç®¡é“å·²åœæ­¢")
    
    async def process_symbols(self, symbols: List[str]) -> Dict[str, Any]:
        """
        å¤„ç†è‚¡ç¥¨åˆ—è¡¨
        
        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            
        Returns:
            å¤„ç†ç»“æœ
        """
        start_time = time.time()
        
        logger.info(f"ğŸ¯ å¼€å§‹å¤„ç† {len(symbols)} åªè‚¡ç¥¨")
        
        # å¯åŠ¨ç®¡é“
        await self.start()
        
        try:
            # åˆ›å»ºæ•°æ®åŒ…å¹¶åŠ å…¥é˜Ÿåˆ—
            packets = []
            for symbol in symbols:
                packet = DataPacket(
                    id=f"{symbol}_{int(time.time()*1000)}",
                    symbol=symbol,
                    stage=PipelineStage.DATA_FETCH,
                    data={'symbol': symbol}
                )
                packets.append(packet)
                await self.queues[PipelineStage.DATA_FETCH].put(packet)
            
            # æ”¶é›†ç»“æœ
            results = {}
            result_queue = self.queues[PipelineStage.DATA_AGGREGATE]
            
            # ç­‰å¾…æ‰€æœ‰ç»“æœ
            processed_count = 0
            while processed_count < len(symbols):
                try:
                    # è®¾ç½®è¶…æ—¶é¿å…æ— é™ç­‰å¾…
                    packet = await asyncio.wait_for(
                        result_queue.get(), 
                        timeout=self.config.timeout_seconds
                    )
                    
                    if packet.error:
                        logger.warning(f"âš ï¸ {packet.symbol} å¤„ç†å¤±è´¥: {packet.error}")
                        results[packet.symbol] = {'error': packet.error}
                    else:
                        results[packet.symbol] = packet.data
                    
                    processed_count += 1
                    
                    # è¿›åº¦æŠ¥å‘Š
                    if processed_count % 50 == 0 or processed_count == len(symbols):
                        progress = processed_count / len(symbols) * 100
                        logger.info(f"ğŸ“ˆ å¤„ç†è¿›åº¦: {processed_count}/{len(symbols)} ({progress:.1f}%)")
                    
                except asyncio.TimeoutError:
                    logger.warning(f"âš ï¸ ç­‰å¾…ç»“æœè¶…æ—¶ï¼Œå·²å¤„ç† {processed_count}/{len(symbols)}")
                    break
            
            # è®¡ç®—åº¦é‡
            total_time = time.time() - start_time
            self.metrics.total_time = total_time
            self.metrics.throughput = processed_count / max(total_time, 1)
            self.metrics.error_rate = (len(symbols) - processed_count) / max(len(symbols), 1)
            
            logger.info(f"âœ… å¤„ç†å®Œæˆ: {processed_count}/{len(symbols)} æˆåŠŸ")
            logger.info(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f}ç§’")
            logger.info(f"ğŸš€ ååé‡: {self.metrics.throughput:.2f}è‚¡ç¥¨/ç§’")
            
            return {
                'results': results,
                'metrics': self._get_metrics_summary()
            }
            
        finally:
            # åœæ­¢ç®¡é“
            await self.stop()
    
    async def _stage_worker(self, stage: PipelineStage):
        """é˜¶æ®µå·¥ä½œå™¨"""
        input_queue = self.queues[stage]
        
        # ç¡®å®šä¸‹ä¸€é˜¶æ®µ
        stage_order = list(PipelineStage)
        current_index = stage_order.index(stage)
        next_stage = stage_order[current_index + 1] if current_index < len(stage_order) - 1 else None
        output_queue = self.queues[next_stage] if next_stage else None
        
        while self.is_running:
            try:
                # ä»è¾“å…¥é˜Ÿåˆ—è·å–æ•°æ®åŒ…
                packet = await asyncio.wait_for(
                    input_queue.get(), 
                    timeout=1.0
                )
                
                # å¤„ç†æ•°æ®åŒ…
                async with self.semaphore:
                    start_time = time.time()
                    
                    try:
                        # è·å–å¤„ç†å™¨
                        processor = self.processors.get(stage)
                        if processor:
                            # æ‰§è¡Œå¤„ç†
                            processed_packet = await processor(packet)
                            
                            # æ›´æ–°é˜¶æ®µ
                            if next_stage:
                                processed_packet.stage = next_stage
                            
                            # è®°å½•å¤„ç†æ—¶é—´
                            process_time = time.time() - start_time
                            self._stage_timers[stage.value].append(process_time)
                            
                            # å‘é€åˆ°ä¸‹ä¸€é˜¶æ®µæˆ–å®Œæˆ
                            if output_queue:
                                await output_queue.put(processed_packet)
                            
                            # æ›´æ–°åº¦é‡
                            self.metrics.processed_packets += 1
                            
                        else:
                            logger.warning(f"âš ï¸ æœªæ‰¾åˆ° {stage.value} é˜¶æ®µçš„å¤„ç†å™¨")
                            packet.error = f"æœªæ‰¾åˆ°å¤„ç†å™¨: {stage.value}"
                            self.metrics.failed_packets += 1
                            
                            # ç›´æ¥å‘é€åˆ°èšåˆé˜¶æ®µ
                            if stage != PipelineStage.DATA_AGGREGATE:
                                await self.queues[PipelineStage.DATA_AGGREGATE].put(packet)
                    
                    except Exception as e:
                        logger.error(f"âŒ å¤„ç† {packet.symbol} åœ¨ {stage.value} é˜¶æ®µå¤±è´¥: {e}")
                        packet.error = str(e)
                        packet.retry_count += 1
                        
                        # é‡è¯•æˆ–æ”¾å¼ƒ
                        if packet.retry_count < self.config.retry_count:
                            await asyncio.sleep(self.config.retry_delay * packet.retry_count)
                            await input_queue.put(packet)
                        else:
                            self.metrics.failed_packets += 1
                            # å‘é€åˆ°èšåˆé˜¶æ®µæ ‡è®°ä¸ºå¤±è´¥
                            if stage != PipelineStage.DATA_AGGREGATE:
                                await self.queues[PipelineStage.DATA_AGGREGATE].put(packet)
                
                # æ ‡è®°ä»»åŠ¡å®Œæˆ
                input_queue.task_done()
                
            except asyncio.TimeoutError:
                # è¶…æ—¶ç»§ç»­å¾ªç¯
                continue
            except asyncio.CancelledError:
                # ä»»åŠ¡è¢«å–æ¶ˆ
                break
            except Exception as e:
                logger.error(f"âŒ å·¥ä½œå™¨ {stage.value} å¼‚å¸¸: {e}")
                await asyncio.sleep(1)
    
    def _register_default_processors(self):
        """æ³¨å†Œé»˜è®¤å¤„ç†å™¨"""
        self.processors[PipelineStage.DATA_FETCH] = self._fetch_processor
        self.processors[PipelineStage.DATA_CLEAN] = self._clean_processor
        self.processors[PipelineStage.DATA_ENRICH] = self._enrich_processor
        self.processors[PipelineStage.DATA_ANALYZE] = self._analyze_processor
        self.processors[PipelineStage.DATA_AGGREGATE] = self._aggregate_processor
    
    async def _fetch_processor(self, packet: DataPacket) -> DataPacket:
        """æ•°æ®è·å–å¤„ç†å™¨"""
        symbol = packet.symbol
        
        # æ£€æŸ¥ç¼“å­˜
        if self.config.enable_caching:
            cached_data = self._get_cached_data(symbol)
            if cached_data:
                packet.data.update(cached_data)
                packet.metadata['from_cache'] = True
                return packet
        
        # å¼‚æ­¥è·å–å¤šæºæ•°æ®
        fetch_tasks = []
        
        # ä»·æ ¼æ•°æ®
        fetch_tasks.append(self._fetch_price_data(symbol))
        
        # åŸºç¡€æ•°æ®
        fetch_tasks.append(self._fetch_basic_data(symbol))
        
        # å¸‚åœºæ•°æ®
        fetch_tasks.append(self._fetch_market_data(symbol))
        
        # ç¤¾äº¤æ•°æ®ï¼ˆé›ªçƒï¼‰
        fetch_tasks.append(self._fetch_social_data(symbol))
        
        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*fetch_tasks, return_exceptions=True)
        
        # åˆå¹¶ç»“æœ
        merged_data = {}
        for result in results:
            if isinstance(result, dict):
                merged_data.update(result)
            elif isinstance(result, Exception):
                logger.debug(f"è·å–æ•°æ®éƒ¨åˆ†å¤±è´¥: {result}")
        
        packet.data.update(merged_data)
        
        # ç¼“å­˜æ•°æ®
        if self.config.enable_caching and merged_data:
            self._cache_data(symbol, merged_data)
        
        return packet
    
    async def _clean_processor(self, packet: DataPacket) -> DataPacket:
        """æ•°æ®æ¸…æ´—å¤„ç†å™¨"""
        data = packet.data
        
        # æ¸…æ´—è§„åˆ™
        cleaned_data = {}
        
        for key, value in data.items():
            # ç§»é™¤Noneå€¼
            if value is None:
                continue
            
            # å¤„ç†æ•°å€¼ç±»å‹
            if key in ['price', 'volume', 'market_cap', 'pe_ratio']:
                try:
                    if isinstance(value, str):
                        value = float(value.replace(',', ''))
                    cleaned_data[key] = value
                except:
                    continue
            
            # å¤„ç†æ—¥æœŸç±»å‹
            elif key in ['date', 'timestamp']:
                try:
                    if isinstance(value, str):
                        value = datetime.fromisoformat(value)
                    cleaned_data[key] = value
                except:
                    cleaned_data[key] = datetime.now()
            
            # å…¶ä»–ç±»å‹ç›´æ¥ä¿ç•™
            else:
                cleaned_data[key] = value
        
        # æ•°æ®éªŒè¯
        if 'price' in cleaned_data and cleaned_data['price'] <= 0:
            packet.error = "æ— æ•ˆçš„ä»·æ ¼æ•°æ®"
        
        packet.data = cleaned_data
        packet.metadata['cleaned'] = True
        
        return packet
    
    async def _enrich_processor(self, packet: DataPacket) -> DataPacket:
        """æ•°æ®å¢å¼ºå¤„ç†å™¨"""
        data = packet.data
        
        # è®¡ç®—è¡ç”ŸæŒ‡æ ‡
        if 'price' in data and 'volume' in data:
            data['turnover'] = data['price'] * data['volume']
        
        if 'high' in data and 'low' in data:
            data['amplitude'] = (data['high'] - data['low']) / data.get('price', 1) * 100
        
        if 'price' in data and 'prev_close' in data:
            data['change_pct'] = (data['price'] - data['prev_close']) / data['prev_close'] * 100
        
        # æ·»åŠ æŠ€æœ¯æŒ‡æ ‡
        if 'price' in data:
            # ç®€å•ç§»åŠ¨å¹³å‡
            data['ma5'] = data.get('ma5', data['price'])  # å®é™…åº”è¯¥ä»å†å²æ•°æ®è®¡ç®—
            data['ma20'] = data.get('ma20', data['price'])
            
            # RSI (ç®€åŒ–ç‰ˆ)
            data['rsi'] = 50.0  # å®é™…åº”è¯¥ä»å†å²æ•°æ®è®¡ç®—
        
        # æ·»åŠ åŸºç¡€è¯„åˆ†
        score = 50.0
        if data.get('change_pct', 0) > 0:
            score += 10
        if data.get('volume', 0) > data.get('avg_volume', 0):
            score += 10
        if data.get('pe_ratio', 100) < 30:
            score += 10
        
        # æ•´åˆç¤¾äº¤æ•°æ®è¯„åˆ†
        if 'social_sentiment_score' in data:
            sentiment = data['social_sentiment_score']
            if sentiment > 0.3:
                score += 15  # ç§¯ææƒ…ç»ªåŠ åˆ†
            elif sentiment < -0.3:
                score -= 10  # æ¶ˆææƒ…ç»ªå‡åˆ†
            
            # çƒ­åº¦åŠ æˆ
            heat = data.get('social_heat', 0)
            if heat > 80:
                score += 10  # é«˜çƒ­åº¦
            elif heat > 50:
                score += 5   # ä¸­ç­‰çƒ­åº¦
            
            # è¶‹åŠ¿åŠ æˆ
            trend = data.get('sentiment_trend', 'neutral')
            if trend == 'improving':
                score += 10
            elif trend == 'deteriorating':
                score -= 10
        
        data['enrich_score'] = min(100, max(0, score))
        
        packet.data = data
        packet.metadata['enriched'] = True
        
        return packet
    
    async def _analyze_processor(self, packet: DataPacket) -> DataPacket:
        """æ•°æ®åˆ†æå¤„ç†å™¨"""
        data = packet.data
        
        # åŸºç¡€åˆ†æ
        analysis = {
            'symbol': packet.symbol,
            'timestamp': datetime.now(),
            'price': data.get('price', 0),
            'volume': data.get('volume', 0),
            'change_pct': data.get('change_pct', 0),
            'enrich_score': data.get('enrich_score', 50)
        }
        
        # è¶‹åŠ¿åˆ†æ
        if data.get('price', 0) > data.get('ma20', 0):
            analysis['trend'] = 'UP'
        else:
            analysis['trend'] = 'DOWN'
        
        # æˆäº¤é‡åˆ†æ
        if data.get('volume', 0) > data.get('avg_volume', 0) * 1.5:
            analysis['volume_signal'] = 'HIGH'
        elif data.get('volume', 0) < data.get('avg_volume', 1) * 0.5:
            analysis['volume_signal'] = 'LOW'
        else:
            analysis['volume_signal'] = 'NORMAL'
        
        # ç»¼åˆè¯„åˆ†
        final_score = data.get('enrich_score', 50)
        if analysis['trend'] == 'UP':
            final_score += 10
        if analysis['volume_signal'] == 'HIGH':
            final_score += 5
        
        analysis['final_score'] = min(100, max(0, final_score))
        
        # æ¨è
        if analysis['final_score'] >= 70:
            analysis['recommendation'] = 'BUY'
        elif analysis['final_score'] <= 30:
            analysis['recommendation'] = 'SELL'
        else:
            analysis['recommendation'] = 'HOLD'
        
        packet.data = analysis
        packet.metadata['analyzed'] = True
        
        return packet
    
    async def _aggregate_processor(self, packet: DataPacket) -> DataPacket:
        """æ•°æ®èšåˆå¤„ç†å™¨"""
        # æœ€ç»ˆèšåˆé˜¶æ®µï¼Œç›´æ¥è¿”å›
        packet.metadata['completed'] = True
        packet.metadata['completion_time'] = datetime.now()
        
        # è®¡ç®—æ€»å¤„ç†æ—¶é—´
        if 'timestamp' in packet.metadata:
            total_time = (datetime.now() - packet.timestamp).total_seconds()
            packet.metadata['total_processing_time'] = total_time
        
        return packet
    
    async def _fetch_price_data(self, symbol: str) -> Dict[str, Any]:
        """å¼‚æ­¥è·å–ä»·æ ¼æ•°æ®"""
        try:
            # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„API
            # æ¨¡æ‹Ÿå¼‚æ­¥è·å–
            await asyncio.sleep(0.1)
            
            # å»¶è¿Ÿåˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨ä»¥é¿å…å¾ªç¯å¯¼å…¥
            if self.data_manager is None:
                try:
                    from ..dataflows.enhanced_data_manager import EnhancedDataManager
                    self.data_manager = EnhancedDataManager()
                except ImportError:
                    pass
            
            # ä½¿ç”¨æ•°æ®ç®¡ç†å™¨è·å–
            if self.data_manager:
                try:
                    price_data = self.data_manager.get_latest_price_data(symbol)
                    if price_data:
                        return price_data
                except Exception as e:
                    logger.debug(f"æ•°æ®ç®¡ç†å™¨è·å–å¤±è´¥: {e}")
            
            # è¿”å›æ¨¡æ‹Ÿæ•°æ®
            return {
                'price': np.random.uniform(10, 100),
                'volume': np.random.randint(1000000, 10000000),
                'high': np.random.uniform(100, 110),
                'low': np.random.uniform(90, 100),
                'prev_close': np.random.uniform(95, 105)
            }
        except Exception as e:
            logger.debug(f"è·å–ä»·æ ¼æ•°æ®å¤±è´¥ {symbol}: {e}")
            return {}
    
    async def _fetch_basic_data(self, symbol: str) -> Dict[str, Any]:
        """å¼‚æ­¥è·å–åŸºç¡€æ•°æ®"""
        try:
            await asyncio.sleep(0.1)
            
            # è¿”å›æ¨¡æ‹Ÿæ•°æ®
            return {
                'market_cap': np.random.uniform(10, 1000) * 100000000,
                'pe_ratio': np.random.uniform(5, 50),
                'pb_ratio': np.random.uniform(0.5, 5),
                'roe': np.random.uniform(5, 30)
            }
        except Exception as e:
            logger.debug(f"è·å–åŸºç¡€æ•°æ®å¤±è´¥ {symbol}: {e}")
            return {}
    
    async def _fetch_market_data(self, symbol: str) -> Dict[str, Any]:
        """å¼‚æ­¥è·å–å¸‚åœºæ•°æ®"""
        try:
            await asyncio.sleep(0.1)
            
            # è¿”å›æ¨¡æ‹Ÿæ•°æ®
            return {
                'sector': 'Technology',
                'industry': 'Software',
                'avg_volume': np.random.randint(500000, 5000000),
                'ma5': np.random.uniform(95, 105),
                'ma20': np.random.uniform(90, 110)
            }
        except Exception as e:
            logger.debug(f"è·å–å¸‚åœºæ•°æ®å¤±è´¥ {symbol}: {e}")
            return {}
    
    async def _fetch_social_data(self, symbol: str) -> Dict[str, Any]:
        """å¼‚æ­¥è·å–ç¤¾äº¤æ•°æ®ï¼ˆé›ªçƒï¼‰"""
        try:
            if not self.xueqiu_provider:
                return {}
            
            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡ŒåŒæ­¥çš„é›ªçƒAPIè°ƒç”¨
            loop = asyncio.get_event_loop()
            
            # è·å–é›ªçƒæƒ…ç»ªæ•°æ®
            xueqiu_sentiment = await loop.run_in_executor(
                None, 
                self.xueqiu_provider.get_stock_sentiment,
                symbol,
                7  # days
            )
            
            if not xueqiu_sentiment:
                return {}
            
            # è·å–è®¨è®ºæ•°æ®ï¼ˆé™åˆ¶æ•°é‡ä»¥æé«˜é€Ÿåº¦ï¼‰
            discussions = await loop.run_in_executor(
                None,
                self.xueqiu_provider.get_stock_discussions,
                symbol,
                20  # limit
            )
            
            # è®¡ç®—ç¤¾äº¤æŒ‡æ ‡
            social_data = {
                'social_sentiment_score': xueqiu_sentiment.get('sentiment_score', 0),
                'total_discussions': xueqiu_sentiment.get('total_discussions', 0),
                'positive_ratio': xueqiu_sentiment.get('positive_ratio', 0),
                'negative_ratio': xueqiu_sentiment.get('negative_ratio', 0),
                'avg_interactions': xueqiu_sentiment.get('avg_interactions', 0),
                'discussion_count': len(discussions) if discussions else 0,
                'social_heat': self._calculate_social_heat(xueqiu_sentiment),
                'sentiment_trend': self._analyze_sentiment_trend(discussions) if discussions else 'neutral'
            }
            
            # æ·»åŠ çƒ­é—¨è¯„è®ºï¼ˆå¦‚æœæœ‰ï¼‰
            if discussions and len(discussions) > 0:
                # æŒ‰äº’åŠ¨æ•°æ’åºï¼Œå–å‰3æ¡
                top_discussions = sorted(
                    discussions, 
                    key=lambda x: x.get('fav_count', 0) + x.get('reply_count', 0),
                    reverse=True
                )[:3]
                
                social_data['top_comments'] = [
                    {
                        'text': d.get('text', '')[:200],  # é™åˆ¶é•¿åº¦
                        'sentiment': d.get('sentiment_score', 0),
                        'interactions': d.get('fav_count', 0) + d.get('reply_count', 0)
                    }
                    for d in top_discussions
                ]
            
            logger.debug(f"ğŸ“± è·å– {symbol} ç¤¾äº¤æ•°æ®: è®¨è®ºæ•°={social_data['total_discussions']}, æƒ…ç»ª={social_data['social_sentiment_score']:.2f}")
            
            return social_data
            
        except Exception as e:
            logger.debug(f"è·å–ç¤¾äº¤æ•°æ®å¤±è´¥ {symbol}: {e}")
            return {}
    
    def _calculate_social_heat(self, sentiment_data: Dict[str, Any]) -> float:
        """è®¡ç®—ç¤¾äº¤çƒ­åº¦"""
        if not sentiment_data:
            return 0.0
        
        # ç»¼åˆè®¨è®ºæ•°ã€äº’åŠ¨æ•°å’Œæƒ…ç»ªå¼ºåº¦
        discussions = sentiment_data.get('total_discussions', 0)
        interactions = sentiment_data.get('avg_interactions', 0)
        sentiment_abs = abs(sentiment_data.get('sentiment_score', 0))
        
        # å½’ä¸€åŒ–åˆ°0-100
        heat_score = min(100, (
            min(discussions / 2, 50) +  # è®¨è®ºæ•°è´¡çŒ®æœ€å¤š50åˆ†
            min(interactions / 10, 30) +  # äº’åŠ¨æ•°è´¡çŒ®æœ€å¤š30åˆ†
            sentiment_abs * 20  # æƒ…ç»ªå¼ºåº¦è´¡çŒ®æœ€å¤š20åˆ†
        ))
        
        return heat_score
    
    def _analyze_sentiment_trend(self, discussions: List[Dict[str, Any]]) -> str:
        """åˆ†ææƒ…ç»ªè¶‹åŠ¿"""
        if not discussions or len(discussions) < 5:
            return 'neutral'
        
        # æŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        sorted_discussions = sorted(
            discussions,
            key=lambda x: x.get('created_at', 0),
            reverse=True
        )
        
        # åˆ†ææœ€è¿‘å’Œä¹‹å‰çš„æƒ…ç»ª
        recent_sentiments = [d.get('sentiment_score', 0) for d in sorted_discussions[:5]]
        older_sentiments = [d.get('sentiment_score', 0) for d in sorted_discussions[5:10]] if len(sorted_discussions) > 5 else []
        
        if not older_sentiments:
            return 'neutral'
        
        recent_avg = sum(recent_sentiments) / len(recent_sentiments)
        older_avg = sum(older_sentiments) / len(older_sentiments)
        
        # åˆ¤æ–­è¶‹åŠ¿
        if recent_avg - older_avg > 0.2:
            return 'improving'  # æƒ…ç»ªæ”¹å–„
        elif recent_avg - older_avg < -0.2:
            return 'deteriorating'  # æƒ…ç»ªæ¶åŒ–
        else:
            return 'stable'  # æƒ…ç»ªç¨³å®š
    
    def _get_cached_data(self, symbol: str) -> Optional[Dict[str, Any]]:
        """è·å–ç¼“å­˜æ•°æ®"""
        if symbol in self._cache:
            cache_time = self._cache_timestamps.get(symbol, 0)
            if time.time() - cache_time < self.config.cache_ttl:
                return self._cache[symbol]
        return None
    
    def _cache_data(self, symbol: str, data: Dict[str, Any]):
        """ç¼“å­˜æ•°æ®"""
        self._cache[symbol] = data
        self._cache_timestamps[symbol] = time.time()
    
    async def _metrics_collector(self):
        """åº¦é‡æ”¶é›†å™¨"""
        while self.is_running:
            try:
                await asyncio.sleep(5)  # æ¯5ç§’æ”¶é›†ä¸€æ¬¡
                
                # æ›´æ–°é˜Ÿåˆ—å¤§å°
                for stage, queue in self.queues.items():
                    self.metrics.queue_sizes[stage.value] = queue.qsize()
                
                # è®¡ç®—é˜¶æ®µå¹³å‡æ—¶é—´
                for stage, times in self._stage_timers.items():
                    if times:
                        self.metrics.stage_times[stage] = sum(times) / len(times)
                
                # æ¸…ç†æ—§çš„è®¡æ—¶æ•°æ®
                for stage in self._stage_timers:
                    if len(self._stage_timers[stage]) > 1000:
                        self._stage_timers[stage] = self._stage_timers[stage][-100:]
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"åº¦é‡æ”¶é›†å™¨é”™è¯¯: {e}")
    
    def _get_metrics_summary(self) -> Dict[str, Any]:
        """è·å–åº¦é‡æ‘˜è¦"""
        return {
            'total_packets': self.metrics.total_packets,
            'processed_packets': self.metrics.processed_packets,
            'failed_packets': self.metrics.failed_packets,
            'total_time': self.metrics.total_time,
            'throughput': self.metrics.throughput,
            'error_rate': self.metrics.error_rate,
            'stage_times': dict(self.metrics.stage_times),
            'queue_sizes': dict(self.metrics.queue_sizes)
        }
    
    def register_processor(self, stage: PipelineStage, processor: Callable):
        """æ³¨å†Œè‡ªå®šä¹‰å¤„ç†å™¨"""
        self.processors[stage] = processor
        logger.info(f"âœ… æ³¨å†Œå¤„ç†å™¨: {stage.value}")


# å…¨å±€ç®¡é“å®ä¾‹
_async_pipeline = None

def get_async_pipeline(config: PipelineConfig = None) -> AsyncDataPipeline:
    """è·å–å…¨å±€å¼‚æ­¥ç®¡é“å®ä¾‹"""
    global _async_pipeline
    if _async_pipeline is None:
        _async_pipeline = AsyncDataPipeline(config)
    return _async_pipeline


# ä¾¿æ·å‡½æ•°
async def async_process_symbols(symbols: List[str], config: PipelineConfig = None) -> Dict[str, Any]:
    """å¼‚æ­¥å¤„ç†è‚¡ç¥¨åˆ—è¡¨"""
    pipeline = get_async_pipeline(config)
    return await pipeline.process_symbols(symbols)