#!/usr/bin/env python3
"""
æœ¬åœ°æ•°æ®æŒä¹…åŒ–å­˜å‚¨ç³»ç»Ÿ
æä¾›æ°¸ä¹…æ€§çš„è‚¡ç¥¨æ•°æ®æœ¬åœ°å­˜å‚¨åŠŸèƒ½
"""

import os
import json
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Union
import pickle
import gzip
import hashlib
from dataclasses import dataclass

from tradingagents.utils.logging_manager import get_logger

logger = get_logger('persistent_storage')


@dataclass
class DataVersion:
    """æ•°æ®ç‰ˆæœ¬ä¿¡æ¯"""
    version: str
    created_at: datetime
    updated_at: datetime
    checksum: str
    data_size: int


class PersistentDataManager:
    """æŒä¹…åŒ–æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self, data_dir: str = "./data/persistent"):
        """
        åˆå§‹åŒ–æŒä¹…åŒ–æ•°æ®ç®¡ç†å™¨
        
        Args:
            data_dir: æ•°æ®å­˜å‚¨ç›®å½•
        """
        self.data_dir = Path(data_dir)
        self._ensure_directories()
        
        # æ•°æ®åˆ†ç±»å­˜å‚¨è·¯å¾„
        self.stocks_dir = self.data_dir / "stocks"
        self.historical_dir = self.data_dir / "historical"
        self.financial_dir = self.data_dir / "financial"
        self.macro_dir = self.data_dir / "macro"
        self.technical_dir = self.data_dir / "technical"
        self.news_dir = self.data_dir / "news"
        
        logger.info("ğŸ—‚ï¸ æŒä¹…åŒ–æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“ æ•°æ®ç›®å½•: {self.data_dir}")
    
    def _ensure_directories(self):
        """ç¡®ä¿æ‰€æœ‰ç›®å½•å­˜åœ¨"""
        directories = [
            self.data_dir,
            self.data_dir / "stocks",
            self.data_dir / "historical" / "daily",
            self.data_dir / "historical" / "weekly",
            self.data_dir / "historical" / "monthly",
            self.data_dir / "financial" / "quarterly",
            self.data_dir / "financial" / "annual",
            self.data_dir / "macro" / "indices",
            self.data_dir / "macro" / "economic",
            self.data_dir / "technical" / "indicators",
            self.data_dir / "news" / "announcements",
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
    
    def _get_file_path(self, category: str, symbol: str, data_type: str, 
                      date_range: Optional[str] = None) -> Path:
        """è·å–æ–‡ä»¶å­˜å‚¨è·¯å¾„"""
        base_dir = self.data_dir / category
        
        if category == "stocks":
            return base_dir / f"{symbol}_{data_type}.parquet"
        elif category == "historical":
            return base_dir / data_type / f"{symbol}.parquet"
        elif category == "financial":
            return base_dir / data_type / f"{symbol}.parquet"
        else:
            return base_dir / f"{symbol}_{data_type}.parquet"
    
    def _calculate_checksum(self, data: pd.DataFrame) -> str:
        """è®¡ç®—æ•°æ®æ ¡éªŒå’Œ"""
        if data.empty:
            return ""
        data_str = data.to_string()
        return hashlib.md5(data_str.encode()).hexdigest()
    
    def save_stock_master(self, stock_data: pd.DataFrame):
        """ä¿å­˜è‚¡ç¥¨åŸºç¡€ä¿¡æ¯"""
        file_path = self.data_dir / "stocks" / "master_data.parquet"
        stock_data.to_parquet(file_path, compression='snappy')
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            "updated_at": datetime.now().isoformat(),
            "record_count": len(stock_data),
            "checksum": self._calculate_checksum(stock_data)
        }
        
        metadata_path = self.data_dir / "stocks" / "master_metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… ä¿å­˜è‚¡ç¥¨åŸºç¡€ä¿¡æ¯: {len(stock_data)} æ¡è®°å½•")
    
    def load_stock_master(self) -> Optional[pd.DataFrame]:
        """åŠ è½½è‚¡ç¥¨åŸºç¡€ä¿¡æ¯"""
        file_path = self.data_dir / "stocks" / "master_data.parquet"
        if file_path.exists():
            return pd.read_parquet(file_path)
        return None
    
    def save_historical_prices(self, symbol: str, data: pd.DataFrame, 
                             frequency: str = "daily"):
        """ä¿å­˜å†å²ä»·æ ¼æ•°æ®"""
        if data.empty:
            return
            
        file_path = self._get_file_path("historical", symbol, frequency)
        
        # ç¡®ä¿æ•°æ®æŒ‰æ—¥æœŸæ’åº
        if 'date' in data.columns:
            data = data.sort_values('date')
        
        # ä¿å­˜æ•°æ®
        data.to_parquet(file_path, compression='snappy')
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            "symbol": symbol,
            "frequency": frequency,
            "updated_at": datetime.now().isoformat(),
            "date_range": {
                "start": data['date'].min() if 'date' in data.columns else None,
                "end": data['date'].max() if 'date' in data.columns else None
            },
            "record_count": len(data),
            "checksum": self._calculate_checksum(data)
        }
        
        metadata_path = file_path.parent / f"{symbol}_{frequency}_metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… ä¿å­˜å†å²ä»·æ ¼æ•°æ®: {symbol} ({frequency}) - {len(data)} æ¡è®°å½•")
    
    def load_historical_prices(self, symbol: str, frequency: str = "daily",
                             start_date: Optional[str] = None, 
                             end_date: Optional[str] = None) -> Optional[pd.DataFrame]:
        """åŠ è½½å†å²ä»·æ ¼æ•°æ®"""
        file_path = self._get_file_path("historical", symbol, frequency)
        
        if not file_path.exists():
            return None
        
        try:
            data = pd.read_parquet(file_path)
            
            if 'date' in data.columns and (start_date or end_date):
                data['date'] = pd.to_datetime(data['date'])
                
                if start_date:
                    data = data[data['date'] >= pd.to_datetime(start_date)]
                if end_date:
                    data = data[data['date'] <= pd.to_datetime(end_date)]
            
            return data
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å†å²ä»·æ ¼æ•°æ®å¤±è´¥: {symbol} - {e}")
            return None
    
    def save_financial_data(self, symbol: str, data: pd.DataFrame, 
                          report_type: str = "quarterly"):
        """ä¿å­˜è´¢åŠ¡æ•°æ®"""
        if data.empty:
            return
            
        file_path = self._get_file_path("financial", symbol, report_type)
        data.to_parquet(file_path, compression='snappy')
        
        metadata = {
            "symbol": symbol,
            "report_type": report_type,
            "updated_at": datetime.now().isoformat(),
            "record_count": len(data),
            "checksum": self._calculate_checksum(data)
        }
        
        metadata_path = file_path.parent / f"{symbol}_{report_type}_metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… ä¿å­˜è´¢åŠ¡æ•°æ®: {symbol} ({report_type}) - {len(data)} æ¡è®°å½•")
    
    def load_financial_data(self, symbol: str, 
                          report_type: str = "quarterly") -> Optional[pd.DataFrame]:
        """åŠ è½½è´¢åŠ¡æ•°æ®"""
        file_path = self._get_file_path("financial", symbol, report_type)
        
        if not file_path.exists():
            return None
        
        try:
            return pd.read_parquet(file_path)
        except Exception as e:
            logger.error(f"âŒ åŠ è½½è´¢åŠ¡æ•°æ®å¤±è´¥: {symbol} - {e}")
            return None
    
    def get_available_symbols(self, category: str = "historical", 
                            frequency: str = "daily") -> List[str]:
        """è·å–å¯ç”¨çš„è‚¡ç¥¨ä»£ç åˆ—è¡¨"""
        if category == "historical":
            directory = self.historical_dir / frequency
        elif category == "financial":
            directory = self.financial_dir / frequency
        else:
            directory = self.data_dir / category
        
        if not directory.exists():
            return []
        
        files = directory.glob("*.parquet")
        return [f.stem for f in files]
    
    def get_data_status(self, symbol: str, category: str, 
                       data_type: str) -> Dict[str, Any]:
        """è·å–æ•°æ®çŠ¶æ€ä¿¡æ¯"""
        file_path = self._get_file_path(category, symbol, data_type)
        metadata_path = file_path.parent / f"{symbol}_{data_type}_metadata.json"
        
        status = {
            "exists": file_path.exists(),
            "file_path": str(file_path),
            "file_size": 0,
            "last_updated": None,
            "record_count": 0,
            "date_range": None
        }
        
        if file_path.exists():
            status["file_size"] = file_path.stat().st_size
            
            if metadata_path.exists():
                try:
                    with open(metadata_path, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                        status.update(metadata)
                except Exception as e:
                    logger.warning(f"âš ï¸ è¯»å–å…ƒæ•°æ®å¤±è´¥: {e}")
        
        return status
    
    def cleanup_old_metadata(self):
        """æ¸…ç†æ—§çš„å…ƒæ•°æ®æ–‡ä»¶"""
        for category_dir in [self.historical_dir, self.financial_dir]:
            for freq_dir in category_dir.iterdir():
                if freq_dir.is_dir():
                    for metadata_file in freq_dir.glob("*_metadata.json"):
                        symbol = metadata_file.stem.replace("_daily_metadata", "").replace("_quarterly_metadata", "").replace("_annual_metadata", "")
                        data_file = freq_dir / f"{symbol}.parquet"
                        
                        if not data_file.exists():
                            metadata_file.unlink()
                            logger.info(f"ğŸ§¹ æ¸…ç†å­¤ç«‹å…ƒæ•°æ®: {metadata_file}")


# å…¨å±€å®ä¾‹
_persistent_manager = None


def get_persistent_manager(data_dir: str = "./data/persistent") -> PersistentDataManager:
    """è·å–å…¨å±€æŒä¹…åŒ–æ•°æ®ç®¡ç†å™¨å®ä¾‹"""
    global _persistent_manager
    if _persistent_manager is None:
        _persistent_manager = PersistentDataManager(data_dir)
    return _persistent_manager