#!/usr/bin/env python3
"""
ç ”ç©¶æ‰€åˆ†æå¸ˆæŠ¥å‘Šæ•°æ®é€‚é…å™¨
é›†æˆå¤šä¸ªåˆ¸å•†ç ”æŠ¥æ•°æ®æºï¼Œæä¾›æ ‡å‡†åŒ–çš„ç ”æŠ¥æ•°æ®æ¥å£
"""

import requests
import time
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
import json
import re
from dataclasses import dataclass

# å¯¼å…¥æ—¥å¿—æ¨¡å—
from tradingagents.utils.logging_manager import get_logger
logger = get_logger('agents')

# å¯¼å…¥è‚¡ç¥¨å·¥å…·ç±»
from tradingagents.utils.stock_utils import StockUtils


@dataclass
class ResearchReport:
    """ç ”æŠ¥æ•°æ®ç»“æ„"""
    title: str                    # ç ”æŠ¥æ ‡é¢˜
    analyst: str                  # åˆ†æå¸ˆ
    institution: str              # åˆ¸å•†æœºæ„
    publish_date: str             # å‘å¸ƒæ—¥æœŸ
    rating: str                   # è¯„çº§ (ä¹°å…¥/å–å‡º/æŒæœ‰ç­‰)
    target_price: Optional[float] # ç›®æ ‡ä»·
    current_price: Optional[float] # å½“å‰ä»·
    summary: str                  # æ‘˜è¦
    key_points: List[str]         # å…³é”®è§‚ç‚¹
    pe_forecast: Optional[float]  # é¢„æµ‹PE
    revenue_growth: Optional[float] # æ”¶å…¥å¢é•¿é¢„æµ‹
    profit_growth: Optional[float]  # åˆ©æ¶¦å¢é•¿é¢„æµ‹
    source: str                   # æ•°æ®æº
    confidence_level: float       # å¯ä¿¡åº¦è¯„åˆ† (0-1)


class ResearchReportProvider:
    """ç ”æŠ¥æ•°æ®æä¾›å™¨åŸºç±»"""
    
    def __init__(self):
        self.name = "BaseProvider"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
    def get_reports(self, ticker: str, limit: int = 10) -> List[ResearchReport]:
        """è·å–ç ”æŠ¥æ•°æ®"""
        raise NotImplementedError
    
    def _parse_rating(self, rating_text: str) -> str:
        """æ ‡å‡†åŒ–è¯„çº§"""
        rating_text = rating_text.upper()
        
        # ä¹°å…¥ç±»
        if any(keyword in rating_text for keyword in ['ä¹°å…¥', 'BUY', 'æ¨è', 'å¢æŒ', 'å¼ºæ¨']):
            return 'ä¹°å…¥'
        # å–å‡ºç±»
        elif any(keyword in rating_text for keyword in ['å–å‡º', 'SELL', 'å‡æŒ']):
            return 'å–å‡º'
        # æŒæœ‰ç±»
        elif any(keyword in rating_text for keyword in ['æŒæœ‰', 'HOLD', 'ä¸­æ€§']):
            return 'æŒæœ‰'
        else:
            return 'æœªçŸ¥'
    
    def _extract_price(self, text: str) -> Optional[float]:
        """ä»æ–‡æœ¬ä¸­æå–ä»·æ ¼"""
        try:
            # åŒ¹é…ä»·æ ¼æ¨¡å¼ï¼šXXå…ƒã€XX.XXå…ƒã€$XX.XXç­‰
            price_patterns = [
                r'(\d+\.?\d*)\s*å…ƒ',
                r'ç›®æ ‡ä»·[ï¼š:]\s*(\d+\.?\d*)',
                r'\$(\d+\.?\d*)',
                r'(\d+\.?\d*)\s*[ç¾æ¸¯]?å…ƒ'
            ]
            
            for pattern in price_patterns:
                match = re.search(pattern, text)
                if match:
                    return float(match.group(1))
            return None
        except:
            return None


class EastMoneyResearchProvider(ResearchReportProvider):
    """ä¸œæ–¹è´¢å¯Œç ”æŠ¥æ•°æ®æä¾›å™¨ - ä½¿ç”¨çœŸå®HTTP API"""
    
    def __init__(self):
        super().__init__()
        self.name = "ä¸œæ–¹è´¢å¯Œ"
        self.api_base_url = "https://datacenter-web.eastmoney.com/api/data/v1/get"
        
        # APIè¯·æ±‚é…ç½®
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': 'https://data.eastmoney.com/',
            'Accept': 'text/javascript, application/javascript, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        }
        
        self.timeout = 30
        
    def get_reports(self, ticker: str, limit: int = 10) -> List[ResearchReport]:
        """è·å–ä¸œæ–¹è´¢å¯Œç ”æŠ¥æ•°æ® - æ”¯æŒå¤šç§API"""
        try:
            logger.info(f"ğŸ” å¼€å§‹ä»ä¸œæ–¹è´¢å¯Œè·å–ç ”æŠ¥: {ticker}")
            
            # æ ¼å¼åŒ–è‚¡ç¥¨ä»£ç 
            formatted_ticker = self._format_ticker(ticker)
            if not formatted_ticker:
                logger.warning(f"âš ï¸ è‚¡ç¥¨ä»£ç æ ¼å¼åŒ–å¤±è´¥: {ticker}")
                return []
            
            all_reports = []
            
            # 1. å°è¯•è·å–æœºæ„è°ƒç ”æ•°æ® (ä¸»è¦æ•°æ®æº)
            survey_reports = self._get_survey_reports(formatted_ticker, limit)
            if survey_reports:
                all_reports.extend(survey_reports)
                logger.info(f"âœ… ä¸œæ–¹è´¢å¯Œæœºæ„è°ƒç ”æ•°æ®è·å–æˆåŠŸ: {len(survey_reports)} æ¡")
            
            # 2. å¦‚æœè°ƒç ”æ•°æ®ä¸è¶³ï¼Œå°è¯•è·å–ä»·å€¼åˆ†ææ•°æ®
            if len(all_reports) < limit:
                value_reports = self._get_value_analysis_reports(formatted_ticker, limit - len(all_reports))
                if value_reports:
                    all_reports.extend(value_reports)
                    logger.info(f"âœ… ä¸œæ–¹è´¢å¯Œä»·å€¼åˆ†ææ•°æ®è·å–æˆåŠŸ: {len(value_reports)} æ¡")
            
            # 3. é™åˆ¶è¿”å›æ•°é‡
            reports = all_reports[:limit]
            
            if reports:
                logger.info(f"âœ… ä»ä¸œæ–¹è´¢å¯Œè·å–åˆ° {len(reports)} æ¡ç»¼åˆæ•°æ®: {ticker}")
            else:
                logger.info(f"â„¹ï¸ ä¸œæ–¹è´¢å¯Œæœªè·å–åˆ°ç ”æŠ¥æ•°æ®: {ticker}")
            
            return reports
            
        except Exception as e:
            logger.error(f"âŒ ä¸œæ–¹è´¢å¯ŒAPIè°ƒç”¨å¤±è´¥: {ticker}, é”™è¯¯: {str(e)}")
            return []
    
    def _format_ticker(self, ticker: str) -> Optional[str]:
        """æ ¼å¼åŒ–è‚¡ç¥¨ä»£ç ä¸ºä¸œæ–¹è´¢å¯ŒAPIæ ¼å¼"""
        try:
            # æ¸…ç†è‚¡ç¥¨ä»£ç 
            clean_ticker = ticker.upper().replace('.SZ', '').replace('.SH', '').replace('.BJ', '').strip()
            
            if not clean_ticker.isdigit():
                return None
            
            # æ ¹æ®è‚¡ç¥¨ä»£ç å‰ç¼€åˆ¤æ–­äº¤æ˜“æ‰€
            if clean_ticker.startswith(('000', '002', '003', '300', '301')):
                return f"{clean_ticker}.SZ"  # æ·±äº¤æ‰€
            elif clean_ticker.startswith(('600', '601', '603', '605', '688', '689')):
                return f"{clean_ticker}.SH"  # ä¸Šäº¤æ‰€
            elif len(clean_ticker) == 8:  # åŒ—äº¤æ‰€
                return f"{clean_ticker}.BJ"
            else:
                return f"{clean_ticker}.SH"  # é»˜è®¤ä¸Šäº¤æ‰€
                
        except Exception as e:
            logger.debug(f"è‚¡ç¥¨ä»£ç æ ¼å¼åŒ–å¼‚å¸¸: {ticker}, {e}")
            return None
    
    def _build_api_params(self, ticker: str, limit: int) -> Dict[str, str]:
        """æ„å»ºAPIè¯·æ±‚å‚æ•° - ä½¿ç”¨æ­£ç¡®çš„ç ”æŠ¥API"""
        import time
        import random
        
        # æå–çº¯æ•°å­—è‚¡ç¥¨ä»£ç 
        stock_code = ticker.split('.')[0]
        
        return {
            'sortColumns': 'NOTICE_DATE',  # æŒ‰å…¬å‘Šæ—¥æœŸæ’åº
            'sortTypes': '-1',  # é™åº
            'pageSize': str(min(limit, 20)),  # é™åˆ¶é¡µé¢å¤§å°
            'pageNumber': '1',
            'reportName': 'RPT_ORG_SURVEY',  # æœºæ„è°ƒç ”æ•°æ® - åŒ…å«ç ”æŠ¥ä¿¡æ¯
            'columns': 'ALL',  # è¿”å›æ‰€æœ‰å­—æ®µ
            'filter': f'(SECURITY_CODE="{stock_code}")',  # è¿‡æ»¤æ¡ä»¶
            'client': 'WEB'
        }
    
    def _make_api_request(self, params: Dict[str, str]) -> Optional[Dict]:
        """å‘é€APIè¯·æ±‚ - ç›´æ¥ä½¿ç”¨JSONå“åº”"""
        try:
            import requests
            import json
            
            response = requests.get(
                self.api_base_url,
                params=params,
                headers=self.headers,
                timeout=self.timeout
            )
            
            if response.status_code != 200:
                logger.warning(f"âš ï¸ ä¸œæ–¹è´¢å¯ŒAPIè¯·æ±‚å¤±è´¥: HTTP {response.status_code}")
                return None
            
            # ç›´æ¥è§£æJSONå“åº”
            try:
                data = response.json()
            except json.JSONDecodeError:
                logger.warning(f"âš ï¸ ä¸œæ–¹è´¢å¯ŒAPIå“åº”æ ¼å¼å¼‚å¸¸ï¼Œå°è¯•JSONPè§£æ...")
                # å°è¯•JSONPæ ¼å¼
                import re
                text = response.text.strip()
                match = re.search(r'\((.*)\);?$', text)
                if match:
                    json_str = match.group(1)
                    data = json.loads(json_str)
                else:
                    logger.warning(f"âš ï¸ æ— æ³•è§£æä¸œæ–¹è´¢å¯ŒAPIå“åº”")
                    return None
            
            if not data.get('success', False):
                logger.warning(f"âš ï¸ ä¸œæ–¹è´¢å¯ŒAPIè¿”å›å¤±è´¥: {data.get('message', 'æœªçŸ¥é”™è¯¯')}")
                return None
            
            return data
            
        except Exception as e:
            logger.error(f"âŒ ä¸œæ–¹è´¢å¯ŒAPIè¯·æ±‚å¼‚å¸¸: {str(e)}")
            return None
    
    def _parse_api_response(self, response_data: Dict, ticker: str, limit: int) -> List[ResearchReport]:
        """è§£æAPIå“åº”æ•°æ®"""
        try:
            reports = []
            
            result = response_data.get('result', {})
            if not result:
                return reports
            
            data_list = result.get('data', [])
            if not data_list:
                return reports
            
            for item in data_list[:limit]:
                try:
                    # è§£æç ”æŠ¥æ•°æ®
                    report = self._parse_single_report(item, ticker)
                    if report:
                        reports.append(report)
                except Exception as e:
                    logger.debug(f"è§£æå•æ¡ç ”æŠ¥æ•°æ®å¤±è´¥: {e}")
                    continue
            
            return reports
            
        except Exception as e:
            logger.error(f"âŒ è§£æä¸œæ–¹è´¢å¯ŒAPIå“åº”å¤±è´¥: {str(e)}")
            return []
    
    def _parse_single_report(self, item: Dict, ticker: str) -> Optional[ResearchReport]:
        """è§£æå•æ¡ç ”æŠ¥æ•°æ® - é€‚é…RPT_ORG_SURVEYæ ¼å¼"""
        try:
            # æå–åŸºç¡€å­—æ®µ
            stock_name = item.get('SECURITY_NAME_ABBR', ticker)
            stock_code = item.get('SECURITY_CODE', ticker)
            institution = item.get('ORG_NAME', 'æœªçŸ¥æœºæ„')
            
            # æ„å»ºæ ‡é¢˜
            notice_date = item.get('NOTICE_DATE', '')
            receive_time = item.get('RECEIVE_TIME_EXPLAIN', '')
            receive_object = item.get('RECEIVE_OBJECT', '')
            
            title = f"{stock_name}({stock_code}) æœºæ„è°ƒç ”æŠ¥å‘Š"
            if receive_time:
                title += f" - {receive_time}"
            
            # åˆ†æå¸ˆä¿¡æ¯
            analyst = "æœºæ„åˆ†æå¸ˆ"  # è°ƒç ”æŠ¥å‘Šé€šå¸¸ä¸æŒ‡å®šå…·ä½“åˆ†æå¸ˆ
            publish_date = self._format_date(notice_date)
            
            # è¯„çº§ç›¸å…³ - è°ƒç ”æŠ¥å‘Šé€šå¸¸ä¸åŒ…å«è¯„çº§
            rating = "æœªçŸ¥"
            target_price = None
            
            # æ„å»ºæ‘˜è¦
            summary = self._build_summary_from_survey(item)
            
            # æå–å…³é”®è§‚ç‚¹
            key_points = self._extract_key_points_from_survey(item)
            
            # è®¡ç®—å¯ä¿¡åº¦
            confidence_level = self._calculate_confidence_for_survey(item, institution)
            
            # æ•°æ®è´¨é‡éªŒè¯
            if not self._validate_report_data(title, institution, publish_date):
                return None
            
            return ResearchReport(
                title=title,
                analyst=analyst,
                institution=institution,
                publish_date=publish_date,
                rating=rating,
                target_price=target_price,
                current_price=None,
                summary=summary,
                key_points=key_points,
                pe_forecast=None,
                revenue_growth=None,
                profit_growth=None,
                source=self.name,
                confidence_level=confidence_level
            )
            
        except Exception as e:
            logger.debug(f"è§£æå•æ¡è°ƒç ”æŠ¥å‘Šå¤±è´¥: {e}")
            return None
    
    def _format_date(self, date_str: str) -> str:
        """æ ¼å¼åŒ–æ—¥æœŸ"""
        try:
            if not date_str:
                return datetime.now().strftime('%Y-%m-%d')
            
            # å¤„ç†ä¸åŒçš„æ—¥æœŸæ ¼å¼
            if 'T' in str(date_str):
                # ISOæ ¼å¼: 2024-01-01T00:00:00
                return str(date_str).split('T')[0]
            elif len(str(date_str)) >= 10:
                # å·²ç»æ˜¯ YYYY-MM-DD æ ¼å¼
                return str(date_str)[:10]
            else:
                return datetime.now().strftime('%Y-%m-%d')
                
        except:
            return datetime.now().strftime('%Y-%m-%d')
    
    def _parse_target_price(self, price_data) -> Optional[float]:
        """è§£æç›®æ ‡ä»·"""
        try:
            if price_data is None:
                return None
            
            if isinstance(price_data, (int, float)):
                return float(price_data) if price_data > 0 else None
            
            if isinstance(price_data, str):
                return self._extract_price(price_data)
            
            return None
        except:
            return None
    
    def _safe_float(self, value) -> Optional[float]:
        """å®‰å…¨è½¬æ¢ä¸ºæµ®ç‚¹æ•°"""
        try:
            if value is None or value == '':
                return None
            return float(value)
        except:
            return None
    
    def _build_summary_from_rating(self, item: Dict) -> str:
        """ä»è¯„çº§æ•°æ®æ„å»ºç ”æŠ¥æ‘˜è¦"""
        summary_parts = []
        
        # è¯„çº§å˜åŒ–ä¿¡æ¯
        rating_change = item.get('RATING_CHANGE_TYPE', '')
        new_rating = item.get('NEW_RATING', '')
        old_rating = item.get('OLD_RATING', '')
        
        if rating_change and old_rating and new_rating:
            summary_parts.append(f"è¯„çº§{rating_change}: {old_rating} -> {new_rating}")
        elif new_rating:
            summary_parts.append(f"è¯„çº§: {new_rating}")
        
        # ç›®æ ‡ä»·ä¿¡æ¯
        target_new = item.get('TARGET_PRICE_NEW')
        target_old = item.get('TARGET_PRICE_OLD')
        if target_new and target_old:
            summary_parts.append(f"ç›®æ ‡ä»·è°ƒæ•´: {target_old}å…ƒ -> {target_new}å…ƒ")
        elif target_new:
            summary_parts.append(f"ç›®æ ‡ä»·: {target_new}å…ƒ")
        
        # è¯„çº§åŸå› 
        reason = item.get('RATING_CHANGE_REASON', '')
        if reason:
            summary_parts.append(f"è°ƒæ•´åŸå› : {reason}")
        
        # æœºæ„è§‚ç‚¹
        if item.get('ORG_NAME'):
            summary_parts.append(f"æœºæ„: {item['ORG_NAME']}")
        
        return "; ".join(summary_parts) if summary_parts else "æš‚æ— æ‘˜è¦"
    
    def _extract_key_points_from_rating(self, item: Dict) -> List[str]:
        """ä»è¯„çº§æ•°æ®æå–å…³é”®è§‚ç‚¹"""
        key_points = []
        
        # è¯„çº§å˜åŒ–
        rating_change = item.get('RATING_CHANGE_TYPE', '')
        new_rating = item.get('NEW_RATING', '')
        if rating_change and new_rating:
            key_points.append(f"è¯„çº§{rating_change}: {new_rating}")
        elif new_rating:
            key_points.append(f"æŠ•èµ„è¯„çº§: {new_rating}")
        
        # ç›®æ ‡ä»·
        target_price = item.get('TARGET_PRICE_NEW') or item.get('TARGET_PRICE_OLD')
        if target_price:
            key_points.append(f"ç›®æ ‡ä»·ä½: {target_price}å…ƒ")
        
        # è°ƒæ•´åŸå› 
        reason = item.get('RATING_CHANGE_REASON', '')
        if reason and len(reason) < 50:  # åªå–è¾ƒçŸ­çš„åŸå› ä½œä¸ºå…³é”®ç‚¹
            key_points.append(f"åŸå› : {reason}")
        
        return key_points[:3]  # æœ€å¤š3ä¸ªå…³é”®è§‚ç‚¹
    
    def _calculate_confidence_for_rating(self, item: Dict, institution: str) -> float:
        """è®¡ç®—è¯„çº§æ•°æ®çš„å¯ä¿¡åº¦è¯„åˆ†"""
        base_score = 0.75  # ä¸œæ–¹è´¢å¯ŒåŸºç¡€å¯ä¿¡åº¦
        
        # çŸ¥åæœºæ„åŠ åˆ†
        prestigious_orgs = ['ä¸­ä¿¡è¯åˆ¸', 'åæ³°è¯åˆ¸', 'å›½æ³°å›å®‰', 'æµ·é€šè¯åˆ¸', 'å¹¿å‘è¯åˆ¸', 'æ‹›å•†è¯åˆ¸', 'ä¸­é‡‘å…¬å¸', 
                           'ç”³ä¸‡å®æº', 'å…´ä¸šè¯åˆ¸', 'ä¸œæ–¹è¯åˆ¸', 'æ–¹æ­£è¯åˆ¸', 'å›½ä¿¡è¯åˆ¸', 'å…‰å¤§è¯åˆ¸']
        if any(org in institution for org in prestigious_orgs):
            base_score += 0.1
        
        # æœ‰ç›®æ ‡ä»·åŠ åˆ†
        if item.get('TARGET_PRICE_NEW') or item.get('TARGET_PRICE_OLD'):
            base_score += 0.05
        
        # æœ‰åˆ†æå¸ˆä¿¡æ¯åŠ åˆ†
        if item.get('RESEARCHER_NAME') and item['RESEARCHER_NAME'] != 'æœªçŸ¥åˆ†æå¸ˆ':
            base_score += 0.05
        
        # æœ‰è¯„çº§å˜åŒ–åŸå› åŠ åˆ†
        if item.get('RATING_CHANGE_REASON'):
            base_score += 0.05
        
        return min(base_score, 1.0)

    def _build_summary_from_survey(self, item: Dict) -> str:
        """ä»è°ƒç ”æ•°æ®æ„å»ºæ‘˜è¦"""
        summary_parts = []
        
        # è°ƒç ”åŸºæœ¬ä¿¡æ¯
        notice_date = item.get('NOTICE_DATE', '')
        receive_time = item.get('RECEIVE_TIME_EXPLAIN', '')
        receive_object = item.get('RECEIVE_OBJECT', '')
        org_name = item.get('ORG_NAME', '')
        
        if org_name:
            summary_parts.append(f"æœºæ„: {org_name}")
        if notice_date:
            summary_parts.append(f"è°ƒç ”æ—¥æœŸ: {notice_date}")
        if receive_time:
            summary_parts.append(f"è°ƒç ”æ—¶é—´: {receive_time}")
        if receive_object:
            summary_parts.append(f"è°ƒç ”å¯¹è±¡: {receive_object}")
        
        # è°ƒç ”å†…å®¹
        content = item.get('CONTENT', '')
        if content:
            summary_parts.append(f"è°ƒç ”å†…å®¹: {content[:200]}...")
        
        # è°ƒç ”åœ°ç‚¹å’Œæ–¹å¼
        receive_place = item.get('RECEIVE_PLACE', '')
        receive_way = item.get('RECEIVE_WAY_EXPLAIN', '')
        if receive_place:
            summary_parts.append(f"è°ƒç ”åœ°ç‚¹: {receive_place}")
        if receive_way:
            summary_parts.append(f"è°ƒç ”æ–¹å¼: {receive_way}")
        
        return "; ".join(summary_parts) if summary_parts else "æœºæ„è°ƒç ”æŠ¥å‘Š"

    def _extract_key_points_from_survey(self, item: Dict) -> List[str]:
        """ä»è°ƒç ”æ•°æ®æå–å…³é”®è§‚ç‚¹"""
        key_points = []
        
        # è°ƒç ”å¯¹è±¡
        receive_object = item.get('RECEIVE_OBJECT', '')
        if receive_object:
            key_points.append(f"è°ƒç ”å¯¹è±¡: {receive_object}")
        
        # è°ƒç ”æ—¶é—´
        receive_time = item.get('RECEIVE_TIME_EXPLAIN', '')
        if receive_time:
            key_points.append(f"è°ƒç ”æ—¶é—´: {receive_time}")
        
        # è°ƒç ”å†…å®¹è¦ç‚¹
        content = item.get('CONTENT', '')
        if content:
            # æå–å†…å®¹ä¸­çš„å…³é”®ä¿¡æ¯
            sentences = content.replace('ã€‚', '\n').replace('ï¼›', '\n').split('\n')
            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) > 10 and len(sentence) < 100:
                    key_points.append(sentence)
        
        # æœºæ„ä¿¡æ¯
        org_name = item.get('ORG_NAME', '')
        if org_name:
            key_points.append(f"è°ƒç ”æœºæ„: {org_name}")
        
        return key_points[:3]  # æœ€å¤š3ä¸ªå…³é”®è§‚ç‚¹

    def _get_survey_reports(self, ticker: str, limit: int) -> List[ResearchReport]:
        """è·å–æœºæ„è°ƒç ”æŠ¥å‘Š"""
        try:
            params = {
                'reportName': 'RPT_ORG_SURVEY',
                'columns': 'ALL',
                'filter': f'(SECURITY_CODE="{ticker.split(".")[0]}")',
                'pageNumber': 1,
                'pageSize': str(limit),
                'sortTypes': -1,
                'sortColumns': 'NOTICE_DATE',
                'client': 'WEB',
            }
            
            response = self._make_api_request(params)
            if not response:
                return []
            
            return self._parse_survey_response(response, ticker, limit)
            
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–æœºæ„è°ƒç ”æ•°æ®å¤±è´¥: {e}")
            return []
    
    def _get_value_analysis_reports(self, ticker: str, limit: int) -> List[ResearchReport]:
        """è·å–ä»·å€¼åˆ†ææ•°æ®ä½œä¸ºè¡¥å……"""
        try:
            params = {
                'reportName': 'RPT_VALUEANALYSIS_DET',
                'columns': 'ALL',
                'filter': f'(SECURITY_CODE="{ticker.split(".")[0]}")',
                'pageNumber': 1,
                'pageSize': str(limit),
                'sortTypes': -1,
                'sortColumns': 'TRADE_DATE',
                'client': 'WEB',
            }
            
            response = self._make_api_request(params)
            if not response:
                return []
            
            return self._parse_value_response(response, ticker, limit)
            
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–ä»·å€¼åˆ†ææ•°æ®å¤±è´¥: {e}")
            return []
    
    def _parse_survey_response(self, response_data: Dict, ticker: str, limit: int) -> List[ResearchReport]:
        """è§£ææœºæ„è°ƒç ”å“åº”æ•°æ®"""
        try:
            reports = []
            
            result = response_data.get('result', {})
            if not result:
                return reports
            
            data_list = result.get('data', [])
            if not data_list:
                return reports
            
            for item in data_list[:limit]:
                try:
                    report = self._parse_survey_report(item, ticker)
                    if report:
                        reports.append(report)
                except Exception as e:
                    logger.debug(f"è§£æå•æ¡è°ƒç ”æŠ¥å‘Šå¤±è´¥: {e}")
                    continue
            
            return reports
            
        except Exception as e:
            logger.error(f"âŒ è§£ææœºæ„è°ƒç ”å“åº”å¤±è´¥: {str(e)}")
            return []
    
    def _parse_value_response(self, response_data: Dict, ticker: str, limit: int) -> List[ResearchReport]:
        """è§£æä»·å€¼åˆ†æå“åº”æ•°æ®"""
        try:
            reports = []
            
            result = response_data.get('result', {})
            if not result:
                return reports
            
            data_list = result.get('data', [])
            if not data_list:
                return reports
            
            for item in data_list[:limit]:
                try:
                    report = self._parse_value_report(item, ticker)
                    if report:
                        reports.append(report)
                except Exception as e:
                    logger.debug(f"è§£æå•æ¡ä»·å€¼åˆ†ææŠ¥å‘Šå¤±è´¥: {e}")
                    continue
            
            return reports
            
        except Exception as e:
            logger.error(f"âŒ è§£æä»·å€¼åˆ†æå“åº”å¤±è´¥: {str(e)}")
            return []
    
    def _parse_survey_report(self, item: Dict, ticker: str) -> Optional[ResearchReport]:
        """è§£æå•æ¡æœºæ„è°ƒç ”æŠ¥å‘Š"""
        try:
            stock_name = item.get('SECURITY_NAME_ABBR', ticker)
            stock_code = item.get('SECURITY_CODE', ticker)
            institution = item.get('ORG_NAME', 'æœªçŸ¥æœºæ„')
            
            # æ„å»ºæ ‡é¢˜
            notice_date = item.get('NOTICE_DATE', '')
            receive_time = item.get('RECEIVE_TIME_EXPLAIN', '')
            
            title = f"{stock_name}({stock_code}) æœºæ„è°ƒç ”æŠ¥å‘Š"
            if receive_time:
                title += f" - {receive_time}"
            
            publish_date = self._format_date(notice_date)
            
            # æ„å»ºæ‘˜è¦
            summary = self._build_summary_from_survey(item)
            key_points = self._extract_key_points_from_survey(item)
            
            confidence_level = self._calculate_confidence_for_survey(item, institution)
            
            if not self._validate_report_data(title, institution, publish_date):
                return None
            
            return ResearchReport(
                title=title,
                analyst="æœºæ„åˆ†æå¸ˆ",
                institution=institution,
                publish_date=publish_date,
                rating="æœºæ„è°ƒç ”",
                target_price=None,
                current_price=item.get('CLOSE_PRICE'),
                summary=summary,
                key_points=key_points,
                pe_forecast=item.get('PE_TTM'),
                revenue_growth=None,
                profit_growth=None,
                source=self.name,
                confidence_level=confidence_level
            )
            
        except Exception as e:
            logger.debug(f"è§£æå•æ¡è°ƒç ”æŠ¥å‘Šå¤±è´¥: {e}")
            return None
    
    def _parse_value_report(self, item: Dict, ticker: str) -> Optional[ResearchReport]:
        """è§£æå•æ¡ä»·å€¼åˆ†ææŠ¥å‘Š"""
        try:
            stock_name = item.get('SECURITY_NAME_ABBR', ticker)
            stock_code = item.get('SECURITY_CODE', ticker)
            
            trade_date = item.get('TRADE_DATE', '')
            close_price = item.get('CLOSE_PRICE')
            pe_ttm = item.get('PE_TTM')
            pb_mrq = item.get('PB_MRQ')
            market_cap = item.get('TOTAL_MARKET_CAP')
            
            title = f"{stock_name}({stock_code}) ä»·å€¼åˆ†ææŠ¥å‘Š - {trade_date}"
            
            # æ„å»ºæ‘˜è¦
            summary_parts = []
            if close_price:
                summary_parts.append(f"æ”¶ç›˜ä»·: {close_price}å…ƒ")
            if pe_ttm:
                summary_parts.append(f"å¸‚ç›ˆç‡(TTM): {pe_ttm}")
            if pb_mrq:
                summary_parts.append(f"å¸‚å‡€ç‡: {pb_mrq}")
            if market_cap:
                summary_parts.append(f"æ€»å¸‚å€¼: {market_cap:,.0f}å…ƒ")
            
            summary = "; ".join(summary_parts)
            
            key_points = []
            if pe_ttm:
                key_points.append(f"å½“å‰PE: {pe_ttm}")
            if pb_mrq:
                key_points.append(f"å½“å‰PB: {pb_mrq}")
            
            confidence_level = 0.7  # ä»·å€¼åˆ†ææ•°æ®å¯ä¿¡åº¦
            
            publish_date = self._format_date(trade_date)
            
            if not self._validate_report_data(title, "ä¸œæ–¹è´¢å¯Œæ•°æ®ä¸­å¿ƒ", publish_date):
                return None
            
            return ResearchReport(
                title=title,
                analyst="ä¸œæ–¹è´¢å¯Œ",
                institution="ä¸œæ–¹è´¢å¯Œæ•°æ®ä¸­å¿ƒ",
                publish_date=publish_date,
                rating="ä»·å€¼åˆ†æ",
                target_price=None,
                current_price=close_price,
                summary=summary,
                key_points=key_points,
                pe_forecast=pe_ttm,
                revenue_growth=None,
                profit_growth=None,
                source=self.name,
                confidence_level=confidence_level
            )
            
        except Exception as e:
            logger.debug(f"è§£æå•æ¡ä»·å€¼åˆ†ææŠ¥å‘Šå¤±è´¥: {e}")
            return None

    def _calculate_confidence_for_survey(self, item: Dict, institution: str) -> float:
        """è®¡ç®—è°ƒç ”æ•°æ®çš„å¯ä¿¡åº¦è¯„åˆ†"""
        base_score = 0.8  # æœºæ„è°ƒç ”åŸºç¡€å¯ä¿¡åº¦è¾ƒé«˜
        
        # çŸ¥åæœºæ„åŠ åˆ†
        prestigious_orgs = ['ä¸­ä¿¡è¯åˆ¸', 'åæ³°è¯åˆ¸', 'å›½æ³°å›å®‰', 'æµ·é€šè¯åˆ¸', 'å¹¿å‘è¯åˆ¸', 'æ‹›å•†è¯åˆ¸', 'ä¸­é‡‘å…¬å¸', 
                           'ç”³ä¸‡å®æº', 'å…´ä¸šè¯åˆ¸', 'ä¸œæ–¹è¯åˆ¸', 'æ–¹æ­£è¯åˆ¸', 'å›½ä¿¡è¯åˆ¸', 'å…‰å¤§è¯åˆ¸']
        if any(org in institution for org in prestigious_orgs):
            base_score += 0.1
        
        # æœ‰è°ƒç ”å†…å®¹åŠ åˆ†
        content = item.get('CONTENT', '')
        if content and len(content) > 50:
            base_score += 0.05
        
        # æœ‰è°ƒç ”å¯¹è±¡åŠ åˆ†
        receive_object = item.get('RECEIVE_OBJECT', '')
        if receive_object:
            base_score += 0.05
        
        return min(base_score, 1.0)
    
    def _validate_report_data(self, title: str, institution: str, publish_date: str) -> bool:
        """éªŒè¯ç ”æŠ¥æ•°æ®è´¨é‡"""
        # æ ‡é¢˜é•¿åº¦æ£€æŸ¥
        if not title or len(title) < 3:
            return False
        
        # æœºæ„åç§°æ£€æŸ¥
        if not institution or institution == 'æœªçŸ¥æœºæ„':
            return False
        
        # æ—¥æœŸæ£€æŸ¥
        if not publish_date:
            return False
        
        return True


class TongHuaShunResearchProvider(ResearchReportProvider):
    """åŒèŠ±é¡ºiFinDç ”æŠ¥æ•°æ®æä¾›å™¨"""
    
    def __init__(self):
        super().__init__()
        self.name = "åŒèŠ±é¡º"
        self.base_url = "http://data.10jqka.com.cn"
        
        # å¯¼å…¥é…ç½®å’Œé…é¢ç®¡ç†å™¨
        from tradingagents.config.tonghuashun_config import tonghuashun_config
        from tradingagents.utils.quota_manager import QuotaManager
        
        self.config = tonghuashun_config
        self.quota_manager = QuotaManager('tonghuashun', self.config.get_quota_config())
        self.is_logged_in = False
        self.ifind_available = False
        
        # å°è¯•åˆå§‹åŒ–iFinDè¿æ¥
        self._initialize_ifind()
    
    def _initialize_ifind(self):
        """åˆå§‹åŒ–iFinDè¿æ¥"""
        try:
            if not self.config.is_enabled():
                logger.info(f"ğŸ”§ åŒèŠ±é¡ºiFinDæœªé…ç½®æˆ–æœªå¯ç”¨ï¼Œä½¿ç”¨å¤‡ç”¨æ¨¡å¼")
                return False
            
            # å°è¯•å¯¼å…¥iFinD Pythonåº“
            try:
                global THS_iFinDLogin, THS_BasicData, THS_Trans2DataFrame
                from iFinDPy import THS_iFinDLogin, THS_BasicData, THS_Trans2DataFrame
                logger.info(f"âœ… æˆåŠŸå¯¼å…¥iFinD Pythonåº“")
                self.ifind_available = True
            except ImportError as e:
                logger.warning(f"âš ï¸ iFinD Pythonåº“æœªå®‰è£…: {e}")
                logger.info(f"ğŸ’¡ è¯·ä¸‹è½½å¹¶å®‰è£…åŒèŠ±é¡ºiFinD Python SDK: http://quantapi.10jqka.com.cn/")
                return False
            
            # å°è¯•ç™»å½•
            api_config = self.config.get_api_config()
            login_result = THS_iFinDLogin(api_config['username'], api_config['password'])
            
            if login_result.errorcode == 0:
                logger.info(f"âœ… åŒèŠ±é¡ºiFinDç™»å½•æˆåŠŸ")
                self.is_logged_in = True
                return True
            else:
                logger.warning(f"âš ï¸ åŒèŠ±é¡ºiFinDç™»å½•å¤±è´¥: {login_result.errmsg}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–åŒèŠ±é¡ºiFinDå¤±è´¥: {e}")
            return False
    
    def get_reports(self, ticker: str, limit: int = 10) -> List[ResearchReport]:
        """è·å–åŒèŠ±é¡ºç ”æŠ¥æ•°æ®"""
        try:
            logger.info(f"ğŸ” å¼€å§‹ä»åŒèŠ±é¡ºè·å–ç ”æŠ¥: {ticker}")
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨
            if not self.ifind_available or not self.is_logged_in:
                logger.warning(f"âš ï¸ åŒèŠ±é¡ºiFinDä¸å¯ç”¨ï¼Œè·³è¿‡è¯¥æ•°æ®æº: {ticker}")
                return []
            
            # æ£€æŸ¥é…é¢
            if not self.quota_manager.wait_if_needed():
                logger.warning(f"âš ï¸ åŒèŠ±é¡ºé…é¢ä¸è¶³ï¼Œè·³è¿‡è¯¥æ•°æ®æº: {ticker}")
                return []
            
            # è°ƒç”¨çœŸå®APIè·å–ç ”æŠ¥æ•°æ®
            reports = self._fetch_research_reports_from_ifind(ticker, limit)
            
            # è®°å½•è¯·æ±‚
            self.quota_manager.record_request()
            
            if reports:
                logger.info(f"âœ… ä»åŒèŠ±é¡ºè·å–åˆ° {len(reports)} æ¡ç ”æŠ¥: {ticker}")
            else:
                logger.info(f"â„¹ï¸ åŒèŠ±é¡ºæœªè·å–åˆ°ç ”æŠ¥æ•°æ®: {ticker}")
            
            return reports
            
        except Exception as e:
            logger.error(f"âŒ ä»åŒèŠ±é¡ºè·å–ç ”æŠ¥å¤±è´¥: {ticker}, é”™è¯¯: {str(e)}")
            return []
    
    def _fetch_research_reports_from_ifind(self, ticker: str, limit: int = 10) -> List[ResearchReport]:
        """ä»iFinD APIè·å–ç ”æŠ¥æ•°æ®"""
        try:
            # æ ¼å¼åŒ–è‚¡ç¥¨ä»£ç ç”¨äºiFinD
            formatted_ticker = self._format_ticker_for_ifind(ticker)
            if not formatted_ticker:
                logger.warning(f"âš ï¸ è‚¡ç¥¨ä»£ç æ ¼å¼åŒ–å¤±è´¥: {ticker}")
                return []
            
            # æ„å»ºiFinDæŸ¥è¯¢å‚æ•°
            indicators = ';'.join(self.config.research_report_fields)
            
            # è°ƒç”¨iFinDæ¥å£è·å–ç ”æŠ¥æ•°æ®
            result = THS_BasicData(formatted_ticker, indicators, 'unit:1')
            
            if result.errorcode != 0:
                logger.warning(f"âš ï¸ iFinD APIè°ƒç”¨å¤±è´¥: {result.errmsg}")
                return []
            
            # è½¬æ¢ä¸ºDataFrame
            df = THS_Trans2DataFrame(result)
            
            if df is None or df.empty:
                logger.warning(f"âš ï¸ iFinDè¿”å›ç©ºæ•°æ®: {ticker}")
                return []
            
            # è§£æç ”æŠ¥æ•°æ®
            reports = self._parse_ifind_research_data(df, ticker, limit)
            
            return reports
            
        except Exception as e:
            logger.error(f"âŒ iFinD APIè°ƒç”¨å¼‚å¸¸: {ticker}, é”™è¯¯: {str(e)}")
            return []
    
    def _format_ticker_for_ifind(self, ticker: str) -> Optional[str]:
        """ä¸ºiFinDæ ¼å¼åŒ–è‚¡ç¥¨ä»£ç """
        try:
            # æ¸…ç†è‚¡ç¥¨ä»£ç 
            clean_ticker = ticker.upper().replace('.SZ', '').replace('.SH', '').replace('.BJ', '').strip()
            
            if not clean_ticker.isdigit():
                return None
            
            # iFinDæ ¼å¼ï¼šè‚¡ç¥¨ä»£ç  + äº¤æ˜“æ‰€åç¼€
            if clean_ticker.startswith(('000', '002', '003', '300', '301')):
                return f"{clean_ticker}.SZ"  # æ·±äº¤æ‰€
            elif clean_ticker.startswith(('600', '601', '603', '605', '688', '689')):
                return f"{clean_ticker}.SH"  # ä¸Šäº¤æ‰€
            elif len(clean_ticker) == 8:  # åŒ—äº¤æ‰€
                return f"{clean_ticker}.BJ"
            else:
                return f"{clean_ticker}.SH"  # é»˜è®¤ä¸Šäº¤æ‰€
                
        except Exception as e:
            logger.warning(f"âš ï¸ è‚¡ç¥¨ä»£ç æ ¼å¼åŒ–å¼‚å¸¸: {ticker}, {e}")
            return None
    
    def _parse_ifind_research_data(self, df, ticker: str, limit: int) -> List[ResearchReport]:
        """è§£æiFinDç ”æŠ¥æ•°æ®"""
        try:
            reports = []
            
            # è·å–å­—æ®µæ˜ å°„
            field_mapping = {
                'ths_report_title_research': 'title',
                'ths_report_institution_research': 'institution', 
                'ths_report_analyst_research': 'analyst',
                'ths_report_publish_date_research': 'publish_date',
                'ths_rating_latest_research': 'rating',
                'ths_target_price_research': 'target_price',
                'ths_report_abstract_research': 'summary'
            }
            
            # å¤„ç†æ•°æ®è¡Œ
            for idx, row in df.head(limit).iterrows():
                try:
                    # æå–åŸºç¡€å­—æ®µ
                    report_data = {}
                    for ifind_field, standard_field in field_mapping.items():
                        if ifind_field in df.columns:
                            report_data[standard_field] = self._safe_get_ifind_value(row[ifind_field])
                    
                    # æ•°æ®è´¨é‡æ£€æŸ¥
                    if not self._validate_report_data(report_data):
                        continue
                    
                    # åˆ›å»ºç ”æŠ¥å¯¹è±¡
                    report = ResearchReport(
                        title=report_data.get('title', f'{ticker}ç ”æŠ¥'),
                        analyst=report_data.get('analyst', 'æœªçŸ¥åˆ†æå¸ˆ'),
                        institution=report_data.get('institution', 'æœªçŸ¥æœºæ„'),
                        publish_date=self._standardize_date(report_data.get('publish_date', '')),
                        rating=self._parse_rating(report_data.get('rating', '')),
                        target_price=self._extract_price(report_data.get('target_price', '')),
                        current_price=None,
                        summary=self._clean_summary(report_data.get('summary', '')),
                        key_points=self._extract_key_points_from_summary(report_data.get('summary', '')),
                        pe_forecast=None,
                        revenue_growth=None,
                        profit_growth=None,
                        source=self.name,
                        confidence_level=self.config.confidence_base_score
                    )
                    
                    reports.append(report)
                    
                except Exception as row_error:
                    logger.warning(f"âš ï¸ è§£æiFinDç ”æŠ¥è¡Œæ•°æ®å¤±è´¥: {row_error}")
                    continue
            
            logger.debug(f"ğŸ“Š iFinDç ”æŠ¥æ•°æ®è§£æå®Œæˆ: {ticker}, æœ‰æ•ˆæ•°æ® {len(reports)} æ¡")
            return reports
            
        except Exception as e:
            logger.error(f"âŒ è§£æiFinDç ”æŠ¥æ•°æ®å¤±è´¥: {ticker}, é”™è¯¯: {str(e)}")
            return []
    
    def _safe_get_ifind_value(self, value) -> str:
        """å®‰å…¨è·å–iFinDå­—æ®µå€¼"""
        if value is None or str(value).lower() in ['nan', 'none', 'null', '']:
            return ''
        return str(value).strip()
    
    def _validate_report_data(self, report_data: Dict[str, str]) -> bool:
        """éªŒè¯ç ”æŠ¥æ•°æ®è´¨é‡"""
        # æ£€æŸ¥æ ‡é¢˜é•¿åº¦
        title = report_data.get('title', '')
        if len(title) < self.config.min_report_title_length:
            return False
        
        # æ£€æŸ¥æœºæ„ä¿¡æ¯
        institution = report_data.get('institution', '')
        if not institution or institution == 'æœªçŸ¥æœºæ„':
            return False
        
        return True
    
    def _standardize_date(self, date_str: str) -> str:
        """æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼"""
        try:
            if not date_str:
                return '2024-08-04'
            
            from datetime import datetime
            # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
            date_formats = ['%Y-%m-%d', '%Y/%m/%d', '%Y%m%d']
            
            for fmt in date_formats:
                try:
                    date_obj = datetime.strptime(date_str, fmt)
                    return date_obj.strftime('%Y-%m-%d')
                except:
                    continue
            
            return date_str if len(date_str) >= 8 else '2024-08-04'
            
        except Exception:
            return '2024-08-04'
    
    def _clean_summary(self, summary: str) -> str:
        """æ¸…ç†ç ”æŠ¥æ‘˜è¦"""
        if not summary:
            return 'æš‚æ— æ‘˜è¦'
        
        # é™åˆ¶é•¿åº¦
        if len(summary) > self.config.max_report_summary_length:
            summary = summary[:self.config.max_report_summary_length] + "..."
        
        return summary.strip()
    
    def _extract_key_points_from_summary(self, summary: str) -> List[str]:
        """ä»æ‘˜è¦æå–å…³é”®è§‚ç‚¹"""
        if not summary or summary == 'æš‚æ— æ‘˜è¦':
            return []
        
        # ç®€å•çš„å…³é”®è¯æå–
        key_phrases = []
        keywords = ['æ¨è', 'ä¹°å…¥', 'å¢æŒ', 'å‡æŒ', 'å–å‡º', 'ç»´æŒ', 'ä¸Šè°ƒ', 'ä¸‹è°ƒ', 'ç›®æ ‡ä»·']
        
        sentences = summary.replace('ã€‚', '\n').replace('ï¼›', '\n').split('\n')
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 10 and any(keyword in sentence for keyword in keywords):
                key_phrases.append(sentence)
        
        return key_phrases[:3]  # æœ€å¤š3ä¸ªå…³é”®è§‚ç‚¹
    
    def _get_sample_tonghuashun_reports(self, ticker: str) -> List[Dict]:
        """è·å–ç¤ºä¾‹æ•°æ®"""
        return [
            {
                'title': f'{ticker} æŠ•èµ„ç­–ç•¥æŠ¥å‘Š',
                'analyst': 'ç‹äº”',
                'institution': 'å›½æ³°å›å®‰',
                'publish_date': '2024-07-28',
                'rating': 'å¼ºæ¨',
                'target_price_text': '48å…ƒ',
                'current_price': 38.5,
                'summary': 'å…¬å¸ä¸šåŠ¡æ¨¡å¼ä¼˜ç§€ï¼Œå¼ºçƒˆæ¨è',
                'key_points': ['ä¸šåŠ¡æ¨¡å¼åˆ›æ–°', 'å¸‚åœºä»½é¢æ‰©å¤§', 'ç›ˆåˆ©èƒ½åŠ›å¼º'],
                'pe_forecast': 14.8,
                'revenue_growth': 0.18,
                'profit_growth': 0.25
            }
        ]


class AKShareResearchProvider(ResearchReportProvider):
    """AKShareç ”æŠ¥æ•°æ®æä¾›å™¨"""
    
    def __init__(self):
        super().__init__()
        self.name = "AKShare"
    
    def get_reports(self, ticker: str, limit: int = 10) -> List[ResearchReport]:
        """ä½¿ç”¨AKShareè·å–ç ”æŠ¥æ•°æ®"""
        try:
            logger.info(f"ğŸ” å¼€å§‹ä»AKShareè·å–ç ”æŠ¥: {ticker}")
            
            # å°è¯•å¯¼å…¥AKShare
            try:
                import akshare as ak
                
                # è‚¡ç¥¨ä»£ç æ ¼å¼æ£€æŸ¥å’Œè½¬æ¢
                formatted_ticker = self._format_ticker_for_akshare(ticker)
                if not formatted_ticker:
                    logger.warning(f"âš ï¸ è‚¡ç¥¨ä»£ç æ ¼å¼éªŒè¯å¤±è´¥: {ticker}")
                    return []
                logger.debug(f"ğŸ”§ AKShareæ ¼å¼è½¬æ¢: {ticker} -> {formatted_ticker}")
                
                # è·å–ä¸ªè‚¡ç ”æŠ¥æ•°æ®ï¼Œä½¿ç”¨æ™ºèƒ½é‡è¯•æœºåˆ¶
                try:
                    research_df = self._call_akshare_with_retry(formatted_ticker)
                    
                    # æ£€æŸ¥è¿”å›æ•°æ®æ˜¯å¦æœ‰æ•ˆ
                    if research_df is None or research_df.empty:
                        logger.warning(f"âš ï¸ AKShareè¿”å›ç©ºæ•°æ®: {ticker}")
                        return []
                    
                    # æ•°æ®è´¨é‡éªŒè¯
                    validation_result = self._validate_research_data(research_df, ticker)
                    if not validation_result['is_valid']:
                        logger.warning(f"âš ï¸ AKShareæ•°æ®éªŒè¯å¤±è´¥: {ticker}, åŸå› : {validation_result['reason']}")
                        return []
                    
                    logger.info(f"âœ… AKShareæ•°æ®éªŒè¯é€šè¿‡: {ticker}, è¡Œæ•°: {len(research_df)}, åˆ—æ•°: {len(research_df.columns)}")
                    
                    # æ•°æ®æ¸…æ´—
                    research_df = self._clean_research_data(research_df)
                    logger.debug(f"ğŸ“‹ AKShareæ•°æ®æ¸…æ´—å®Œæˆ: {ticker}, æœ€ç»ˆè¡Œæ•°: {len(research_df)}")
                    
                    reports = []
                    
                    # å®‰å…¨åœ°éå†æ•°æ®ï¼Œå¢å¼ºå­—æ®µæå–
                    for idx, row in research_df.head(limit).iterrows():
                        try:
                            # åŸºç¡€å­—æ®µæå–
                            title = self._safe_get_value(row, ['æ ‡é¢˜', 'title', 'ç ”æŠ¥æ ‡é¢˜', 'æŠ¥å‘Šæ ‡é¢˜'], f'{ticker}ç ”æŠ¥')
                            analyst = self._safe_get_value(row, ['åˆ†æå¸ˆ', 'analyst', 'ç ”ç©¶å‘˜'], 'æœªçŸ¥åˆ†æå¸ˆ')
                            institution = self._safe_get_value(row, ['æœºæ„', 'institution', 'åˆ¸å•†', 'ç ”ç©¶æœºæ„'], 'æœªçŸ¥æœºæ„')
                            publish_date = self._safe_get_value(row, ['å‘å¸ƒæ—¥æœŸ', 'date', 'æ—¥æœŸ', 'å‘å¸ƒæ—¶é—´'], '2024-08-04')
                            rating_text = self._safe_get_value(row, ['è¯„çº§', 'rating', 'æŠ•èµ„è¯„çº§', 'ç ”æŠ¥è¯„çº§'], 'æœªçŸ¥')
                            target_price_text = self._safe_get_value(row, ['ç›®æ ‡ä»·', 'target_price', 'ç›®æ ‡ä»·æ ¼'], '')
                            summary = self._safe_get_value(row, ['æ‘˜è¦', 'summary', 'å†…å®¹', 'ç ”æŠ¥æ‘˜è¦', 'æ ¸å¿ƒè§‚ç‚¹'], 'æš‚æ— æ‘˜è¦')
                            
                            # å¢å¼ºå­—æ®µæå–
                            report_type = self._safe_get_value(row, ['æŠ¥å‘Šç±»å‹', 'report_type', 'ç ”æŠ¥ç±»å‹'], 'æ·±åº¦ç ”ç©¶')
                            page_count = self._safe_get_value(row, ['é¡µæ•°', 'pages', 'æŠ¥å‘Šé¡µæ•°'], '')
                            report_url = self._safe_get_value(row, ['é“¾æ¥', 'url', 'link', 'pdfUrl'], '')
                            
                            # è´¢åŠ¡é¢„æµ‹å­—æ®µæå–
                            pe_ratio = self._extract_number(self._safe_get_value(row, ['PE', 'pe', 'å¸‚ç›ˆç‡'], ''))
                            pb_ratio = self._extract_number(self._safe_get_value(row, ['PB', 'pb', 'å¸‚å‡€ç‡'], ''))
                            revenue_forecast = self._extract_number(self._safe_get_value(row, ['è¥æ”¶é¢„æµ‹', 'revenue_forecast'], ''))
                            profit_forecast = self._extract_number(self._safe_get_value(row, ['åˆ©æ¶¦é¢„æµ‹', 'profit_forecast'], ''))
                            
                            # å…³é”®è§‚ç‚¹æå–
                            key_points = self._extract_key_points(summary, title)
                            
                            # æ‘˜è¦ä¼˜åŒ–å¤„ç†
                            if len(summary) > 300:
                                summary = summary[:300] + "..."
                            
                            report = ResearchReport(
                                title=title,  
                                analyst=analyst,
                                institution=institution,
                                publish_date=publish_date,
                                rating=self._parse_rating(rating_text),
                                target_price=self._extract_price(target_price_text),
                                current_price=None,
                                summary=summary,
                                key_points=key_points,
                                pe_forecast=pe_ratio,
                                revenue_growth=revenue_forecast,
                                profit_growth=profit_forecast,
                                source=self.name,
                                confidence_level=self._calculate_confidence_level(institution, analyst, publish_date)
                            )
                            reports.append(report)
                            
                        except Exception as row_error:
                            logger.warning(f"âš ï¸ å¤„ç†å•è¡Œæ•°æ®å¤±è´¥: {row_error}, è·³è¿‡è¯¥è¡Œ")
                            continue
                    
                    if reports:
                        logger.info(f"âœ… ä»AKShareè·å–åˆ° {len(reports)} æ¡ç ”æŠ¥: {ticker}")
                        return reports
                    else:
                        logger.warning(f"âš ï¸ AKShareæ•°æ®å¤„ç†åä¸ºç©º: {ticker}")
                        return []
                
                except Exception as api_error:
                    # AKShare APIè°ƒç”¨å¤±è´¥çš„è¯¦ç»†é”™è¯¯å¤„ç†
                    error_msg = str(api_error)
                    if 'infoCode' in error_msg:
                        logger.warning(f"âš ï¸ AKShare APIè¿”å›é”™è¯¯ä»£ç : {ticker}, å°è¯•æ›¿ä»£æ–¹æ¡ˆ")
                    elif 'timeout' in error_msg.lower():
                        logger.warning(f"âš ï¸ AKShare APIè¶…æ—¶: {ticker}, å°è¯•æ›¿ä»£æ–¹æ¡ˆ")
                    elif 'connection' in error_msg.lower():
                        logger.warning(f"âš ï¸ AKShareç½‘ç»œè¿æ¥å¤±è´¥: {ticker}, å°è¯•æ›¿ä»£æ–¹æ¡ˆ")
                    else:
                        logger.warning(f"âš ï¸ AKShare APIè°ƒç”¨å¤±è´¥: {ticker}, é”™è¯¯: {error_msg}, å°è¯•æ›¿ä»£æ–¹æ¡ˆ")
                    
                    # å¯¹äºç ”æŠ¥æ•°æ®ï¼Œä¸ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨
                    # è¿™ç¡®ä¿äº†æ•°æ®çš„çœŸå®æ€§å’Œå‡†ç¡®æ€§
                    logger.warning(f"âš ï¸ AKShareç ”æŠ¥APIå¤±è´¥ï¼Œä¸ä½¿ç”¨æ›¿ä»£æ•°æ®: {ticker}")
                    return []
                
            except ImportError:
                logger.warning("âš ï¸ AKShareæœªå®‰è£…ï¼Œè·³è¿‡AKShareæ•°æ®æº")
                return []
                
        except Exception as e:
            logger.error(f"âŒ ä»AKShareè·å–ç ”æŠ¥å¤±è´¥: {ticker}, é”™è¯¯: {str(e)}")
            # å‡ºé”™æ—¶è¿”å›ç©ºåˆ—è¡¨ï¼Œä¸ä½¿ç”¨ç¤ºä¾‹æ•°æ®
            return []
    
    def _format_ticker_for_akshare(self, ticker: str) -> Optional[str]:
        """ä¸ºAKShareæ ¼å¼åŒ–å’ŒéªŒè¯è‚¡ç¥¨ä»£ç ï¼Œæ”¯æŒæ‰€æœ‰ä¸­å›½è‚¡ç¥¨å¸‚åœº"""
        if not ticker:
            return None
            
        # æ¸…ç†è‚¡ç¥¨ä»£ç ï¼Œæ”¯æŒæ›´å¤šåç¼€æ ¼å¼
        clean_ticker = ticker.upper().replace('.SZ', '').replace('.SH', '').replace('.BJ', '').replace('.sz', '').replace('.sh', '').replace('.bj', '').strip()
        
        # ç¡®ä¿æ˜¯çº¯æ•°å­—
        if not clean_ticker.isdigit():
            logger.warning(f"âš ï¸ è‚¡ç¥¨ä»£ç åŒ…å«éæ•°å­—å­—ç¬¦: {ticker}")
            return None
        
        # é•¿åº¦æ£€æŸ¥å’Œæ ‡å‡†åŒ– - æ”¯æŒ6ä½Aè‚¡å’Œ8ä½åŒ—äº¤æ‰€
        if len(clean_ticker) > 8:
            logger.warning(f"âš ï¸ è‚¡ç¥¨ä»£ç é•¿åº¦è¶…è¿‡8ä½: {ticker}")
            return None
        elif len(clean_ticker) == 6:
            # æ ‡å‡†6ä½Aè‚¡ä»£ç 
            pass
        elif len(clean_ticker) == 8:
            # 8ä½åŒ—äº¤æ‰€ä»£ç 
            pass
        elif len(clean_ticker) < 6:
            # è¡¥é½åˆ°6ä½
            clean_ticker = clean_ticker.zfill(6)
        else:
            logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„è‚¡ç¥¨ä»£ç é•¿åº¦: {ticker} (é•¿åº¦: {len(clean_ticker)})")
            return None
        
        # å¸‚åœºåˆ†ç±»éªŒè¯
        board_info = self._classify_stock_board(clean_ticker)
        if not board_info['is_valid']:
            logger.warning(f"âš ï¸ æ— æ³•è¯†åˆ«çš„è‚¡ç¥¨ä»£ç æ ¼å¼: {ticker} -> {clean_ticker}")
            return None
        
        logger.info(f"âœ… è‚¡ç¥¨ä»£ç æ ¼å¼åŒ–æˆåŠŸ: {ticker} -> {clean_ticker} ({board_info['board_name']})")
        return clean_ticker
    
    def _classify_stock_board(self, ticker: str) -> Dict[str, Any]:
        """ç»¼åˆåˆ†ç±»è¯†åˆ«ä¸­å›½è‚¡ç¥¨å¸‚åœºæ¿å—ï¼Œæ”¯æŒæ‰€æœ‰äº¤æ˜“æ‰€"""
        ticker_len = len(ticker)
        
        # å¤„ç†6ä½æ ‡å‡†Aè‚¡ä»£ç 
        if ticker_len == 6:
            first_char = ticker[0]
            first_three = ticker[:3]
            
            # ä¸Šæµ·äº¤æ˜“æ‰€
            if first_char == '6':
                if first_three in ['600', '601', '603', '605']:
                    return {
                        'is_valid': True, 
                        'board_name': 'ä¸Šæµ·ä¸»æ¿', 
                        'exchange': 'SH',
                        'board_code': 'SH_MAIN',
                        'special_handling': False
                    }
                elif first_three == '688':
                    return {
                        'is_valid': True, 
                        'board_name': 'ç§‘åˆ›æ¿', 
                        'exchange': 'SH',
                        'board_code': 'SH_STAR',
                        'special_handling': True,  # ç§‘åˆ›æ¿éœ€è¦ç‰¹æ®Šå¤„ç†
                        'notes': 'ç§‘åˆ›æ¿è‚¡ç¥¨ï¼Œæ³¨å†Œåˆ¶ï¼Œé£é™©è¾ƒé«˜'
                    }
                elif first_three in ['689']:
                    return {
                        'is_valid': True, 
                        'board_name': 'ç§‘åˆ›æ¿', 
                        'exchange': 'SH',
                        'board_code': 'SH_STAR',
                        'special_handling': True,
                        'notes': 'ç§‘åˆ›æ¿é¢„ç•™å·æ®µ'
                    }
                else:
                    return {
                        'is_valid': True, 
                        'board_name': 'ä¸Šæµ·å…¶ä»–', 
                        'exchange': 'SH',
                        'board_code': 'SH_OTHER',
                        'special_handling': False
                    }
            
            # æ·±åœ³äº¤æ˜“æ‰€  
            elif first_char == '0':
                if first_three in ['000', '001']:
                    return {
                        'is_valid': True, 
                        'board_name': 'æ·±åœ³ä¸»æ¿', 
                        'exchange': 'SZ',
                        'board_code': 'SZ_MAIN',
                        'special_handling': False
                    }
                elif first_three in ['002', '003']:
                    return {
                        'is_valid': True, 
                        'board_name': 'ä¸­å°æ¿', 
                        'exchange': 'SZ',
                        'board_code': 'SZ_SME',
                        'special_handling': False,
                        'notes': 'ä¸­å°æ¿å·²å¹¶å…¥ä¸»æ¿'
                    }
                else:
                    return {
                        'is_valid': True, 
                        'board_name': 'æ·±åœ³å…¶ä»–', 
                        'exchange': 'SZ',
                        'board_code': 'SZ_OTHER',
                        'special_handling': False
                    }
            
            # åˆ›ä¸šæ¿
            elif first_char == '3':
                if first_three == '300':
                    return {
                        'is_valid': True, 
                        'board_name': 'åˆ›ä¸šæ¿', 
                        'exchange': 'SZ',
                        'board_code': 'SZ_GEM',
                        'special_handling': True,  # åˆ›ä¸šæ¿éœ€è¦ç‰¹æ®Šæƒé™
                        'notes': 'åˆ›ä¸šæ¿ï¼Œéœ€è¦æŠ•èµ„è€…é€‚å½“æ€§ç®¡ç†'
                    }
                elif first_three == '301':
                    return {
                        'is_valid': True, 
                        'board_name': 'åˆ›ä¸šæ¿', 
                        'exchange': 'SZ',
                        'board_code': 'SZ_GEM',
                        'special_handling': True,
                        'notes': 'åˆ›ä¸šæ¿æ³¨å†Œåˆ¶æ–°è‚¡'
                    }
                else:
                    return {
                        'is_valid': True, 
                        'board_name': 'æ·±åœ³å…¶ä»–', 
                        'exchange': 'SZ',
                        'board_code': 'SZ_OTHER',
                        'special_handling': False
                    }
            
            # åŒ—äº¬äº¤æ˜“æ‰€ç›¸å…³ï¼ˆéƒ¨åˆ†6ä½ä»£ç ï¼‰
            elif first_char in ['4', '8', '9']:
                if first_three in ['430', '831', '832', '833', '834', '835', '836', '837', '838', '839']:
                    return {
                        'is_valid': True, 
                        'board_name': 'åŒ—äº¤æ‰€/æ–°ä¸‰æ¿', 
                        'exchange': 'BJ',
                        'board_code': 'BJ_NEEQ',
                        'special_handling': True,
                        'notes': 'åŒ—äº¤æ‰€æˆ–æ–°ä¸‰æ¿ï¼ŒæµåŠ¨æ€§ç›¸å¯¹è¾ƒä½'
                    }
                else:
                    return {
                        'is_valid': True, 
                        'board_name': 'å…¶ä»–å¸‚åœº', 
                        'exchange': 'OTHER',
                        'board_code': 'OTHER',
                        'special_handling': True
                    }
                    
            # æ¸¯è‚¡é€šä»£ç ï¼ˆéƒ¨åˆ†ä»¥9å¼€å¤´ï¼‰
            elif first_char == '9':
                if first_three in ['900', '901', '902']:
                    return {
                        'is_valid': True, 
                        'board_name': 'Bè‚¡', 
                        'exchange': 'SH',
                        'board_code': 'SH_B',
                        'special_handling': True,
                        'notes': 'Bè‚¡ï¼Œå¤–å¸äº¤æ˜“'
                    }
                else:
                    return {
                        'is_valid': True, 
                        'board_name': 'å…¶ä»–', 
                        'exchange': 'OTHER',
                        'board_code': 'OTHER',
                        'special_handling': True
                    }
            
            else:
                # å…¶ä»–æœªè¯†åˆ«çš„6ä½ä»£ç 
                return {
                    'is_valid': False, 
                    'board_name': 'æœªçŸ¥', 
                    'error': f'æ— æ³•è¯†åˆ«çš„6ä½è‚¡ç¥¨ä»£ç : {ticker}'
                }
        
        # å¤„ç†8ä½åŒ—äº¤æ‰€ä»£ç 
        elif ticker_len == 8:
            first_two = ticker[:2]
            
            if first_two in ['43', '83', '87', '88']:
                return {
                    'is_valid': True, 
                    'board_name': 'åŒ—äº¤æ‰€', 
                    'exchange': 'BJ',
                    'board_code': 'BJ_MAIN',
                    'special_handling': True,
                    'notes': 'åŒ—äº¤æ‰€ä¸»æ¿ï¼Œ8ä½ä»£ç ï¼Œæ³¨å†Œåˆ¶'
                }
            else:
                return {
                    'is_valid': True, 
                    'board_name': 'åŒ—äº¤æ‰€å…¶ä»–', 
                    'exchange': 'BJ',
                    'board_code': 'BJ_OTHER',
                    'special_handling': True,
                    'notes': 'åŒ—äº¤æ‰€ç›¸å…³8ä½ä»£ç '
                }
                
        # å¤„ç†å…¶ä»–é•¿åº¦ä»£ç 
        else:
            return {
                'is_valid': False, 
                'board_name': 'æ— æ•ˆé•¿åº¦', 
                'error': f'ä¸æ”¯æŒçš„è‚¡ç¥¨ä»£ç é•¿åº¦: {ticker_len} ä½'
            }
    
    def _call_akshare_with_retry(self, ticker: str, max_retries: int = 3) -> Optional[Any]:
        """å¸¦æ™ºèƒ½é‡è¯•çš„AKShareè°ƒç”¨ï¼Œæ”¯æŒç‰¹æ®Šæ¿å—å¤„ç†"""
        import time
        import akshare as ak
        
        # è·å–è‚¡ç¥¨æ¿å—ä¿¡æ¯ç”¨äºç‰¹æ®Šå¤„ç†
        board_info = self._classify_stock_board(ticker)
        
        # é’ˆå¯¹ç‰¹æ®Šæ¿å—è°ƒæ•´é‡è¯•ç­–ç•¥
        if board_info.get('special_handling'):
            max_retries = max_retries + 1  # ç‰¹æ®Šæ¿å—å¤šé‡è¯•ä¸€æ¬¡
            logger.info(f"ğŸ¯ ç‰¹æ®Šæ¿å—è‚¡ç¥¨ï¼Œä½¿ç”¨å¢å¼ºé‡è¯•ç­–ç•¥: {ticker} ({board_info['board_name']})")
        
        for attempt in range(max_retries):
            try:
                # é€’å¢å»¶æ—¶ç­–ç•¥ï¼Œç‰¹æ®Šæ¿å—ä½¿ç”¨æ›´é•¿å»¶æ—¶
                if attempt > 0:
                    base_delay = 0.5 * (2 ** (attempt - 1))  # æŒ‡æ•°é€€é¿
                    if board_info.get('special_handling'):
                        delay = base_delay * 1.5  # ç‰¹æ®Šæ¿å—å»¶æ—¶æ›´é•¿
                    else:
                        delay = base_delay
                    
                    logger.debug(f"ğŸ”„ é‡è¯•å‰ç­‰å¾… {delay:.1f} ç§’: {ticker}")
                    time.sleep(delay)
                
                logger.debug(f"ğŸ” å°è¯•è°ƒç”¨AKShare (ç¬¬{attempt + 1}æ¬¡): {ticker}")
                
                # é’ˆå¯¹ç‰¹æ®Šæ¿å—çš„APIè°ƒç”¨é€‚é…
                if board_info.get('board_code') == 'SH_STAR':
                    # ç§‘åˆ›æ¿ç‰¹æ®Šå¤„ç†
                    logger.debug(f"ğŸš€ ç§‘åˆ›æ¿è‚¡ç¥¨ç‰¹æ®Šå¤„ç†: {ticker}")
                elif board_info.get('board_code') in ['BJ_MAIN', 'BJ_NEEQ', 'BJ_OTHER']:
                    # åŒ—äº¤æ‰€ç‰¹æ®Šå¤„ç†
                    logger.debug(f"ğŸ¢ åŒ—äº¤æ‰€è‚¡ç¥¨ç‰¹æ®Šå¤„ç†: {ticker}")
                elif board_info.get('board_code') == 'SZ_GEM':
                    # åˆ›ä¸šæ¿ç‰¹æ®Šå¤„ç†
                    logger.debug(f"ğŸ’ åˆ›ä¸šæ¿è‚¡ç¥¨ç‰¹æ®Šå¤„ç†: {ticker}")
                
                # ç»Ÿä¸€APIè°ƒç”¨
                result = ak.stock_research_report_em(symbol=ticker)
                
                # æˆåŠŸè·å–æ•°æ®
                if result is not None:
                    logger.debug(f"âœ… AKShareè°ƒç”¨æˆåŠŸ (ç¬¬{attempt + 1}æ¬¡): {ticker}")
                    return result
                else:
                    logger.warning(f"âš ï¸ AKShareè¿”å›None (ç¬¬{attempt + 1}æ¬¡): {ticker}")
                    
            except Exception as e:
                error_msg = str(e).lower()
                
                # åˆ†æé”™è¯¯ç±»å‹ï¼Œè€ƒè™‘ç‰¹æ®Šæ¿å—
                error_type = self._classify_akshare_error_enhanced(error_msg, board_info)
                
                if error_type == 'permanent':
                    # æ°¸ä¹…æ€§é”™è¯¯ï¼Œä¸é‡è¯•
                    logger.warning(f"âš ï¸ æ°¸ä¹…æ€§é”™è¯¯ï¼Œåœæ­¢é‡è¯•: {ticker}, é”™è¯¯: {e}")
                    return None
                elif error_type == 'board_specific':
                    # æ¿å—ç‰¹å®šé”™è¯¯ï¼Œç»™å‡ºå…·ä½“å»ºè®®
                    logger.warning(f"âš ï¸ {board_info['board_name']}ç‰¹å®šé”™è¯¯: {ticker}, {e}")
                    if attempt < max_retries - 1:
                        continue
                    else:
                        return None
                elif error_type == 'temporary' and attempt < max_retries - 1:
                    # ä¸´æ—¶æ€§é”™è¯¯ï¼Œç»§ç»­é‡è¯•
                    logger.warning(f"âš ï¸ ä¸´æ—¶æ€§é”™è¯¯ï¼Œå‡†å¤‡é‡è¯• (ç¬¬{attempt + 1}æ¬¡): {ticker}, é”™è¯¯: {e}")
                    continue
                else:
                    # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°æˆ–å…¶ä»–é”™è¯¯
                    logger.error(f"âŒ AKShareè°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {ticker}, é”™è¯¯: {e}")
                    return None
        
        logger.warning(f"âš ï¸ AKShareé‡è¯•å…¨éƒ¨å¤±è´¥: {ticker} ({board_info.get('board_name', 'æœªçŸ¥æ¿å—')})")
        return None
    
    def _classify_akshare_error(self, error_msg: str) -> str:
        """åˆ†ç±»AKShareé”™è¯¯ç±»å‹ï¼ˆåŸºç¡€ç‰ˆæœ¬ï¼‰"""
        # æ°¸ä¹…æ€§é”™è¯¯ï¼ˆä¸éœ€è¦é‡è¯•ï¼‰
        permanent_indicators = [
            'infocode',  # è‚¡ç¥¨ä»£ç ä¸å­˜åœ¨æˆ–æ— æƒé™
            'not found', # æ•°æ®ä¸å­˜åœ¨
            'no data',   # æ— æ•°æ®
            'invalid symbol', # æ— æ•ˆä»£ç 
            'invalid parameter' # æ— æ•ˆå‚æ•°
        ]
        
        # ä¸´æ—¶æ€§é”™è¯¯ï¼ˆå¯ä»¥é‡è¯•ï¼‰
        temporary_indicators = [
            'timeout',     # è¶…æ—¶
            'connection',  # è¿æ¥é”™è¯¯
            'network',     # ç½‘ç»œé”™è¯¯
            'server',      # æœåŠ¡å™¨é”™è¯¯
            'rate limit',  # é¢‘ç‡é™åˆ¶
            'too many',    # è¯·æ±‚è¿‡å¤š
            'busy',        # æœåŠ¡ç¹å¿™
            'temporarily', # ä¸´æ—¶é”™è¯¯
            'retry'        # å»ºè®®é‡è¯•
        ]
        
        error_lower = error_msg.lower()
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ°¸ä¹…æ€§é”™è¯¯
        if any(indicator in error_lower for indicator in permanent_indicators):
            return 'permanent'
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºä¸´æ—¶æ€§é”™è¯¯
        if any(indicator in error_lower for indicator in temporary_indicators):
            return 'temporary'
        
        # é»˜è®¤å½“ä½œä¸´æ—¶æ€§é”™è¯¯ï¼Œç»™ä¸€æ¬¡é‡è¯•æœºä¼š
        return 'temporary'
    
    def _classify_akshare_error_enhanced(self, error_msg: str, board_info: Dict[str, Any]) -> str:
        """å¢å¼ºç‰ˆAKShareé”™è¯¯åˆ†ç±»ï¼Œè€ƒè™‘ç‰¹æ®Šæ¿å—ç‰¹æ€§"""
        error_lower = error_msg.lower()
        board_code = board_info.get('board_code', '')
        board_name = board_info.get('board_name', 'æœªçŸ¥')
        
        # ç§‘åˆ›æ¿ç‰¹å®šé”™è¯¯
        if board_code == 'SH_STAR':
            if 'infocode' in error_lower:
                logger.info(f"ğŸš€ ç§‘åˆ›æ¿è‚¡ç¥¨å¯èƒ½ç ”æŠ¥è¦†ç›–è¾ƒå°‘ï¼Œè¿™æ˜¯æ­£å¸¸æƒ…å†µ")
                return 'permanent'
            elif any(indicator in error_lower for indicator in ['688', 'kcb', 'star']):
                return 'board_specific'
                
        # åŒ—äº¤æ‰€ç‰¹å®šé”™è¯¯
        elif board_code in ['BJ_MAIN', 'BJ_NEEQ', 'BJ_OTHER']:
            if 'infocode' in error_lower:
                logger.info(f"ğŸ¢ åŒ—äº¤æ‰€è‚¡ç¥¨ç ”æŠ¥è¦†ç›–æœ‰é™ï¼Œè¿™æ˜¯å¸¸è§æƒ…å†µ")
                return 'permanent'
            elif any(indicator in error_lower for indicator in ['bj', 'neeq', '43', '83']):
                return 'board_specific'
                
        # åˆ›ä¸šæ¿ç‰¹å®šé”™è¯¯
        elif board_code == 'SZ_GEM':
            if 'infocode' in error_lower:
                logger.info(f"ğŸ’ åˆ›ä¸šæ¿è‚¡ç¥¨ï¼Œå¯èƒ½éœ€è¦ç‰¹æ®Šæƒé™æˆ–ç ”æŠ¥è¾ƒå°‘")
                return 'permanent'
            elif any(indicator in error_lower for indicator in ['300', '301', 'gem']):
                return 'board_specific'
        
        # é€šç”¨é”™è¯¯å¤„ç†
        # æ°¸ä¹…æ€§é”™è¯¯
        permanent_indicators = [
            'infocode',  # è‚¡ç¥¨ä»£ç ä¸å­˜åœ¨æˆ–æ— æƒé™
            'not found', # æ•°æ®ä¸å­˜åœ¨
            'no data',   # æ— æ•°æ®
            'invalid symbol', # æ— æ•ˆä»£ç 
            'invalid parameter', # æ— æ•ˆå‚æ•°
            'access denied', # è®¿é—®æ‹’ç»
            'unauthorized' # æœªæˆæƒ
        ]
        
        # ä¸´æ—¶æ€§é”™è¯¯
        temporary_indicators = [
            'timeout',     # è¶…æ—¶
            'connection',  # è¿æ¥é”™è¯¯
            'network',     # ç½‘ç»œé”™è¯¯
            'server',      # æœåŠ¡å™¨é”™è¯¯
            'rate limit',  # é¢‘ç‡é™åˆ¶
            'too many',    # è¯·æ±‚è¿‡å¤š
            'busy',        # æœåŠ¡ç¹å¿™
            'temporarily', # ä¸´æ—¶é”™è¯¯
            'retry',       # å»ºè®®é‡è¯•
            'service unavailable', # æœåŠ¡ä¸å¯ç”¨
            '502', '503', '504'  # HTTPé”™è¯¯ç 
        ]
        
        # æ£€æŸ¥é”™è¯¯ç±»å‹
        if any(indicator in error_lower for indicator in permanent_indicators):
            return 'permanent'
        elif any(indicator in error_lower for indicator in temporary_indicators):
            return 'temporary'
        else:
            # å¯¹äºç‰¹æ®Šæ¿å—ï¼Œé»˜è®¤æ›´ä¿å®ˆ
            if board_info.get('special_handling'):
                return 'board_specific'
            else:
                return 'temporary'
    
    def _log_structured_error(self, ticker: str, error: Exception, context: str, 
                             error_type: str = 'unknown', recoverable: bool = False):
        """ç»“æ„åŒ–é”™è¯¯è®°å½•"""
        error_info = {
            'ticker': ticker,
            'context': context,
            'error_type': error_type,
            'error_message': str(error),
            'recoverable': recoverable,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # æ ¹æ®é”™è¯¯ä¸¥é‡ç¨‹åº¦é€‰æ‹©æ—¥å¿—çº§åˆ«
        if error_type == 'permanent':
            logger.warning(f"ğŸš« [{context}] æ°¸ä¹…æ€§é”™è¯¯: {ticker} - {error}")
        elif error_type == 'temporary':
            logger.info(f"â³ [{context}] ä¸´æ—¶æ€§é”™è¯¯: {ticker} - {error}")
        elif error_type == 'validation':
            logger.warning(f"ğŸ“‹ [{context}] éªŒè¯é”™è¯¯: {ticker} - {error}")
        elif error_type == 'network':
            logger.info(f"ğŸŒ [{context}] ç½‘ç»œé”™è¯¯: {ticker} - {error}")
        else:
            logger.error(f"âŒ [{context}] æœªçŸ¥é”™è¯¯: {ticker} - {error}")
    
    def _handle_akshare_error(self, ticker: str, error: Exception) -> tuple[bool, str]:
        """ç»Ÿä¸€å¤„ç†AKShareé”™è¯¯
        
        Returns:
            tuple: (should_retry, error_type)
        """
        error_msg = str(error).lower()
        
        # åˆ†æå…·ä½“é”™è¯¯ç±»å‹
        if 'infocode' in error_msg:
            self._log_structured_error(ticker, error, 'AKShareç ”æŠ¥API', 'permanent', False)
            return False, 'infoCodeé”™è¯¯ - è‚¡ç¥¨ä»£ç å¯èƒ½ä¸å­˜åœ¨æˆ–æ— æƒé™'
            
        elif any(indicator in error_msg for indicator in ['timeout', 'timed out']):
            self._log_structured_error(ticker, error, 'AKShareç ”æŠ¥API', 'network', True)
            return True, 'ç½‘ç»œè¶…æ—¶ - å¯é‡è¯•'
            
        elif any(indicator in error_msg for indicator in ['connection', 'network']):
            self._log_structured_error(ticker, error, 'AKShareç ”æŠ¥API', 'network', True)
            return True, 'ç½‘ç»œè¿æ¥å¤±è´¥ - å¯é‡è¯•'
            
        elif any(indicator in error_msg for indicator in ['rate limit', 'too many', 'busy']):
            self._log_structured_error(ticker, error, 'AKShareç ”æŠ¥API', 'temporary', True)
            return True, 'è®¿é—®é¢‘ç‡é™åˆ¶ - å¯é‡è¯•'
            
        elif 'not found' in error_msg or 'no data' in error_msg:
            self._log_structured_error(ticker, error, 'AKShareç ”æŠ¥API', 'permanent', False)
            return False, 'æ•°æ®ä¸å­˜åœ¨ - è¯¥è‚¡ç¥¨å¯èƒ½æ— ç ”æŠ¥æ•°æ®'
            
        else:
            self._log_structured_error(ticker, error, 'AKShareç ”æŠ¥API', 'unknown', True)
            return True, f'æœªçŸ¥é”™è¯¯ - {error_msg[:100]}'
    
    def _get_error_suggestions(self, ticker: str, error_type: str, board_info: Dict[str, Any] = None) -> List[str]:
        """æ ¹æ®é”™è¯¯ç±»å‹å’Œæ¿å—ä¿¡æ¯æä¾›è§£å†³å»ºè®®"""
        suggestions = []
        board_name = board_info.get('board_name', 'æœªçŸ¥') if board_info else 'æœªçŸ¥'
        board_code = board_info.get('board_code', '') if board_info else ''
        
        if error_type.startswith('infoCode'):
            suggestions.extend([
                f"æ£€æŸ¥è‚¡ç¥¨ä»£ç  {ticker} æ˜¯å¦æ­£ç¡®",
                "ç¡®è®¤è¯¥è‚¡ç¥¨æ˜¯å¦æœ‰ç ”æŠ¥è¦†ç›–"
            ])
            
            # æ ¹æ®æ¿å—æä¾›ç‰¹å®šå»ºè®®
            if board_code == 'SH_STAR':
                suggestions.extend([
                    "ç§‘åˆ›æ¿è‚¡ç¥¨ç ”æŠ¥è¦†ç›–ç›¸å¯¹è¾ƒå°‘ï¼Œè¿™æ˜¯æ­£å¸¸æƒ…å†µ",
                    "ç§‘åˆ›æ¿å…¬å¸å¤šä¸ºç§‘æŠ€åˆ›æ–°ä¼ä¸šï¼Œå¯å…³æ³¨å…¬å¸å…¬å‘Šå’Œè´¢æŠ¥",
                    "å»ºè®®é‡ç‚¹åˆ†æåŸºæœ¬é¢å’Œè¡Œä¸šå‘å±•è¶‹åŠ¿"
                ])
            elif board_code in ['BJ_MAIN', 'BJ_NEEQ', 'BJ_OTHER']:
                suggestions.extend([
                    "åŒ—äº¤æ‰€è‚¡ç¥¨ç ”æŠ¥è¦†ç›–æœ‰é™ï¼ŒæµåŠ¨æ€§ç›¸å¯¹è¾ƒä½",
                    "å»ºè®®å…³æ³¨å…¬å¸å®šæœŸæŠ¥å‘Šå’Œå…¬å‘Šä¿¡æ¯",
                    "å¯é‡ç‚¹åˆ†æå…¬å¸æˆé•¿æ€§å’Œä¸šåŠ¡æ¨¡å¼"
                ])
            elif board_code == 'SZ_GEM':
                suggestions.extend([
                    "åˆ›ä¸šæ¿è‚¡ç¥¨æ³¢åŠ¨è¾ƒå¤§ï¼Œç ”æŠ¥è¦†ç›–å­˜åœ¨å·®å¼‚",
                    "å»ºè®®ç»“åˆè¡Œä¸šåˆ†æå’Œå…¬å¸æˆé•¿æ€§è¯„ä¼°",
                    "å…³æ³¨å…¬å¸åˆ›æ–°èƒ½åŠ›å’Œå¸‚åœºå‰æ™¯"
                ])
            else:
                suggestions.append("å°è¯•ä½¿ç”¨å…¶ä»–æ•°æ®æº")
            
        elif 'ç½‘ç»œ' in error_type:
            suggestions.extend([
                "æ£€æŸ¥ç½‘ç»œè¿æ¥",
                "ç¨åé‡è¯•",
                "ä½¿ç”¨VPNæˆ–æ›´æ¢ç½‘ç»œ"
            ])
            
        elif 'é¢‘ç‡é™åˆ¶' in error_type:
            suggestions.extend([
                "é™ä½è¯·æ±‚é¢‘ç‡",
                "æ·»åŠ è¯·æ±‚é—´éš”",
                "è€ƒè™‘å‡çº§åˆ°ä»˜è´¹API"
            ])
            
        elif 'æ•°æ®ä¸å­˜åœ¨' in error_type:
            base_suggestion = f"è¯¥è‚¡ç¥¨ {ticker} ({board_name}) å¯èƒ½ç¼ºä¹æœºæ„ç ”æŠ¥è¦†ç›–"
            suggestions.append(base_suggestion)
            
            # æ ¹æ®æ¿å—ç‰¹æ€§æä¾›å»ºè®®
            if board_info and board_info.get('special_handling'):
                suggestions.extend([
                    f"{board_name}è‚¡ç¥¨é€šå¸¸ç ”æŠ¥è¦†ç›–è¾ƒå°‘æˆ–éœ€è¦ç‰¹æ®Šæƒé™",
                    "å»ºè®®ä¾èµ–åŸºæœ¬é¢åˆ†æå’Œå…¬å¸å…¬å‘Š",
                    "å…³æ³¨è¡Œä¸šç ”ç©¶æŠ¥å‘Šå’Œå®è§‚åˆ†æ"
                ])
            else:
                suggestions.extend([
                    "ä¾èµ–å…¶ä»–åˆ†ææ–¹æ³•ï¼ˆåŸºæœ¬é¢ã€æŠ€æœ¯é¢ï¼‰",
                    "æ£€æŸ¥æ˜¯å¦ä¸ºæ–°è‚¡æˆ–å†·é—¨è‚¡ç¥¨"
                ])
                
        elif error_type == 'board_specific':
            suggestions.extend([
                f"{board_name}è‚¡ç¥¨å­˜åœ¨ç‰¹æ®Šé™åˆ¶æˆ–æ•°æ®è·å–å›°éš¾",
                "å»ºè®®ä½¿ç”¨ä¸“é—¨é’ˆå¯¹è¯¥æ¿å—çš„æ•°æ®æº",
                "å…³æ³¨å®˜æ–¹äº¤æ˜“æ‰€å…¬å‘Šå’Œè¡Œä¸šæŠ¥å‘Š"
            ])
            
        return suggestions
    
    def _safe_get_value(self, row, possible_keys: List[str], default: str = '') -> str:
        """å®‰å…¨åœ°ä»è¡Œæ•°æ®ä¸­è·å–å€¼"""
        for key in possible_keys:
            if key in row and row[key] is not None:
                value = str(row[key]).strip()
                if value and value.lower() not in ['nan', 'none', 'null', '']:
                    return value
        return default
    
    def _extract_number(self, text: str) -> Optional[float]:
        """ä»æ–‡æœ¬ä¸­æå–æ•°å­—"""
        try:
            if not text or text.lower() in ['nan', 'none', 'null', '']:
                return None
            
            # ç§»é™¤å¸¸è§çš„éæ•°å­—å­—ç¬¦
            cleaned = re.sub(r'[^\d\.\-\+]', '', str(text))
            if cleaned:
                return float(cleaned)
        except:
            pass
        return None
    
    def _extract_key_points(self, summary: str, title: str) -> List[str]:
        """ä»æ‘˜è¦å’Œæ ‡é¢˜ä¸­æå–å…³é”®è§‚ç‚¹"""
        key_points = []
        
        if not summary or summary == 'æš‚æ— æ‘˜è¦':
            return key_points
        
        # åŸºäºå¸¸è§åˆ†éš”ç¬¦åˆ†å‰²å…³é”®è§‚ç‚¹
        delimiters = ['ï¼›', ';', 'ã€‚', 'ï¼Œ', ',', 'ï¼š', ':', 'ã€']
        sentences = [summary]
        
        for delimiter in delimiters:
            new_sentences = []
            for sentence in sentences:
                new_sentences.extend(sentence.split(delimiter))
            sentences = new_sentences
        
        # è¿‡æ»¤å’Œæ¸…ç†å…³é”®è§‚ç‚¹
        for sentence in sentences:
            cleaned = sentence.strip()
            if len(cleaned) > 5 and len(cleaned) < 100:  # åˆç†çš„é•¿åº¦èŒƒå›´
                # è¯†åˆ«å…·æœ‰æ˜ç¡®è§‚ç‚¹çš„å¥å­
                if any(keyword in cleaned for keyword in ['æ¨è', 'å»ºè®®', 'é¢„è®¡', 'é¢„æœŸ', 'ç»´æŒ', 'ä¸Šè°ƒ', 'ä¸‹è°ƒ', 'ç›®æ ‡', 'ä¼°å€¼']):
                    key_points.append(cleaned)
        
        # å¦‚æœæ²¡æœ‰æå–åˆ°å…³é”®è§‚ç‚¹ï¼Œä»æ ‡é¢˜ä¸­æå–
        if not key_points and title:
            if 'æ¨è' in title or 'ä¹°å…¥' in title or 'å¢æŒ' in title:
                key_points.append('ç»´æŒç§¯ææŠ•èµ„å»ºè®®')
        
        return key_points[:5]  # æœ€å¤šè¿”å›5ä¸ªå…³é”®è§‚ç‚¹
    
    def _calculate_confidence_level(self, institution: str, analyst: str, publish_date: str) -> float:
        """è®¡ç®—ç ”æŠ¥å¯ä¿¡åº¦è¯„åˆ†"""
        base_confidence = 0.6
        
        # çŸ¥åæœºæ„åŠ åˆ†
        top_institutions = ['ä¸­ä¿¡è¯åˆ¸', 'åæ³°è¯åˆ¸', 'å›½æ³°å›å®‰', 'æ‹›å•†è¯åˆ¸', 'ä¸­é‡‘å…¬å¸', 'æµ·é€šè¯åˆ¸', 'ç”³ä¸‡å®æº', 'ä¸œæ–¹è¯åˆ¸', 'å…‰å¤§è¯åˆ¸', 'å…´ä¸šè¯åˆ¸']
        if any(inst in institution for inst in top_institutions):
            base_confidence += 0.2
        
        # åˆ†æå¸ˆä¿¡æ¯å®Œæ•´æ€§åŠ åˆ†
        if analyst and analyst != 'æœªçŸ¥åˆ†æå¸ˆ':
            base_confidence += 0.1
        
        # å‘å¸ƒæ—¥æœŸæ–°é²œåº¦åŠ åˆ†
        try:
            from datetime import datetime, timedelta
            pub_date = datetime.strptime(publish_date, '%Y-%m-%d')
            days_ago = (datetime.now() - pub_date).days
            
            if days_ago <= 30:  # 30å¤©å†…
                base_confidence += 0.1
            elif days_ago <= 90:  # 90å¤©å†…
                base_confidence += 0.05
        except:
            pass
        
        return min(base_confidence, 1.0)  # æœ€é«˜1.0
    
    def _validate_research_data(self, df, ticker: str) -> Dict[str, Any]:
        """éªŒè¯ç ”æŠ¥æ•°æ®è´¨é‡"""
        try:
            # åŸºç¡€æ£€æŸ¥
            if df is None or df.empty:
                return {'is_valid': False, 'reason': 'æ•°æ®ä¸ºç©º'}
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«é”™è¯¯ä¿¡æ¯
            if 'infoCode' in df.columns:
                return {'is_valid': False, 'reason': 'infoCodeé”™è¯¯ï¼Œå¯èƒ½è‚¡ç¥¨ä»£ç ä¸å­˜åœ¨æˆ–æ— ç ”æŠ¥æ•°æ®'}
            
            # æ£€æŸ¥åˆ—æ•°æ˜¯å¦åˆç†
            if len(df.columns) < 3:
                return {'is_valid': False, 'reason': f'åˆ—æ•°è¿‡å°‘: {len(df.columns)}ï¼Œæ•°æ®æ ¼å¼å¼‚å¸¸'}
            
            # æ£€æŸ¥æ•°æ®è¡Œæ•°
            if len(df) == 0:
                return {'is_valid': False, 'reason': 'æ•°æ®è¡Œæ•°ä¸º0'}
            
            # æ£€æŸ¥å…³é”®å­—æ®µæ˜¯å¦å­˜åœ¨
            required_fields = ['æ ‡é¢˜', 'æœºæ„', 'æ—¥æœŸ']
            alternative_fields = [
                ['title', 'ç ”æŠ¥æ ‡é¢˜', 'æŠ¥å‘Šæ ‡é¢˜'],
                ['institution', 'åˆ¸å•†', 'ç ”ç©¶æœºæ„'], 
                ['date', 'å‘å¸ƒæ—¥æœŸ', 'å‘å¸ƒæ—¶é—´']
            ]
            
            missing_fields = []
            for i, field_group in enumerate([required_fields] + alternative_fields):
                field_found = False
                for field in field_group:
                    if field in df.columns:
                        field_found = True
                        break
                
                if not field_found and i == 0:  # æ£€æŸ¥å¿…é¡»å­—æ®µ
                    missing_fields.extend(field_group)
            
            # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
            completeness_score = self._calculate_data_completeness(df)
            if completeness_score < 0.3:  # å®Œæ•´æ€§ä½äº30%
                return {'is_valid': False, 'reason': f'æ•°æ®å®Œæ•´æ€§è¿‡ä½: {completeness_score:.1%}'}
            
            # é€šè¿‡æ‰€æœ‰éªŒè¯
            return {
                'is_valid': True, 
                'completeness_score': completeness_score,
                'row_count': len(df),
                'column_count': len(df.columns)
            }
            
        except Exception as e:
            return {'is_valid': False, 'reason': f'éªŒè¯è¿‡ç¨‹å¼‚å¸¸: {str(e)}'}
    
    def _calculate_data_completeness(self, df) -> float:
        """è®¡ç®—æ•°æ®å®Œæ•´æ€§è¯„åˆ†"""
        try:
            total_cells = df.shape[0] * df.shape[1]
            if total_cells == 0:
                return 0.0
            
            # è®¡ç®—éç©ºå€¼æ•°é‡
            non_null_count = 0
            for column in df.columns:
                series = df[column]
                non_null_count += series.count()  # pandas count()ä¼šå¿½ç•¥NaNå€¼
            
            return non_null_count / total_cells
            
        except Exception:
            return 0.0
    
    def _clean_research_data(self, df) -> Any:
        """æ¸…æ´—ç ”æŠ¥æ•°æ®"""
        try:
            if df is None or df.empty:
                return df
            
            # åˆ›å»ºæ•°æ®å‰¯æœ¬é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
            cleaned_df = df.copy()
            
            # ç§»é™¤å®Œå…¨ç©ºçš„è¡Œ
            cleaned_df = cleaned_df.dropna(how='all')
            
            # å­—ç¬¦ä¸²å­—æ®µæ¸…ç†
            string_columns = cleaned_df.select_dtypes(include=['object']).columns
            for col in string_columns:
                if col in cleaned_df.columns:
                    # ç§»é™¤å‰åç©ºæ ¼
                    cleaned_df[col] = cleaned_df[col].astype(str).str.strip()
                    # æ›¿æ¢å¸¸è§çš„æ— æ•ˆå€¼
                    cleaned_df[col] = cleaned_df[col].replace(['nan', 'NaN', 'None', 'null', ''], None)
            
            # æ—¥æœŸå­—æ®µæ ‡å‡†åŒ–
            date_columns = ['æ—¥æœŸ', 'date', 'å‘å¸ƒæ—¥æœŸ', 'å‘å¸ƒæ—¶é—´']
            for col in date_columns:
                if col in cleaned_df.columns:
                    cleaned_df[col] = self._standardize_date_format(cleaned_df[col])
            
            # å»é‡ï¼ˆåŸºäºæ ‡é¢˜å’Œæœºæ„ï¼‰
            title_cols = ['æ ‡é¢˜', 'title', 'ç ”æŠ¥æ ‡é¢˜']
            institution_cols = ['æœºæ„', 'institution', 'åˆ¸å•†']
            
            title_col = None
            institution_col = None
            
            for col in title_cols:
                if col in cleaned_df.columns:
                    title_col = col
                    break
                    
            for col in institution_cols:
                if col in cleaned_df.columns:
                    institution_col = col
                    break
            
            if title_col and institution_col:
                cleaned_df = cleaned_df.drop_duplicates(subset=[title_col, institution_col], keep='first')
            
            logger.debug(f"æ•°æ®æ¸…æ´—å®Œæˆ: åŸå§‹è¡Œæ•° {len(df)} -> æ¸…æ´—å {len(cleaned_df)}")
            return cleaned_df
            
        except Exception as e:
            logger.warning(f"æ•°æ®æ¸…æ´—å¤±è´¥: {e}ï¼Œè¿”å›åŸå§‹æ•°æ®")
            return df
    
    def _standardize_date_format(self, date_series) -> Any:
        """æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼"""
        try:
            import pandas as pd
            # å°è¯•è½¬æ¢ä¸ºæ ‡å‡†æ—¥æœŸæ ¼å¼
            return pd.to_datetime(date_series, errors='coerce').dt.strftime('%Y-%m-%d')
        except Exception:
            return date_series
    
    def _get_sample_akshare_reports(self, ticker: str, limit: int) -> List[ResearchReport]:
        """è·å–ç¤ºä¾‹æ•°æ®"""
        sample_data = [
            {
                'title': f'{ticker} æ·±åº¦ç ”ç©¶æŠ¥å‘Š',
                'analyst': 'èµµå…­',
                'institution': 'åæ³°è¯åˆ¸',
                'publish_date': '2024-07-20',
                'rating': 'ä¹°å…¥',
                'target_price_text': 'ç›®æ ‡ä»·ï¼š46å…ƒ',
                'summary': 'å…¬å¸è¡Œä¸šé¢†å…ˆåœ°ä½ç¨³å›ºï¼Œç»´æŒä¹°å…¥è¯„çº§'
            }
        ]
        
        reports = []
        for data in sample_data[:limit]:
            report = ResearchReport(
                title=data['title'],
                analyst=data['analyst'],
                institution=data['institution'],
                publish_date=data['publish_date'],
                rating=self._parse_rating(data['rating']),
                target_price=self._extract_price(data['target_price_text']),
                current_price=None,
                summary=data['summary'],
                key_points=[],
                pe_forecast=None,
                revenue_growth=None,
                profit_growth=None,
                source=self.name,
                confidence_level=0.7
            )
            reports.append(report)
        
        return reports


class ResearchReportManager:
    """ç ”æŠ¥æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self):
        self.providers = [
            EastMoneyResearchProvider(),
            TongHuaShunResearchProvider(),
            AKShareResearchProvider()
        ]
        logger.info(f"ğŸš€ ç ”æŠ¥æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œå¯ç”¨æ•°æ®æº: {[p.name for p in self.providers]}")
    
    def get_comprehensive_reports(self, ticker: str, limit_per_source: int = 5) -> List[ResearchReport]:
        """è·å–ç»¼åˆç ”æŠ¥æ•°æ®"""
        try:
            logger.info(f"ğŸ“Š å¼€å§‹è·å– {ticker} çš„ç»¼åˆç ”æŠ¥æ•°æ®")
            
            all_reports = []
            successful_sources = []
            failed_sources = []
            
            # å¹¶è¡Œè·å–å„æ•°æ®æºçš„ç ”æŠ¥
            for provider in self.providers:
                try:
                    reports = provider.get_reports(ticker, limit_per_source)
                    if reports:  # åªæœ‰åœ¨æœ‰æ•°æ®æ—¶æ‰ç®—æˆåŠŸ
                        all_reports.extend(reports)
                        successful_sources.append(provider.name)
                        logger.info(f"âœ… {provider.name} è·å–åˆ° {len(reports)} æ¡ç ”æŠ¥")
                    else:
                        failed_sources.append(provider.name)
                        logger.warning(f"âš ï¸ {provider.name} è¿”å›ç©ºæ•°æ®")
                except Exception as e:
                    failed_sources.append(provider.name)
                    logger.warning(f"âš ï¸ {provider.name} è·å–ç ”æŠ¥å¤±è´¥: {str(e)}")
                    continue
            
            # å¦‚æœæ‰€æœ‰æ•°æ®æºéƒ½å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨ï¼Œä¸ä½¿ç”¨å¤‡ç”¨æ•°æ®
            if not all_reports:
                logger.warning(f"âš ï¸ æ‰€æœ‰æ•°æ®æºéƒ½å¤±è´¥ï¼Œæ— ç ”æŠ¥æ•°æ®å¯ç”¨: {ticker}")
                logger.info(f"ğŸ“Š å»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–è‚¡ç¥¨ä»£ç æ˜¯å¦æ­£ç¡®: {ticker}")
                return []
            
            # æ•°æ®å»é‡å’Œè´¨é‡ä¼˜åŒ–
            all_reports = self._deduplicate_and_optimize_reports(all_reports, ticker)
            
            # æŒ‰å‘å¸ƒæ—¶é—´å’Œå¯ä¿¡åº¦ç»¼åˆæ’åº
            all_reports = self._sort_reports_by_quality(all_reports)
            
            logger.info(f"ğŸ“ˆ ç»¼åˆè·å–åˆ° {len(all_reports)} æ¡ç ”æŠ¥æ•°æ®: {ticker}")
            logger.info(f"ğŸ“Š æˆåŠŸæ•°æ®æº: {successful_sources}, å¤±è´¥æ•°æ®æº: {failed_sources}")
            return all_reports
            
        except Exception as e:
            logger.error(f"âŒ è·å–ç»¼åˆç ”æŠ¥æ•°æ®å¤±è´¥: {ticker}, é”™è¯¯: {str(e)}")
            # å‡ºç°å¼‚å¸¸æ—¶è¿”å›ç©ºåˆ—è¡¨ï¼Œä¸ä½¿ç”¨å¤‡ç”¨æ•°æ®
            return []
    
    def _deduplicate_and_optimize_reports(self, reports: List[ResearchReport], ticker: str) -> List[ResearchReport]:
        """æ•°æ®å»é‡å’Œè´¨é‡ä¼˜åŒ–"""
        try:
            if not reports:
                return reports
            
            logger.debug(f"ğŸ”„ å¼€å§‹æ•°æ®å»é‡å’Œä¼˜åŒ–: {ticker}, åŸå§‹æ•°æ® {len(reports)} æ¡")
            
            # 1. åŸºäºæ ‡é¢˜å’Œæœºæ„çš„å»é‡
            unique_reports = {}
            for report in reports:
                # åˆ›å»ºå»é‡é”®
                dedup_key = self._create_dedup_key(report)
                
                if dedup_key not in unique_reports:
                    unique_reports[dedup_key] = report
                else:
                    # å¦‚æœå­˜åœ¨é‡å¤ï¼Œä¿ç•™è´¨é‡æ›´é«˜çš„
                    existing_report = unique_reports[dedup_key]
                    if self._compare_report_quality(report, existing_report) > 0:
                        unique_reports[dedup_key] = report
            
            deduplicated_reports = list(unique_reports.values())
            
            # 2. æ•°æ®è´¨é‡è¿‡æ»¤
            quality_reports = []
            for report in deduplicated_reports:
                if self._validate_report_quality(report):
                    quality_reports.append(report)
                else:
                    logger.debug(f"ğŸš® è¿‡æ»¤ä½è´¨é‡ç ”æŠ¥: {report.title[:50]}...")
            
            # 3. å¯ä¿¡åº¦é‡æ–°è®¡ç®—ï¼ˆåŸºäºæ•°æ®æºç»„åˆï¼‰
            for report in quality_reports:
                report.confidence_level = self._recalculate_confidence(report, quality_reports)
            
            logger.debug(f"âœ… æ•°æ®ä¼˜åŒ–å®Œæˆ: {ticker}, å»é‡å {len(deduplicated_reports)} æ¡, è´¨é‡è¿‡æ»¤å {len(quality_reports)} æ¡")
            
            return quality_reports
            
        except Exception as e:
            logger.warning(f"âš ï¸ æ•°æ®å»é‡ä¼˜åŒ–å¤±è´¥: {ticker}, {e}, è¿”å›åŸå§‹æ•°æ®")
            return reports
    
    def _create_dedup_key(self, report: ResearchReport) -> str:
        """åˆ›å»ºå»é‡é”®"""
        # æ ‡å‡†åŒ–æ ‡é¢˜ï¼ˆå»é™¤ç©ºæ ¼ã€æ ‡ç‚¹ï¼‰
        title_cleaned = re.sub(r'[^\w\u4e00-\u9fff]', '', report.title.lower())
        
        # æ ‡å‡†åŒ–æœºæ„åç§°
        institution_cleaned = re.sub(r'[^\w\u4e00-\u9fff]', '', report.institution.lower())
        
        # ç»„åˆé”®ï¼šæ ‡é¢˜å‰20å­—ç¬¦ + æœºæ„ + å‘å¸ƒæ—¥æœŸ
        dedup_key = f"{title_cleaned[:20]}_{institution_cleaned}_{report.publish_date}"
        
        return dedup_key
    
    def _compare_report_quality(self, report1: ResearchReport, report2: ResearchReport) -> int:
        """æ¯”è¾ƒä¸¤ä¸ªç ”æŠ¥çš„è´¨é‡ï¼Œè¿”å›1è¡¨ç¤ºreport1æ›´å¥½ï¼Œ-1è¡¨ç¤ºreport2æ›´å¥½ï¼Œ0è¡¨ç¤ºç›¸åŒ"""
        score1 = self._calculate_report_quality_score(report1)
        score2 = self._calculate_report_quality_score(report2)
        
        if score1 > score2:
            return 1
        elif score1 < score2:
            return -1
        else:
            return 0
    
    def _calculate_report_quality_score(self, report: ResearchReport) -> float:
        """è®¡ç®—ç ”æŠ¥è´¨é‡è¯„åˆ†"""
        score = 0.0
        
        # åŸºç¡€å¯ä¿¡åº¦
        score += report.confidence_level * 40
        
        # æ ‡é¢˜è´¨é‡
        if len(report.title) > 10:
            score += 10
        if len(report.title) > 20:
            score += 5
        
        # åˆ†æå¸ˆä¿¡æ¯
        if report.analyst and report.analyst != 'æœªçŸ¥åˆ†æå¸ˆ':
            score += 10
        
        # æœºæ„æƒå¨æ€§
        authoritative_institutions = ['ä¸­ä¿¡è¯åˆ¸', 'åæ³°è¯åˆ¸', 'å›½æ³°å›å®‰', 'æ‹›å•†è¯åˆ¸', 'ä¸­é‡‘å…¬å¸']
        if any(inst in report.institution for inst in authoritative_institutions):
            score += 15
        
        # è¯„çº§ä¿¡æ¯
        if report.rating and report.rating != 'æœªçŸ¥':
            score += 8
        
        # ç›®æ ‡ä»·
        if report.target_price:
            score += 12
        
        # æ‘˜è¦è´¨é‡
        if report.summary and len(report.summary) > 50:
            score += 8
        
        # å…³é”®è§‚ç‚¹
        if report.key_points:
            score += len(report.key_points) * 3
        
        # æ—¶æ•ˆæ€§
        try:
            from datetime import datetime, timedelta
            pub_date = datetime.strptime(report.publish_date, '%Y-%m-%d')
            days_old = (datetime.now() - pub_date).days
            
            if days_old <= 7:
                score += 10
            elif days_old <= 30:
                score += 5
            elif days_old <= 90:
                score += 2
        except:
            pass
        
        return score
    
    def _validate_report_quality(self, report: ResearchReport) -> bool:
        """éªŒè¯ç ”æŠ¥åŸºæœ¬è´¨é‡è¦æ±‚"""
        # æ ‡é¢˜é•¿åº¦æ£€æŸ¥
        if not report.title or len(report.title) < 5:
            return False
        
        # æœºæ„ä¿¡æ¯æ£€æŸ¥
        if not report.institution or report.institution in ['æœªçŸ¥æœºæ„', '']:
            return False
        
        # å¯ä¿¡åº¦æ£€æŸ¥
        if report.confidence_level < 0.3:
            return False
        
        return True
    
    def _recalculate_confidence(self, report: ResearchReport, all_reports: List[ResearchReport]) -> float:
        """åŸºäºæ•´ä½“æ•°æ®é‡æ–°è®¡ç®—å¯ä¿¡åº¦"""
        base_confidence = report.confidence_level
        
        # å¦‚æœåŒä¸€æœºæ„æœ‰å¤šä»½ç ”æŠ¥ï¼Œæé«˜å¯ä¿¡åº¦
        same_institution_count = sum(1 for r in all_reports if r.institution == report.institution)
        if same_institution_count > 1:
            base_confidence += 0.1
        
        # å¦‚æœæœ‰ç›®æ ‡ä»·ä¸”åœ¨åˆç†èŒƒå›´å†…ï¼Œæé«˜å¯ä¿¡åº¦
        if report.target_price:
            base_confidence += 0.05
        
        # ç¡®ä¿å¯ä¿¡åº¦åœ¨åˆç†èŒƒå›´å†…
        return min(max(base_confidence, 0.0), 1.0)
    
    def _sort_reports_by_quality(self, reports: List[ResearchReport]) -> List[ResearchReport]:
        """æŒ‰è´¨é‡å’Œæ—¶æ•ˆæ€§ç»¼åˆæ’åº"""
        try:
            def sort_key(report):
                # ç»¼åˆè¯„åˆ†ï¼šè´¨é‡åˆ†æ•° + æ—¶æ•ˆæ€§åˆ†æ•°
                quality_score = self._calculate_report_quality_score(report)
                
                # æ—¶æ•ˆæ€§åˆ†æ•°
                recency_score = 0
                try:
                    from datetime import datetime
                    pub_date = datetime.strptime(report.publish_date, '%Y-%m-%d')
                    days_old = (datetime.now() - pub_date).days
                    recency_score = max(0, 100 - days_old)  # æœ€è¿‘çš„ç ”æŠ¥åˆ†æ•°æ›´é«˜
                except:
                    recency_score = 0
                
                # ç»¼åˆåˆ†æ•°ï¼ˆè´¨é‡æƒé‡70%ï¼Œæ—¶æ•ˆæ€§æƒé‡30%ï¼‰
                total_score = quality_score * 0.7 + recency_score * 0.3
                
                return total_score
            
            sorted_reports = sorted(reports, key=sort_key, reverse=True)
            logger.debug(f"ğŸ“Š ç ”æŠ¥æ’åºå®Œæˆï¼ŒæŒ‰è´¨é‡å’Œæ—¶æ•ˆæ€§æ’åº")
            
            return sorted_reports
            
        except Exception as e:
            logger.warning(f"âš ï¸ ç ”æŠ¥æ’åºå¤±è´¥: {e}ï¼Œä¿æŒåŸé¡ºåº")
            return reports
    
    
    def analyze_consensus(self, reports: List[ResearchReport]) -> Dict[str, Any]:
        """åˆ†ææœºæ„ä¸€è‡´é¢„æœŸ"""
        try:
            if not reports:
                return {}
            
            # è¯„çº§ç»Ÿè®¡
            ratings = [r.rating for r in reports if r.rating != 'æœªçŸ¥']
            rating_counts = {}
            for rating in ratings:
                rating_counts[rating] = rating_counts.get(rating, 0) + 1
            
            # ç›®æ ‡ä»·ç»Ÿè®¡
            target_prices = [r.target_price for r in reports if r.target_price is not None]
            avg_target_price = sum(target_prices) / len(target_prices) if target_prices else None
            
            # PEé¢„æµ‹ç»Ÿè®¡
            pe_forecasts = [r.pe_forecast for r in reports if r.pe_forecast is not None]
            avg_pe_forecast = sum(pe_forecasts) / len(pe_forecasts) if pe_forecasts else None
            
            # å¢é•¿é¢„æµ‹ç»Ÿè®¡
            revenue_growths = [r.revenue_growth for r in reports if r.revenue_growth is not None]
            profit_growths = [r.profit_growth for r in reports if r.profit_growth is not None]
            
            avg_revenue_growth = sum(revenue_growths) / len(revenue_growths) if revenue_growths else None
            avg_profit_growth = sum(profit_growths) / len(profit_growths) if profit_growths else None
            
            # æœºæ„è¦†ç›–åº¦
            institutions = list(set([r.institution for r in reports if r.institution]))
            
            consensus = {
                'total_reports': len(reports),
                'rating_distribution': rating_counts,
                'average_target_price': avg_target_price,
                'target_price_range': {
                    'min': min(target_prices) if target_prices else None,
                    'max': max(target_prices) if target_prices else None
                },
                'average_pe_forecast': avg_pe_forecast,
                'average_revenue_growth': avg_revenue_growth,
                'average_profit_growth': avg_profit_growth,
                'covering_institutions': institutions,
                'institution_count': len(institutions),
                'data_sources': list(set([r.source for r in reports])),
                'latest_report_date': max([r.publish_date for r in reports]) if reports else None
            }
            
            return consensus
            
        except Exception as e:
            logger.error(f"âŒ åˆ†ææœºæ„ä¸€è‡´é¢„æœŸå¤±è´¥: {str(e)}")
            return {}


# å…¨å±€ç ”æŠ¥ç®¡ç†å™¨å®ä¾‹
_research_manager = None

def get_research_report_manager() -> ResearchReportManager:
    """è·å–ç ”æŠ¥ç®¡ç†å™¨å•ä¾‹"""
    global _research_manager
    if _research_manager is None:
        _research_manager = ResearchReportManager()
    return _research_manager


# ä¾¿æ·å‡½æ•°
def get_stock_research_reports(ticker: str, limit: int = 15) -> List[ResearchReport]:
    """è·å–è‚¡ç¥¨ç ”æŠ¥æ•°æ®"""
    manager = get_research_report_manager()
    return manager.get_comprehensive_reports(ticker, limit // 3)  # æ¯ä¸ªæ•°æ®æºå–limit/3æ¡


def get_institutional_consensus(ticker: str) -> Dict[str, Any]:
    """è·å–æœºæ„ä¸€è‡´é¢„æœŸ"""
    reports = get_stock_research_reports(ticker)
    manager = get_research_report_manager()
    return manager.analyze_consensus(reports)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    ticker = "000001"
    
    print(f"ğŸ” æµ‹è¯•è·å– {ticker} çš„ç ”æŠ¥æ•°æ®...")
    reports = get_stock_research_reports(ticker)
    print(f"ğŸ“Š è·å–åˆ° {len(reports)} æ¡ç ”æŠ¥")
    
    if reports:
        print(f"ğŸ“ˆ æœ€æ–°ç ”æŠ¥: {reports[0].title}")
        print(f"ğŸ“ˆ è¯„çº§: {reports[0].rating}")
        print(f"ğŸ“ˆ ç›®æ ‡ä»·: {reports[0].target_price}")
    
    print(f"\nğŸ¯ æœºæ„ä¸€è‡´é¢„æœŸåˆ†æ:")
    consensus = get_institutional_consensus(ticker)
    for key, value in consensus.items():
        print(f"  {key}: {value}")