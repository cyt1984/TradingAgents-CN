#!/usr/bin/env python3
"""
åˆ†å±‚æ•°æ®ç®¡ç†å™¨ - æ ¸å¿ƒè°ƒåº¦é€»è¾‘
å®ç°"æ‰¹é‡ä¼˜å…ˆ + å®æ—¶è¡¥å……"çš„æ™ºèƒ½æ•°æ®è·å–ç­–ç•¥
è§£å†³ç”¨æˆ·æå‡ºçš„é€Ÿåº¦é—®é¢˜ï¼šä»"ä¸€ä¸ªä¸ªè·å–"æ”¹ä¸º"æ‰¹é‡ä¸‹è½½+ç²¾å‡†è¡¥å……"
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Any, Tuple, Union
from datetime import datetime, timedelta
from enum import Enum
from dataclasses import dataclass
import time
import threading
from pathlib import Path
import json

# å¯¼å…¥æ—¥å¿—æ¨¡å—
from tradingagents.utils.logging_manager import get_logger
logger = get_logger('tiered_data')

# å¯¼å…¥å„ç§æ•°æ®æº
from .baostock_utils import get_baostock_provider
from .akshare_utils import get_akshare_provider
from .tushare_adapter import get_tushare_adapter

# å¯¼å…¥å®æ—¶æ•°æ®æº
from .eastmoney_utils import get_eastmoney_provider
from .tencent_utils import get_tencent_provider
from .sina_utils import get_sina_provider
from .xueqiu_utils import get_xueqiu_provider


class DataTier(Enum):
    """æ•°æ®å±‚çº§æšä¸¾"""
    BATCH = "batch"           # æ‰¹é‡æ•°æ®æºå±‚
    REALTIME = "realtime"     # å®æ—¶æ•°æ®æºå±‚
    CACHE = "cache"           # ç¼“å­˜å±‚


class DataType(Enum):
    """æ•°æ®ç±»å‹æšä¸¾"""
    HISTORICAL = "historical"  # å†å²Kçº¿æ•°æ®
    FINANCIAL = "financial"    # è´¢åŠ¡æ•°æ®
    NEWS = "news"             # æ–°é—»æ•°æ®
    REALTIME_PRICE = "realtime_price"  # å®æ—¶ä»·æ ¼
    SOCIAL = "social"         # ç¤¾äº¤åª’ä½“æ•°æ®


@dataclass
class DataRequest:
    """æ•°æ®è¯·æ±‚ç»“æ„"""
    symbols: List[str]
    data_type: DataType
    start_date: Optional[str] = None
    end_date: Optional[str] = None
    priority: int = 5  # 1-10ï¼Œæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜
    max_age_hours: int = 24  # æ•°æ®æœ€å¤§å¹´é¾„ï¼ˆå°æ—¶ï¼‰
    batch_preferred: bool = True  # æ˜¯å¦ä¼˜å…ˆä½¿ç”¨æ‰¹é‡æ•°æ®æº


@dataclass
class DataResponse:
    """æ•°æ®å“åº”ç»“æ„"""
    data: Dict[str, pd.DataFrame]
    source_info: Dict[str, str]  # {symbol: source}
    stats: Dict[str, Any]
    cache_hits: int = 0
    batch_hits: int = 0
    realtime_hits: int = 0


class TieredDataManager:
    """åˆ†å±‚æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†å±‚æ•°æ®ç®¡ç†å™¨"""
        self.batch_providers = {}
        self.realtime_providers = {}
        self.provider_status = {}
        
        # æ•°æ®è·å–ç»Ÿè®¡
        self.stats = {
            'batch_requests': 0,
            'realtime_requests': 0,
            'cache_hits': 0,
            'total_requests': 0,
            'performance_log': []
        }
        
        # åˆå§‹åŒ–æ‰€æœ‰æ•°æ®æº
        self._init_batch_providers()
        self._init_realtime_providers()
        
        logger.info("ğŸ—ï¸ åˆ†å±‚æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   æ‰¹é‡æ•°æ®æº: {list(self.batch_providers.keys())}")
        logger.info(f"   å®æ—¶æ•°æ®æº: {list(self.realtime_providers.keys())}")

    def _init_batch_providers(self):
        """åˆå§‹åŒ–æ‰¹é‡æ•°æ®æº"""
        logger.info("ğŸ“¦ åˆå§‹åŒ–æ‰¹é‡æ•°æ®æº...")
        
        # BaoStock - å®Œå…¨å…è´¹çš„æ‰¹é‡æ•°æ®æº
        try:
            baostock = get_baostock_provider()
            if baostock.connected:
                self.batch_providers['baostock'] = baostock
                self.provider_status['baostock'] = True
                logger.info("âœ… BaoStockæ‰¹é‡æ•°æ®æºå°±ç»ª")
            else:
                self.provider_status['baostock'] = False
                logger.warning("âš ï¸ BaoStockè¿æ¥å¤±è´¥")
        except Exception as e:
            logger.error(f"âŒ BaoStockåˆå§‹åŒ–å¤±è´¥: {e}")
            self.provider_status['baostock'] = False
        
        # AKShare - å…è´¹æ‰¹é‡æ•°æ®æº
        try:
            akshare = get_akshare_provider()
            if akshare.connected:
                self.batch_providers['akshare'] = akshare
                self.provider_status['akshare'] = True
                logger.info("âœ… AKShareæ‰¹é‡æ•°æ®æºå°±ç»ª")
            else:
                self.provider_status['akshare'] = False
                logger.warning("âš ï¸ AKShareè¿æ¥å¤±è´¥")
        except Exception as e:
            logger.error(f"âŒ AKShareåˆå§‹åŒ–å¤±è´¥: {e}")
            self.provider_status['akshare'] = False
        
        # Tushare - ä»˜è´¹ä½†é«˜è´¨é‡çš„æ‰¹é‡æ•°æ®æº
        try:
            if get_tushare_adapter:
                tushare = get_tushare_adapter()
                if hasattr(tushare, 'connected') and tushare.connected:
                    self.batch_providers['tushare'] = tushare
                    self.provider_status['tushare'] = True
                    logger.info("âœ… Tushareæ‰¹é‡æ•°æ®æºå°±ç»ª")
                else:
                    self.provider_status['tushare'] = False
                    logger.info("â„¹ï¸ Tushareæœªé…ç½®æˆ–è¿æ¥å¤±è´¥")
        except Exception as e:
            logger.warning(f"âš ï¸ Tushareåˆå§‹åŒ–å¤±è´¥: {e}")
            self.provider_status['tushare'] = False

    def _init_realtime_providers(self):
        """åˆå§‹åŒ–å®æ—¶æ•°æ®æº"""
        logger.info("âš¡ åˆå§‹åŒ–å®æ—¶æ•°æ®æº...")
        
        # ä¸œæ–¹è´¢å¯Œ - å®æ—¶æ•°æ®
        try:
            eastmoney = get_eastmoney_provider()
            self.realtime_providers['eastmoney'] = eastmoney
            self.provider_status['eastmoney'] = True
            logger.info("âœ… ä¸œæ–¹è´¢å¯Œå®æ—¶æ•°æ®æºå°±ç»ª")
        except Exception as e:
            logger.error(f"âŒ ä¸œæ–¹è´¢å¯Œåˆå§‹åŒ–å¤±è´¥: {e}")
            self.provider_status['eastmoney'] = False
        
        # è…¾è®¯è´¢ç» - å®æ—¶æ•°æ®
        try:
            tencent = get_tencent_provider()
            self.realtime_providers['tencent'] = tencent
            self.provider_status['tencent'] = True
            logger.info("âœ… è…¾è®¯è´¢ç»å®æ—¶æ•°æ®æºå°±ç»ª")
        except Exception as e:
            logger.error(f"âŒ è…¾è®¯è´¢ç»åˆå§‹åŒ–å¤±è´¥: {e}")
            self.provider_status['tencent'] = False
        
        # æ–°æµªè´¢ç» - å®æ—¶æ•°æ®
        try:
            sina = get_sina_provider()
            self.realtime_providers['sina'] = sina
            self.provider_status['sina'] = True
            logger.info("âœ… æ–°æµªè´¢ç»å®æ—¶æ•°æ®æºå°±ç»ª")
        except Exception as e:
            logger.error(f"âŒ æ–°æµªè´¢ç»åˆå§‹åŒ–å¤±è´¥: {e}")
            self.provider_status['sina'] = False
        
        # é›ªçƒ - ç¤¾äº¤å’Œå®æ—¶æ•°æ®
        try:
            xueqiu = get_xueqiu_provider()
            self.realtime_providers['xueqiu'] = xueqiu
            self.provider_status['xueqiu'] = True
            logger.info("âœ… é›ªçƒå®æ—¶æ•°æ®æºå°±ç»ª")
        except Exception as e:
            logger.error(f"âŒ é›ªçƒåˆå§‹åŒ–å¤±è´¥: {e}")
            self.provider_status['xueqiu'] = False

    def get_data(self, request: DataRequest) -> DataResponse:
        """
        æ™ºèƒ½è·å–æ•°æ® - æ ¸å¿ƒè°ƒåº¦æ–¹æ³•
        
        Args:
            request: æ•°æ®è¯·æ±‚
            
        Returns:
            æ•°æ®å“åº”
        """
        start_time = time.time()
        self.stats['total_requests'] += 1
        
        logger.info(f"ğŸ¯ å¼€å§‹å¤„ç†æ•°æ®è¯·æ±‚: {len(request.symbols)}åªè‚¡ç¥¨, ç±»å‹:{request.data_type.value}")
        
        # åˆå§‹åŒ–å“åº”
        response = DataResponse(
            data={},
            source_info={},
            stats={}
        )
        
        try:
            # 1. æ£€æŸ¥ç¼“å­˜ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            cached_data = self._check_cache(request)
            if cached_data:
                response.data.update(cached_data)
                response.cache_hits = len(cached_data)
                logger.info(f"ğŸ’¾ ç¼“å­˜å‘½ä¸­: {len(cached_data)}åªè‚¡ç¥¨")
            
            # 2. ç¡®å®šéœ€è¦è·å–çš„è‚¡ç¥¨
            remaining_symbols = [s for s in request.symbols if s not in response.data]
            
            if not remaining_symbols:
                logger.info("âœ… æ‰€æœ‰æ•°æ®éƒ½æ¥è‡ªç¼“å­˜")
                return response
            
            # 3. åˆ†å±‚æ•°æ®è·å–ç­–ç•¥
            if request.batch_preferred and request.data_type in [DataType.HISTORICAL, DataType.FINANCIAL]:
                # ä¼˜å…ˆä½¿ç”¨æ‰¹é‡æ•°æ®æº
                batch_data = self._get_batch_data(remaining_symbols, request)
                if batch_data:
                    response.data.update(batch_data['data'])
                    response.source_info.update(batch_data['source_info'])
                    response.batch_hits = len(batch_data['data'])
                    
                    # æ›´æ–°å‰©ä½™éœ€è¦è·å–çš„è‚¡ç¥¨
                    remaining_symbols = [s for s in remaining_symbols if s not in batch_data['data']]
            
            # 4. å®æ—¶æ•°æ®æºè¡¥å……
            if remaining_symbols:
                realtime_data = self._get_realtime_data(remaining_symbols, request)
                if realtime_data:
                    response.data.update(realtime_data['data'])
                    response.source_info.update(realtime_data['source_info'])
                    response.realtime_hits = len(realtime_data['data'])
            
            # 5. ä¿å­˜æ–°è·å–çš„æ•°æ®åˆ°ç¼“å­˜
            self._save_to_cache(response.data, request)
            
            # 6. æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            elapsed_time = time.time() - start_time
            response.stats = {
                'total_symbols': len(request.symbols),
                'successful': len(response.data),
                'failed': len(request.symbols) - len(response.data),
                'elapsed_time': elapsed_time,
                'cache_hits': response.cache_hits,
                'batch_hits': response.batch_hits,
                'realtime_hits': response.realtime_hits,
                'success_rate': len(response.data) / len(request.symbols) * 100 if request.symbols else 0
            }
            
            # 6. è®°å½•æ€§èƒ½æ—¥å¿—
            self.stats['performance_log'].append({
                'timestamp': datetime.now().isoformat(),
                'symbols_count': len(request.symbols),
                'data_type': request.data_type.value,
                'elapsed_time': elapsed_time,
                'success_rate': response.stats['success_rate'],
                'strategy': 'batch' if response.batch_hits > response.realtime_hits else 'realtime'
            })
            
            logger.info(f"âœ… æ•°æ®è·å–å®Œæˆ: æˆåŠŸ{response.stats['successful']}/{response.stats['total_symbols']} "
                       f"({response.stats['success_rate']:.1f}%), è€—æ—¶{elapsed_time:.2f}ç§’")
            
            return response
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®è·å–å¤±è´¥: {e}")
            response.stats = {'error': str(e)}
            return response

    def _check_cache(self, request: DataRequest) -> Optional[Dict[str, pd.DataFrame]]:
        """æ£€æŸ¥ç¼“å­˜æ•°æ®"""
        try:
            # å¯¼å…¥ç°æœ‰çš„ç¼“å­˜ç®¡ç†å™¨
            from tradingagents.dataflows.cache_manager import get_cache
            from tradingagents.dataflows.historical_data_manager import get_historical_manager
            
            cached_data = {}
            cache_manager = get_cache()
            historical_manager = get_historical_manager()
            
            for symbol in request.symbols:
                # 1. å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜ - ä½¿ç”¨å¤šç§ç¼“å­˜é”®å°è¯•
                cache_keys = [
                    f"stock_data_{symbol}_{request.start_date}_{request.end_date}",  # ç²¾ç¡®åŒ¹é…
                    f"stock_data_{symbol}_recent",  # æœ€è¿‘æ•°æ®ï¼ˆä¸ä¾èµ–å…·ä½“æ—¥æœŸï¼‰
                    f"stock_data_{symbol}_daily"  # æ—¥çº¿æ•°æ®é€šç”¨ç¼“å­˜
                ]
                
                cached_df = None
                for cache_key in cache_keys:
                    cached_df = cache_manager.get(cache_key)
                    if cached_df is not None and not cached_df.empty:
                        # æ£€æŸ¥æ•°æ®æ˜¯å¦åŒ…å«æ‰€éœ€çš„æ—¥æœŸèŒƒå›´
                        if hasattr(cached_df, 'index') and len(cached_df.index) > 0:
                            try:
                                # æ£€æŸ¥æ•°æ®çš„æ—¥æœŸèŒƒå›´æ˜¯å¦æ»¡è¶³éœ€æ±‚
                                data_start = cached_df.index[0].date() if hasattr(cached_df.index[0], 'date') else cached_df.index[0]
                                data_end = cached_df.index[-1].date() if hasattr(cached_df.index[-1], 'date') else cached_df.index[-1]
                                
                                from datetime import datetime
                                req_start = datetime.strptime(request.start_date, '%Y-%m-%d').date() if request.start_date else None
                                req_end = datetime.strptime(request.end_date, '%Y-%m-%d').date() if request.end_date else None
                                
                                # æ£€æŸ¥ç¼“å­˜æ•°æ®æ˜¯å¦è¦†ç›–æ‰€éœ€èŒƒå›´
                                if req_start and req_end:
                                    if data_start <= req_start and data_end >= req_end:
                                        # è¿‡æ»¤å‡ºæ‰€éœ€çš„æ—¥æœŸèŒƒå›´
                                        filtered_df = cached_df[(cached_df.index >= request.start_date) & (cached_df.index <= request.end_date)]
                                        if not filtered_df.empty:
                                            cached_data[symbol] = filtered_df
                                            logger.debug(f"âœ… å†…å­˜ç¼“å­˜å‘½ä¸­(èŒƒå›´è¿‡æ»¤): {symbol} (key: {cache_key})")
                                            break
                                elif req_end is None or data_end >= req_end:
                                    # å¦‚æœæ²¡æœ‰ç»“æŸæ—¥æœŸé™åˆ¶ï¼Œæˆ–æ•°æ®è¶³å¤Ÿæ–°
                                    cached_data[symbol] = cached_df
                                    logger.debug(f"âœ… å†…å­˜ç¼“å­˜å‘½ä¸­(æœ€æ–°æ•°æ®): {symbol} (key: {cache_key})")
                                    break
                            except Exception as e:
                                logger.debug(f"ç¼“å­˜æ•°æ®æ—¥æœŸæ£€æŸ¥å¤±è´¥ {symbol}: {e}")
                                continue
                        else:
                            cached_data[symbol] = cached_df
                            logger.debug(f"âœ… å†…å­˜ç¼“å­˜å‘½ä¸­(æ— ç´¢å¼•): {symbol} (key: {cache_key})")
                            break
                
                if symbol in cached_data:
                    continue
                else:
                    logger.debug(f"âŒ å†…å­˜ç¼“å­˜å®Œå…¨æœªå‘½ä¸­: {symbol}")
                
                # 2. æ£€æŸ¥å†å²æ•°æ®ç®¡ç†å™¨ï¼ˆæŒä¹…åŒ–å­˜å‚¨ï¼‰
                if request.data_type == DataType.HISTORICAL:
                    try:
                        stored_data = historical_manager.get_data(
                            symbol=symbol,
                            start_date=request.start_date,
                            end_date=request.end_date
                        )
                        
                        if stored_data is not None and not stored_data.empty:
                            # æ£€æŸ¥æ•°æ®æ˜¯å¦è¶³å¤Ÿæ–°
                            latest_date = stored_data.index[-1] if hasattr(stored_data, 'index') else None
                            if latest_date:
                                # å¦‚æœæ•°æ®åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…ï¼Œä½¿ç”¨ç¼“å­˜
                                current_date = datetime.now().date()
                                data_date = latest_date.date() if hasattr(latest_date, 'date') else latest_date
                                
                                # æ¿€è¿›ç¼“å­˜ç­–ç•¥ï¼šå¦‚æœæ•°æ®ä¸è¶…è¿‡3å¤©ï¼Œè®¤ä¸ºæ˜¯æ–°é²œçš„ï¼ˆæ™ºèƒ½é€‰è‚¡æ€§èƒ½ä¼˜åŒ–ï¼‰
                                if (current_date - data_date).days <= 3:
                                    cached_data[symbol] = stored_data
                                    logger.debug(f"âœ… æŒä¹…åŒ–ç¼“å­˜å‘½ä¸­: {symbol} (æ•°æ®æ—¥æœŸ: {data_date}, å¹´é¾„: {(current_date - data_date).days}å¤©)")
                                    
                                    # åŒæ—¶æ›´æ–°å†…å­˜ç¼“å­˜
                                    cache_manager.set(cache_key, stored_data, expire_minutes=60)
                                else:
                                    logger.debug(f"âŒ æŒä¹…åŒ–ç¼“å­˜è¿‡æœŸ: {symbol} (æ•°æ®æ—¥æœŸ: {data_date}, å¹´é¾„: {(current_date - data_date).days}å¤©)")
                                    
                    except Exception as e:
                        logger.debug(f"æ£€æŸ¥ {symbol} å†å²ç¼“å­˜å¤±è´¥: {e}")
                        continue
            
            if cached_data:
                logger.info(f"ğŸ’¾ ç¼“å­˜å‘½ä¸­: {len(cached_data)} åªè‚¡ç¥¨ï¼Œè·³è¿‡é‡å¤è·å–")
                
            return cached_data if cached_data else None
            
        except Exception as e:
            logger.warning(f"âš ï¸ ç¼“å­˜æ£€æŸ¥å¤±è´¥: {e}")
            return None

    def _save_to_cache(self, data: Dict[str, pd.DataFrame], request: DataRequest):
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        try:
            # å¯¼å…¥ç¼“å­˜ç®¡ç†å™¨
            from tradingagents.dataflows.cache_manager import get_cache
            from tradingagents.dataflows.historical_data_manager import get_historical_manager
            
            cache_manager = get_cache()
            historical_manager = get_historical_manager()
            
            for symbol, df in data.items():
                if df is not None and not df.empty:
                    # 1. ä¿å­˜åˆ°å†…å­˜ç¼“å­˜ï¼ˆçŸ­æœŸç¼“å­˜ï¼‰- ä½¿ç”¨å¤šç§ç¼“å­˜é”®
                    cache_keys = [
                        f"stock_data_{symbol}_{request.start_date}_{request.end_date}",  # ç²¾ç¡®åŒ¹é…
                        f"stock_data_{symbol}_recent",  # æœ€è¿‘æ•°æ®
                        f"stock_data_{symbol}_daily"  # æ—¥çº¿æ•°æ®é€šç”¨
                    ]
                    
                    for cache_key in cache_keys:
                        cache_manager.set(cache_key, df, expire_minutes=480)  # æ¿€è¿›ç¼“å­˜ï¼šå»¶é•¿åˆ°8å°æ—¶
                    
                    # 2. ä¿å­˜åˆ°å†å²æ•°æ®ç®¡ç†å™¨ï¼ˆæŒä¹…åŒ–å­˜å‚¨ï¼‰
                    if request.data_type == DataType.HISTORICAL:
                        try:
                            historical_manager.save_data(
                                symbol=symbol,
                                data=df,
                                data_type='daily'  # é»˜è®¤æ—¥çº¿æ•°æ®
                            )
                        except Exception as e:
                            logger.debug(f"ä¿å­˜ {symbol} åˆ°å†å²æ•°æ®åº“å¤±è´¥: {e}")
            
            if data:
                logger.info(f"ğŸ’¾ å·²ä¿å­˜ {len(data)} åªè‚¡ç¥¨æ•°æ®åˆ°ç¼“å­˜")
                
        except Exception as e:
            logger.warning(f"âš ï¸ ä¿å­˜æ•°æ®åˆ°ç¼“å­˜å¤±è´¥: {e}")

    def _get_batch_data(self, symbols: List[str], request: DataRequest) -> Optional[Dict[str, Any]]:
        """
        ä»æ‰¹é‡æ•°æ®æºè·å–æ•°æ®
        
        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            request: æ•°æ®è¯·æ±‚
            
        Returns:
            æ‰¹é‡æ•°æ®ç»“æœ
        """
        logger.info(f"ğŸ“¦ ä½¿ç”¨æ‰¹é‡æ•°æ®æºè·å– {len(symbols)} åªè‚¡ç¥¨æ•°æ®...")
        
        # æ‰¹é‡æ•°æ®æºä¼˜å…ˆçº§ï¼šBaoStock -> AKShare -> Tushare
        batch_order = ['baostock', 'akshare', 'tushare']
        
        for source in batch_order:
            if source not in self.batch_providers or not self.provider_status.get(source, False):
                continue
            
            try:
                provider = self.batch_providers[source]
                logger.info(f"ğŸ”„ å°è¯•ä½¿ç”¨ {source} æ‰¹é‡è·å–æ•°æ®...")
                
                if request.data_type == DataType.HISTORICAL:
                    # è·å–å†å²Kçº¿æ•°æ®
                    if source == 'baostock':
                        data = provider.batch_get_stock_data(
                            symbols=symbols,
                            start_date=request.start_date,
                            end_date=request.end_date,
                            batch_size=100,
                            delay=0.05
                        )
                    elif source == 'akshare':
                        data = provider.batch_get_stock_data(
                            symbols=symbols,
                            start_date=request.start_date,
                            end_date=request.end_date,
                            max_workers=8,
                            delay=0.1
                        )
                    elif source == 'tushare':
                        # Tushareæ‰¹é‡è·å–é€»è¾‘
                        data = self._get_tushare_batch_data(provider, symbols, request)
                    else:
                        continue
                
                elif request.data_type == DataType.FINANCIAL:
                    # è·å–è´¢åŠ¡æ•°æ®
                    if hasattr(provider, 'batch_get_financial_data'):
                        data = provider.batch_get_financial_data(symbols)
                    else:
                        continue
                else:
                    continue
                
                if data and len(data) > 0:
                    # æˆåŠŸè·å–æ•°æ®
                    self.stats['batch_requests'] += 1
                    source_info = {symbol: source for symbol in data.keys()}
                    
                    logger.info(f"âœ… {source} æ‰¹é‡è·å–æˆåŠŸ: {len(data)}/{len(symbols)} åªè‚¡ç¥¨")
                    
                    return {
                        'data': data,
                        'source_info': source_info,
                        'source': source
                    }
                else:
                    logger.warning(f"âš ï¸ {source} æ‰¹é‡è·å–å¤±è´¥æˆ–æ— æ•°æ®")
                    
            except Exception as e:
                logger.error(f"âŒ {source} æ‰¹é‡è·å–å¼‚å¸¸: {e}")
                continue
        
        logger.warning("âš ï¸ æ‰€æœ‰æ‰¹é‡æ•°æ®æºéƒ½å¤±è´¥")
        return None

    def _get_realtime_data(self, symbols: List[str], request: DataRequest) -> Optional[Dict[str, Any]]:
        """
        ä»å®æ—¶æ•°æ®æºè·å–æ•°æ®
        
        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            request: æ•°æ®è¯·æ±‚
            
        Returns:
            å®æ—¶æ•°æ®ç»“æœ
        """
        logger.info(f"âš¡ ä½¿ç”¨å®æ—¶æ•°æ®æºè¡¥å……è·å– {len(symbols)} åªè‚¡ç¥¨æ•°æ®...")
        
        # å®æ—¶æ•°æ®æºä¼˜å…ˆçº§ï¼šä¸œæ–¹è´¢å¯Œ -> è…¾è®¯ -> æ–°æµª -> é›ªçƒ
        realtime_order = ['eastmoney', 'tencent', 'sina', 'xueqiu']
        
        data = {}
        source_info = {}
        remaining_symbols = symbols.copy()
        
        for source in realtime_order:
            if not remaining_symbols:
                break
                
            if source not in self.realtime_providers or not self.provider_status.get(source, False):
                continue
            
            try:
                provider = self.realtime_providers[source]
                logger.info(f"ğŸ”„ å°è¯•ä½¿ç”¨ {source} è·å– {len(remaining_symbols)} åªè‚¡ç¥¨...")
                
                # é€ä¸ªè·å–ï¼ˆå®æ—¶æ•°æ®æºé€šå¸¸ä¸æ”¯æŒæ‰¹é‡ï¼‰
                successful_symbols = []
                for symbol in remaining_symbols:
                    try:
                        if request.data_type == DataType.HISTORICAL:
                            result = provider.get_stock_data(symbol, request.start_date, request.end_date)
                        elif request.data_type == DataType.REALTIME_PRICE:
                            result = provider.get_real_time_data(symbol)
                        elif request.data_type == DataType.NEWS:
                            result = provider.get_news_data(symbol)
                        else:
                            continue
                        
                        if result is not None and not (hasattr(result, 'empty') and result.empty):
                            data[symbol] = result
                            source_info[symbol] = source
                            successful_symbols.append(symbol)
                            
                    except Exception as e:
                        logger.debug(f"âŒ {source} è·å– {symbol} å¤±è´¥: {e}")
                        continue
                
                # æ›´æ–°å‰©ä½™éœ€è¦è·å–çš„è‚¡ç¥¨
                remaining_symbols = [s for s in remaining_symbols if s not in successful_symbols]
                
                if successful_symbols:
                    logger.info(f"âœ… {source} è·å–æˆåŠŸ: {len(successful_symbols)} åªè‚¡ç¥¨")
                
            except Exception as e:
                logger.error(f"âŒ {source} è·å–å¼‚å¸¸: {e}")
                continue
        
        if data:
            self.stats['realtime_requests'] += 1
            return {
                'data': data,
                'source_info': source_info
            }
        else:
            logger.warning("âš ï¸ æ‰€æœ‰å®æ—¶æ•°æ®æºéƒ½å¤±è´¥")
            return None

    def _get_tushare_batch_data(self, provider, symbols: List[str], request: DataRequest) -> Dict[str, pd.DataFrame]:
        """è·å–Tushareæ‰¹é‡æ•°æ®"""
        # TODO: å®ç°Tushareæ‰¹é‡è·å–é€»è¾‘
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„Tushareé€‚é…å™¨APIå®ç°
        return {}

    def get_stock_list_batch(self) -> Optional[pd.DataFrame]:
        """
        æ‰¹é‡è·å–è‚¡ç¥¨åˆ—è¡¨
        
        Returns:
            è‚¡ç¥¨åˆ—è¡¨DataFrame
        """
        logger.info("ğŸ“‹ æ‰¹é‡è·å–è‚¡ç¥¨åˆ—è¡¨...")
        
        # ä¼˜å…ˆä½¿ç”¨æ‰¹é‡æ•°æ®æºè·å–è‚¡ç¥¨åˆ—è¡¨
        for source in ['akshare', 'baostock']:
            if source in self.batch_providers and self.provider_status.get(source, False):
                try:
                    provider = self.batch_providers[source]
                    stock_list = provider.get_stock_list()
                    
                    if stock_list is not None and not stock_list.empty:
                        logger.info(f"âœ… ä» {source} è·å–è‚¡ç¥¨åˆ—è¡¨æˆåŠŸ: {len(stock_list)} åªè‚¡ç¥¨")
                        return stock_list
                        
                except Exception as e:
                    logger.error(f"âŒ ä» {source} è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
                    continue
        
        logger.error("âŒ æ— æ³•ä»ä»»ä½•æ‰¹é‡æ•°æ®æºè·å–è‚¡ç¥¨åˆ—è¡¨")
        return None

    def batch_download_all(self, start_date: str = None, end_date: str = None, 
                          data_types: List[DataType] = None) -> Dict[str, Any]:
        """
        æ‰¹é‡ä¸‹è½½æ‰€æœ‰è‚¡ç¥¨æ•°æ®
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            data_types: è¦ä¸‹è½½çš„æ•°æ®ç±»å‹åˆ—è¡¨
            
        Returns:
            ä¸‹è½½ç»“æœç»Ÿè®¡
        """
        if data_types is None:
            data_types = [DataType.HISTORICAL]
        
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡ä¸‹è½½æ‰€æœ‰è‚¡ç¥¨æ•°æ®: {[dt.value for dt in data_types]}")
        
        # 1. è·å–è‚¡ç¥¨åˆ—è¡¨
        stock_list = self.get_stock_list_batch()
        if stock_list is None or stock_list.empty:
            return {'error': 'æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨'}
        
        symbols = stock_list['code'].tolist() if 'code' in stock_list.columns else stock_list['symbol'].tolist()
        
        # 2. åˆ†æ‰¹ä¸‹è½½ä¸åŒç±»å‹çš„æ•°æ®
        results = {}
        total_start_time = time.time()
        
        for data_type in data_types:
            logger.info(f"ğŸ“Š ä¸‹è½½ {data_type.value} æ•°æ®...")
            
            request = DataRequest(
                symbols=symbols,
                data_type=data_type,
                start_date=start_date,
                end_date=end_date,
                batch_preferred=True
            )
            
            response = self.get_data(request)
            results[data_type.value] = response
        
        # 3. ç”Ÿæˆæ€»ä½“ç»Ÿè®¡
        total_elapsed = time.time() - total_start_time
        total_stats = {
            'total_symbols': len(symbols),
            'data_types': [dt.value for dt in data_types],
            'total_elapsed_time': total_elapsed,
            'results': {dt.value: resp.stats for dt, resp in zip(data_types, results.values())},
            'overall_success_rate': sum(resp.stats.get('success_rate', 0) for resp in results.values()) / len(data_types)
        }
        
        logger.info(f"ğŸ‰ æ‰¹é‡ä¸‹è½½å®Œæˆ: æ€»è€—æ—¶{total_elapsed:.2f}ç§’, å¹³å‡æˆåŠŸç‡{total_stats['overall_success_rate']:.1f}%")
        
        return {
            'stats': total_stats,
            'results': results
        }

    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_requests': self.stats['total_requests'],
            'batch_requests': self.stats['batch_requests'],
            'realtime_requests': self.stats['realtime_requests'],
            'cache_hits': self.stats['cache_hits'],
            'provider_status': self.provider_status.copy(),
            'recent_performance': self.stats['performance_log'][-10:] if self.stats['performance_log'] else []
        }


# å•ä¾‹æ¨¡å¼
_tiered_manager_instance = None

def get_tiered_data_manager() -> TieredDataManager:
    """è·å–åˆ†å±‚æ•°æ®ç®¡ç†å™¨å•ä¾‹"""
    global _tiered_manager_instance
    if _tiered_manager_instance is None:
        _tiered_manager_instance = TieredDataManager()
    return _tiered_manager_instance


# ä¾¿æ·å‡½æ•°
def smart_get_stock_data(symbols: Union[str, List[str]], start_date: str = None, 
                        end_date: str = None, batch_preferred: bool = True) -> Dict[str, pd.DataFrame]:
    """
    æ™ºèƒ½è·å–è‚¡ç¥¨å†å²æ•°æ®
    
    Args:
        symbols: è‚¡ç¥¨ä»£ç æˆ–è‚¡ç¥¨ä»£ç åˆ—è¡¨
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        batch_preferred: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨æ‰¹é‡æ•°æ®æº
        
    Returns:
        {symbol: DataFrame} æ•°æ®å­—å…¸
    """
    if isinstance(symbols, str):
        symbols = [symbols]
    
    manager = get_tiered_data_manager()
    request = DataRequest(
        symbols=symbols,
        data_type=DataType.HISTORICAL,
        start_date=start_date,
        end_date=end_date,
        batch_preferred=batch_preferred
    )
    
    response = manager.get_data(request)
    return response.data


def smart_batch_download(start_date: str = None, end_date: str = None) -> Dict[str, Any]:
    """
    æ™ºèƒ½æ‰¹é‡ä¸‹è½½æ‰€æœ‰è‚¡ç¥¨æ•°æ®
    
    Args:
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        
    Returns:
        ä¸‹è½½ç»“æœ
    """
    manager = get_tiered_data_manager()
    return manager.batch_download_all(start_date, end_date)


if __name__ == "__main__":
    # æµ‹è¯•åˆ†å±‚æ•°æ®ç®¡ç†å™¨
    manager = get_tiered_data_manager()
    
    # æµ‹è¯•è·å–å•åªè‚¡ç¥¨æ•°æ®
    data = smart_get_stock_data("000001", start_date="2024-01-01", end_date="2024-01-10")
    print(f"è·å–ç»“æœ: {len(data)} åªè‚¡ç¥¨")
    
    # æµ‹è¯•æ‰¹é‡è·å–
    symbols = ["000001", "000002", "600000"]
    batch_data = smart_get_stock_data(symbols, start_date="2024-01-01", end_date="2024-01-10")
    print(f"æ‰¹é‡è·å–ç»“æœ: {len(batch_data)} åªè‚¡ç¥¨")
    
    # æ€§èƒ½ç»Ÿè®¡
    stats = manager.get_performance_stats()
    print(f"æ€§èƒ½ç»Ÿè®¡: {stats}")