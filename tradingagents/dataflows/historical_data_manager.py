#!/usr/bin/env python3
"""
å†å²æ•°æ®ç®¡ç†å™¨
ä¸“é—¨å¤„ç†å†å²ä»·æ ¼æ•°æ®çš„æ°¸ä¹…å­˜å‚¨å’Œé«˜æ•ˆæŸ¥è¯¢
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Union, Tuple
import json
import sqlite3
import threading
from concurrent.futures import ThreadPoolExecutor

from tradingagents.utils.logging_manager import get_logger
from .persistent_storage import get_persistent_manager

logger = get_logger('historical_data')


class HistoricalDataManager:
    """å†å²æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self, data_dir: str = "./data/historical"):
        """
        åˆå§‹åŒ–å†å²æ•°æ®ç®¡ç†å™¨
        
        Args:
            data_dir: å†å²æ•°æ®å­˜å‚¨ç›®å½•
        """
        self.data_dir = Path(data_dir)
        self.persistent_manager = get_persistent_manager()
        self._lock = threading.Lock()
        
        # SQLiteæ•°æ®åº“ç”¨äºç´¢å¼•å’Œå¿«é€ŸæŸ¥è¯¢
        self.db_path = self.data_dir / "historical_index.db"
        self._init_database()
        
        logger.info("ğŸ“ˆ å†å²æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ—„ï¸ æ•°æ®åº“è·¯å¾„: {self.db_path}")
    
    def _init_database(self):
        """åˆå§‹åŒ–SQLiteæ•°æ®åº“"""
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        with sqlite3.connect(self.db_path) as conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS stock_data_index (
                    symbol TEXT PRIMARY KEY,
                    frequency TEXT,
                    start_date TEXT,
                    end_date TEXT,
                    record_count INTEGER,
                    file_path TEXT,
                    last_updated TEXT,
                    checksum TEXT,
                    metadata TEXT
                )
            ''')
            
            conn.execute('''
                CREATE TABLE IF NOT EXISTS data_gaps (
                    symbol TEXT,
                    frequency TEXT,
                    gap_start TEXT,
                    gap_end TEXT,
                    PRIMARY KEY (symbol, frequency, gap_start, gap_end)
                )
            ''')
            
            conn.commit()
    
    def save_historical_data(self, symbol: str, data: pd.DataFrame, 
                           frequency: str = "daily", overwrite: bool = False) -> bool:
        """
        ä¿å­˜å†å²ä»·æ ¼æ•°æ®
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            data: ä»·æ ¼æ•°æ®DataFrame
            frequency: æ•°æ®é¢‘ç‡ (daily, weekly, monthly)
            overwrite: æ˜¯å¦è¦†ç›–ç°æœ‰æ•°æ®
        
        Returns:
            æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        if data.empty or 'date' not in data.columns:
            logger.warning(f"âš ï¸ æ— æ•ˆæ•°æ®æ ¼å¼: {symbol}")
            return False
        
        try:
            # ç¡®ä¿æ•°æ®æŒ‰æ—¥æœŸæ’åº
            data = data.sort_values('date')
            data['date'] = pd.to_datetime(data['date'])
            
            # æ£€æŸ¥ç°æœ‰æ•°æ®
            existing_data = self.load_historical_data(symbol, frequency)
            if existing_data is not None and not overwrite:
                # åˆå¹¶æ•°æ®ï¼Œé¿å…é‡å¤
                combined_data = pd.concat([existing_data, data])
                combined_data = combined_data.drop_duplicates(subset=['date'], keep='last')
                combined_data = combined_data.sort_values('date')
                data = combined_data
            
            # ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨
            self.persistent_manager.save_historical_prices(symbol, data, frequency)
            
            # æ›´æ–°æ•°æ®åº“ç´¢å¼•
            self._update_data_index(symbol, frequency, data)
            
            logger.info(f"âœ… ä¿å­˜å†å²æ•°æ®: {symbol} ({frequency}) - {len(data)} æ¡è®°å½•")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å†å²æ•°æ®å¤±è´¥: {symbol} - {e}")
            return False
    
    def load_historical_data(self, symbol: str, frequency: str = "daily",
                           start_date: Optional[str] = None,
                           end_date: Optional[str] = None) -> Optional[pd.DataFrame]:
        """
        åŠ è½½å†å²ä»·æ ¼æ•°æ®
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            frequency: æ•°æ®é¢‘ç‡
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
        
        Returns:
            ä»·æ ¼æ•°æ®DataFrameæˆ–None
        """
        try:
            return self.persistent_manager.load_historical_prices(
                symbol, frequency, start_date, end_date
            )
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å†å²æ•°æ®å¤±è´¥: {symbol} - {e}")
            return None
    
    def _update_data_index(self, symbol: str, frequency: str, data: pd.DataFrame):
        """æ›´æ–°æ•°æ®ç´¢å¼•"""
        if data.empty or 'date' not in data.columns:
            return
        
        start_date = data['date'].min().strftime('%Y-%m-%d')
        end_date = data['date'].max().strftime('%Y-%m-%d')
        record_count = len(data)
        
        file_path = str(self.persistent_manager._get_file_path("historical", symbol, frequency))
        checksum = self.persistent_manager._calculate_checksum(data)
        
        metadata = {
            "columns": list(data.columns),
            "data_types": str(data.dtypes.to_dict())
        }
        
        with sqlite3.connect(self.db_path) as conn:
            conn.execute('''
                INSERT OR REPLACE INTO stock_data_index 
                (symbol, frequency, start_date, end_date, record_count, 
                 file_path, last_updated, checksum, metadata)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                symbol, frequency, start_date, end_date, record_count,
                file_path, datetime.now().isoformat(), checksum,
                json.dumps(metadata)
            ))
            conn.commit()
    
    def get_data_availability(self, symbol: str, frequency: str = "daily") -> Dict[str, Any]:
        """è·å–æ•°æ®å¯ç”¨æ€§ä¿¡æ¯"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute('''
                SELECT start_date, end_date, record_count, last_updated
                FROM stock_data_index
                WHERE symbol = ? AND frequency = ?
            ''', (symbol, frequency))
            
            result = cursor.fetchone()
            
            if result:
                return {
                    "available": True,
                    "start_date": result[0],
                    "end_date": result[1],
                    "record_count": result[2],
                    "last_updated": result[3]
                }
            else:
                return {
                    "available": False,
                    "start_date": None,
                    "end_date": None,
                    "record_count": 0,
                    "last_updated": None
                }
    
    def get_missing_date_ranges(self, symbol: str, frequency: str = "daily",
                              start_date: str = None, end_date: str = None) -> List[Dict[str, str]]:
        """è·å–ç¼ºå¤±çš„æ—¥æœŸèŒƒå›´"""
        availability = self.get_data_availability(symbol, frequency)
        
        if not availability["available"]:
            return [{"start": start_date or "2000-01-01", "end": end_date or datetime.now().strftime('%Y-%m-%d')}]
        
        data_start = pd.to_datetime(availability["start_date"])
        data_end = pd.to_datetime(availability["end_date"])
        
        target_start = pd.to_datetime(start_date) if start_date else data_start
        target_end = pd.to_datetime(end_date) if end_date else pd.to_datetime(datetime.now().strftime('%Y-%m-%d'))
        
        missing_ranges = []
        
        if target_start < data_start:
            missing_ranges.append({
                "start": target_start.strftime('%Y-%m-%d'),
                "end": (data_start - timedelta(days=1)).strftime('%Y-%m-%d')
            })
        
        if target_end > data_end:
            missing_ranges.append({
                "start": (data_end + timedelta(days=1)).strftime('%Y-%m-%d'),
                "end": target_end.strftime('%Y-%m-%d')
            })
        
        return missing_ranges
    
    def list_available_symbols(self, frequency: str = "daily") -> List[str]:
        """è·å–å¯ç”¨çš„è‚¡ç¥¨ä»£ç åˆ—è¡¨"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute('''
                SELECT DISTINCT symbol FROM stock_data_index
                WHERE frequency = ?
            ''', (frequency,))
            
            return [row[0] for row in cursor.fetchall()]
    
    def get_data_summary(self, frequency: str = "daily") -> Dict[str, Any]:
        """è·å–æ•°æ®æ‘˜è¦ä¿¡æ¯"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute('''
                SELECT COUNT(DISTINCT symbol), SUM(record_count),
                       MIN(start_date), MAX(end_date)
                FROM stock_data_index
                WHERE frequency = ?
            ''', (frequency,))
            
            result = cursor.fetchone()
            
            return {
                "total_symbols": result[0] or 0,
                "total_records": result[1] or 0,
                "earliest_date": result[2],
                "latest_date": result[3],
                "frequency": frequency
            }
    
    def verify_data_integrity(self, symbol: str, frequency: str = "daily") -> Dict[str, Any]:
        """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
        result = {
            "symbol": symbol,
            "frequency": frequency,
            "integrity_check": "failed",
            "issues": []
        }
        
        try:
            # ä»ç´¢å¼•è·å–æ–‡ä»¶ä¿¡æ¯
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute('''
                    SELECT file_path, checksum, record_count, start_date, end_date
                    FROM stock_data_index
                    WHERE symbol = ? AND frequency = ?
                ''', (symbol, frequency))
                
                index_info = cursor.fetchone()
                
                if not index_info:
                    result["issues"].append("æ•°æ®ç´¢å¼•ä¸å­˜åœ¨")
                    return result
            
            file_path = Path(index_info[0])
            expected_checksum = index_info[1]
            expected_count = index_info[2]
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not file_path.exists():
                result["issues"].append("æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
                return result
            
            # åŠ è½½æ•°æ®å¹¶éªŒè¯
            data = self.load_historical_data(symbol, frequency)
            if data is None or data.empty:
                result["issues"].append("æ•°æ®ä¸ºç©ºæˆ–æ— æ³•åŠ è½½")
                return result
            
            # éªŒè¯è®°å½•æ•°é‡
            actual_count = len(data)
            if actual_count != expected_count:
                result["issues"].append(f"è®°å½•æ•°é‡ä¸åŒ¹é…: æœŸæœ›{expected_count}, å®é™…{actual_count}")
            
            # éªŒè¯æ—¥æœŸèŒƒå›´
            actual_start = data['date'].min().strftime('%Y-%m-%d')
            actual_end = data['date'].max().strftime('%Y-%m-%d')
            
            if actual_start != index_info[3] or actual_end != index_info[4]:
                result["issues"].append("æ—¥æœŸèŒƒå›´ä¸åŒ¹é…")
            
            # éªŒè¯æ•°æ®è¿ç»­æ€§
            expected_days = (pd.to_datetime(actual_end) - pd.to_datetime(actual_start)).days + 1
            if frequency == "daily" and actual_count != expected_days:
                result["issues"].append(f"æ•°æ®ä¸è¿ç»­: æœŸæœ›{expected_days}å¤©, å®é™…{actual_count}æ¡è®°å½•")
            
            if not result["issues"]:
                result["integrity_check"] = "passed"
            
        except Exception as e:
            result["issues"].append(f"éªŒè¯é”™è¯¯: {str(e)}")
        
        return result
    
    def bulk_save_data(self, data_dict: Dict[str, pd.DataFrame], 
                      frequency: str = "daily", max_workers: int = 4) -> Dict[str, bool]:
        """æ‰¹é‡ä¿å­˜æ•°æ®"""
        results = {}
        
        def save_single(symbol_data):
            symbol, data = symbol_data
            return symbol, self.save_historical_data(symbol, data, frequency)
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_symbol = {
                executor.submit(save_single, (symbol, data)): symbol
                for symbol, data in data_dict.items()
            }
            
            for future in future_to_symbol:
                symbol = future_to_symbol[future]
                try:
                    symbol, success = future.result()
                    results[symbol] = success
                except Exception as e:
                    logger.error(f"âŒ æ‰¹é‡ä¿å­˜å¤±è´¥: {symbol} - {e}")
                    results[symbol] = False
        
        return results
    
    def export_data(self, symbol: str, frequency: str = "daily", 
                   format: str = "csv", output_path: Optional[str] = None) -> bool:
        """å¯¼å‡ºæ•°æ®"""
        data = self.load_historical_data(symbol, frequency)
        if data is None or data.empty:
            logger.warning(f"âš ï¸ æ— æ•°æ®å¯å¯¼å‡º: {symbol}")
            return False
        
        if output_path is None:
            output_path = self.data_dir / "exports" / f"{symbol}_{frequency}.{format}"
        
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        try:
            if format.lower() == "csv":
                data.to_csv(output_path, index=False, encoding='utf-8')
            elif format.lower() == "json":
                data.to_json(output_path, orient='records', indent=2, force_ascii=False)
            elif format.lower() == "excel":
                data.to_excel(output_path, index=False)
            else:
                logger.error(f"âŒ ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {format}")
                return False
            
            logger.info(f"âœ… å¯¼å‡ºæ•°æ®: {symbol} -> {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å¯¼å‡ºæ•°æ®å¤±è´¥: {symbol} - {e}")
            return False


# å…¨å±€å®ä¾‹
_historical_manager = None


def get_historical_manager(data_dir: str = "./data/historical") -> HistoricalDataManager:
    """è·å–å…¨å±€å†å²æ•°æ®ç®¡ç†å™¨å®ä¾‹"""
    global _historical_manager
    if _historical_manager is None:
        _historical_manager = HistoricalDataManager(data_dir)
    return _historical_manager