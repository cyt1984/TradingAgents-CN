#!/usr/bin/env python3
"""
GLM-4.5 æ¨¡å‹é€‚é…å™¨
æ”¯æŒæ™ºè°± GLM-4.5 æ¨¡å‹çš„é€‚é…å™¨å®ç°
"""

import os
import time
from typing import Any, Dict, List, Optional, Union
from langchain_core.messages import BaseMessage
from langchain_core.outputs import ChatResult
from langchain_core.callbacks import CallbackManagerForLLMRun

# å¯¼å…¥åŸºç±»
from .openai_compatible_base import OpenAICompatibleBase

# å¯¼å…¥æ—¥å¿—æ¨¡å—
from tradingagents.utils.logging_manager import get_logger
logger = get_logger('agents')

# å¯¼å…¥tokenè·Ÿè¸ªå™¨
try:
    from tradingagents.config.config_manager import token_tracker
    TOKEN_TRACKING_ENABLED = True
except ImportError:
    TOKEN_TRACKING_ENABLED = False


class ChatGLM(OpenAICompatibleBase):
    """
    GLM-4.5 æ¨¡å‹é€‚é…å™¨
    åŸºäº OpenAI å…¼å®¹æ¥å£å®ç°
    """
    
    def __init__(
        self,
        model: str = "glm-4-plus",
        api_key: Optional[str] = None,
        base_url: str = "https://open.bigmodel.cn/api/paas/v4",
        temperature: float = 0.1,
        max_tokens: Optional[int] = 4096,
        **kwargs
    ):
        """
        åˆå§‹åŒ– GLM-4.5 é€‚é…å™¨
        
        Args:
            model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ "glm-4-plus"
            api_key: APIå¯†é’¥ï¼Œå¦‚ä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡ GLM_API_KEY è·å–
            base_url: APIåŸºç¡€URL
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            **kwargs: å…¶ä»–å‚æ•°
        """
        
        # ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
        if api_key is None:
            api_key = os.getenv("GLM_API_KEY")
            if not api_key:
                raise ValueError(
                    "GLM APIå¯†é’¥æœªæä¾›ã€‚è¯·è®¾ç½®ç¯å¢ƒå˜é‡ GLM_API_KEY æˆ–åœ¨åˆå§‹åŒ–æ—¶æä¾› api_key å‚æ•°ã€‚"
                )
        
        # è°ƒç”¨åŸºç±»åˆå§‹åŒ–
        super().__init__(
            provider_name="GLM",
            model=model,
            api_key_env_var="GLM_API_KEY",
            base_url=base_url,
            api_key=api_key,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )
        
        # GLMç‰¹å®šé…ç½®
        self.supports_system_message = True
        self.supports_function_calling = True
        self.context_window = 128000  # GLM-4.5 æ”¯æŒ128Kä¸Šä¸‹æ–‡
        self.max_output_tokens = 4096
        
        logger.info(f"âœ… GLM-4.5 é€‚é…å™¨åˆå§‹åŒ–æˆåŠŸ")
        logger.info(f"   æ¨¡å‹: {self.model_name}")
        logger.info(f"   åŸºç¡€URL: {self.openai_api_base}")
        logger.info(f"   ä¸Šä¸‹æ–‡çª—å£: {self.context_window:,} tokens")
    
    def _create_chat_result(self, response: Any) -> ChatResult:
        """
        åˆ›å»ºèŠå¤©ç»“æœï¼ŒåŒ…å«tokenä½¿ç”¨ç»Ÿè®¡
        """
        try:
            result = super()._create_chat_result(response)
            
            # è®°å½•tokenä½¿ç”¨æƒ…å†µ
            if TOKEN_TRACKING_ENABLED and hasattr(response, 'usage'):
                usage = response.usage
                input_tokens = getattr(usage, 'prompt_tokens', 0)
                output_tokens = getattr(usage, 'completion_tokens', 0)
                total_tokens = getattr(usage, 'total_tokens', input_tokens + output_tokens)
                
                # è®°å½•åˆ°tokenè·Ÿè¸ªå™¨
                token_tracker.record_usage(
                    provider="GLM",
                    model=self.model_name,
                    input_tokens=input_tokens,
                    output_tokens=output_tokens,
                    total_tokens=total_tokens
                )
                
                logger.debug(f"ğŸ¯ GLM tokenä½¿ç”¨: è¾“å…¥={input_tokens}, è¾“å‡º={output_tokens}, æ€»è®¡={total_tokens}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºGLMèŠå¤©ç»“æœå¤±è´¥: {e}")
            raise
    
    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """
        ç”ŸæˆèŠå¤©å›å¤
        """
        start_time = time.time()
        
        try:
            logger.debug(f"ğŸš€ å¼€å§‹GLM-4.5ç”Ÿæˆï¼Œæ¶ˆæ¯æ•°é‡: {len(messages)}")
            
            # è°ƒç”¨åŸºç±»æ–¹æ³•
            result = super()._generate(messages, stop, run_manager, **kwargs)
            
            end_time = time.time()
            generation_time = end_time - start_time
            
            logger.info(f"âœ… GLM-4.5ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {generation_time:.2f}ç§’")
            
            return result
            
        except Exception as e:
            end_time = time.time()
            generation_time = end_time - start_time
            
            logger.error(f"âŒ GLM-4.5ç”Ÿæˆå¤±è´¥ï¼Œè€—æ—¶: {generation_time:.2f}ç§’ï¼Œé”™è¯¯: {e}")
            raise
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        """
        return {
            "provider": "GLM",
            "model": self.model_name,
            "max_tokens": self.max_tokens,
            "temperature": self.temperature,
            "context_window": self.context_window,
            "supports_function_calling": self.supports_function_calling,
            "supports_system_message": self.supports_system_message,
            "base_url": self.openai_api_base
        }
    
    @classmethod
    def get_available_models(cls) -> List[str]:
        """
        è·å–å¯ç”¨çš„ GLM æ¨¡å‹åˆ—è¡¨
        """
        return [
            "glm-4-plus",         # GLM-4.5 ä¸»æ¨¡å‹  
            "glm-4-0520",         # GLM-4 æ ‡å‡†ç‰ˆ
            "glm-4-air",          # GLM-4 è½»é‡ç‰ˆ
            "glm-4-airx",         # GLM-4 å¢å¼ºç‰ˆ
            "glm-4-flash",        # GLM-4 å¿«é€Ÿç‰ˆ
            "glm-4",              # GLM-4 é€šç”¨ç‰ˆ
            "glm-3-turbo"         # GLM-3 åŠ é€Ÿç‰ˆ
        ]
    
    def estimate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """
        ä¼°ç®—ä½¿ç”¨æˆæœ¬ï¼ˆäººæ°‘å¸ï¼‰
        åŸºäº GLM çš„å®šä»·ç­–ç•¥
        """
        # GLM-4.5 å®šä»· (å‚è€ƒä»·æ ¼ï¼Œå®é™…ä»·æ ¼è¯·æŸ¥çœ‹å®˜ç½‘)
        # GLM-4-Plus: è¾“å…¥ Â¥0.05/1K tokens, è¾“å‡º Â¥0.15/1K tokens
        # GLM-4: è¾“å…¥ Â¥0.01/1K tokens, è¾“å‡º Â¥0.03/1K tokens
        
        if "plus" in self.model_name.lower():
            # GLM-4-Plus å®šä»·
            input_cost = (input_tokens / 1000) * 0.05
            output_cost = (output_tokens / 1000) * 0.15
        elif "flash" in self.model_name.lower():
            # GLM-4-Flash å®šä»·ï¼ˆæ›´ä¾¿å®œï¼‰
            input_cost = (input_tokens / 1000) * 0.001
            output_cost = (output_tokens / 1000) * 0.002
        else:
            # æ ‡å‡† GLM-4 å®šä»·
            input_cost = (input_tokens / 1000) * 0.01
            output_cost = (output_tokens / 1000) * 0.03
        
        total_cost = input_cost + output_cost
        return total_cost


def create_glm_adapter(
    model: str = "glm-4-plus",
    temperature: float = 0.1,
    max_tokens: Optional[int] = 4096,
    **kwargs
) -> ChatGLM:
    """
    åˆ›å»º GLM é€‚é…å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        model: æ¨¡å‹åç§°
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°
        **kwargs: å…¶ä»–å‚æ•°
    
    Returns:
        ChatGLM: GLMé€‚é…å™¨å®ä¾‹
    """
    return ChatGLM(
        model=model,
        temperature=temperature,
        max_tokens=max_tokens,
        **kwargs
    )


# ä¸ºäº†å…¼å®¹æ€§ï¼Œæä¾›å¤šä¸ªåˆ«å
ChatZhipu = ChatGLM  # æ™ºè°±åˆ«å
create_zhipu_adapter = create_glm_adapter  # æ™ºè°±åˆ«å


def get_glm_adapter(
    model: str = "glm-4-plus",
    temperature: float = 0.1,
    max_tokens: Optional[int] = 4096
) -> ChatGLM:
    """
    è·å– GLM é€‚é…å™¨å®ä¾‹
    
    Args:
        model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ "glm-4-plus"
        temperature: æ¸©åº¦å‚æ•°ï¼Œé»˜è®¤ 0.1
        max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°ï¼Œé»˜è®¤ 4096
    
    Returns:
        ChatGLM: GLMé€‚é…å™¨å®ä¾‹
    """
    try:
        adapter = create_glm_adapter(
            model=model,
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        logger.info(f"âœ… GLMé€‚é…å™¨åˆ›å»ºæˆåŠŸ: {model}")
        return adapter
        
    except Exception as e:
        logger.error(f"âŒ GLMé€‚é…å™¨åˆ›å»ºå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    try:
        # åˆ›å»ºé€‚é…å™¨
        glm = create_glm_adapter()
        
        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        info = glm.get_model_info()
        print("GLM æ¨¡å‹ä¿¡æ¯:")
        for key, value in info.items():
            print(f"  {key}: {value}")
        
        # æ˜¾ç¤ºå¯ç”¨æ¨¡å‹
        models = glm.get_available_models()
        print(f"\nå¯ç”¨æ¨¡å‹: {', '.join(models)}")
        
        # æˆæœ¬ä¼°ç®—ç¤ºä¾‹
        cost = glm.estimate_cost(1000, 500)
        print(f"\næˆæœ¬ä¼°ç®— (1000è¾“å…¥+500è¾“å‡ºtokens): Â¥{cost:.4f}")
        
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")