#!/usr/bin/env python3
"""
åŠ¨æ€LLMç®¡ç†å™¨
æ”¯æŒç”¨æˆ·åœ¨è¿è¡Œæ—¶é€‰æ‹©å’Œåˆ‡æ¢ä¸åŒçš„AIæ¨¡å‹æä¾›å•†
è§£å†³å›ºå®šOpenAIæ¨¡å‹çš„é—®é¢˜ï¼Œæ”¯æŒå¤šç§LLMæä¾›å•†
"""

import os
import json
from typing import Dict, Any, Optional, List, Union
from dataclasses import dataclass, asdict
from pathlib import Path
from enum import Enum

from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI
from tradingagents.llm_adapters import ChatDashScope, ChatDashScopeOpenAI

from tradingagents.utils.logging_manager import get_logger
logger = get_logger('llm_manager')


class LLMProvider(Enum):
    """LLMæä¾›å•†æšä¸¾"""
    OPENAI = "openai"
    DASHSCOPE = "dashscope"
    DEEPSEEK = "deepseek"
    GOOGLE = "google"
    ANTHROPIC = "anthropic"
    OPENROUTER = "openrouter"
    OLLAMA = "ollama"
    KIMI = "kimi"


@dataclass
class LLMConfig:
    """LLMé…ç½®"""
    provider: str                          # æä¾›å•†
    model_name: str                        # æ¨¡å‹åç§°
    api_key: Optional[str] = None          # APIå¯†é’¥
    base_url: Optional[str] = None         # åŸºç¡€URL
    max_tokens: int = 4000                 # æœ€å¤§tokenæ•°
    temperature: float = 0.7               # æ¸©åº¦å‚æ•°
    enabled: bool = True                   # æ˜¯å¦å¯ç”¨
    display_name: Optional[str] = None     # æ˜¾ç¤ºåç§°
    description: Optional[str] = None      # æè¿°


class DynamicLLMManager:
    """åŠ¨æ€LLMç®¡ç†å™¨"""
    
    def __init__(self, config_file: str = "config/llm_models.json"):
        """
        åˆå§‹åŒ–LLMç®¡ç†å™¨
        
        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_file = Path(config_file)
        self.config_file.parent.mkdir(exist_ok=True)
        
        # å½“å‰æ´»è·ƒçš„LLMé…ç½®
        self.current_config = None
        self.current_llm = None
        
        # å¯ç”¨çš„LLMé…ç½®
        self.available_configs = {}
        
        # åˆå§‹åŒ–é»˜è®¤é…ç½®
        self._init_default_configs()
        
        # åŠ è½½é…ç½®æ–‡ä»¶
        self._load_configs()
        
        logger.info("ğŸ¤– åŠ¨æ€LLMç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   å¯ç”¨æ¨¡å‹: {len(self.available_configs)} ä¸ª")

    def _init_default_configs(self):
        """åˆå§‹åŒ–é»˜è®¤LLMé…ç½®"""
        self.default_configs = {
            # OpenAIç³»åˆ—
            "openai_gpt4o": LLMConfig(
                provider="openai",
                model_name="gpt-4o",
                base_url="https://api.openai.com/v1",
                display_name="GPT-4o",
                description="OpenAIæœ€æ–°çš„å¤šæ¨¡æ€æ¨¡å‹"
            ),
            "openai_gpt4o_mini": LLMConfig(
                provider="openai", 
                model_name="gpt-4o-mini",
                base_url="https://api.openai.com/v1",
                display_name="GPT-4o Mini",
                description="OpenAIç»æµå‹æ¨¡å‹"
            ),
            
            # DashScopeç³»åˆ—
            "dashscope_qwen_max": LLMConfig(
                provider="dashscope",
                model_name="qwen-max",
                display_name="Qwen Max",
                description="é˜¿é‡Œäº‘é€šä¹‰åƒé—®æœ€å¼ºæ¨¡å‹"
            ),
            "dashscope_qwen_plus": LLMConfig(
                provider="dashscope",
                model_name="qwen-plus", 
                display_name="Qwen Plus",
                description="é˜¿é‡Œäº‘é€šä¹‰åƒé—®å¹³è¡¡æ¨¡å‹"
            ),
            "dashscope_qwen_turbo": LLMConfig(
                provider="dashscope",
                model_name="qwen-turbo",
                display_name="Qwen Turbo",
                description="é˜¿é‡Œäº‘é€šä¹‰åƒé—®é«˜é€Ÿæ¨¡å‹"
            ),
            
            # DeepSeekç³»åˆ—
            "deepseek_chat": LLMConfig(
                provider="deepseek",
                model_name="deepseek-chat",
                base_url="https://api.deepseek.com/v1",
                display_name="DeepSeek Chat",
                description="DeepSeekå¯¹è¯æ¨¡å‹"
            ),
            "deepseek_coder": LLMConfig(
                provider="deepseek", 
                model_name="deepseek-coder",
                base_url="https://api.deepseek.com/v1",
                display_name="DeepSeek Coder",
                description="DeepSeekä»£ç æ¨¡å‹"
            ),
            
            # Googleç³»åˆ—
            "google_gemini_pro": LLMConfig(
                provider="google",
                model_name="gemini-pro",
                display_name="Gemini Pro",
                description="Google Geminiä¸“ä¸šç‰ˆ"
            ),
            
            # OpenRouterç³»åˆ—
            "openrouter_claude": LLMConfig(
                provider="openrouter",
                model_name="anthropic/claude-3-sonnet",
                base_url="https://openrouter.ai/api/v1",
                display_name="Claude 3 Sonnet",
                description="OpenRouterçš„Claudeæ¨¡å‹"
            ),
            
            # Kimiç³»åˆ—
            "kimi_chat": LLMConfig(
                provider="kimi",
                model_name="moonshot-v1-8k",
                base_url="https://api.moonshot.cn/v1",
                display_name="Kimi Chat",
                description="æœˆä¹‹æš—é¢Kimiæ¨¡å‹"
            ),
            
            # GLMç³»åˆ—
            "glm_chat": LLMConfig(
                provider="glm",
                model_name="glm-4-plus",
                base_url="https://open.bigmodel.cn/api/paas/v4/",
                display_name="GLM-4 Plus",
                description="æ™ºè°±AI GLM-4 Plusæ¨¡å‹"
            )
        }

    def _load_configs(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if self.config_file.exists():
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # åŠ è½½è‡ªå®šä¹‰é…ç½®
                for key, config_dict in data.get('custom_configs', {}).items():
                    self.available_configs[key] = LLMConfig(**config_dict)
                
                # è®¾ç½®å½“å‰é…ç½®
                current_key = data.get('current_config')
                if current_key and current_key in self.available_configs:
                    self.current_config = self.available_configs[current_key]
                    
            except Exception as e:
                logger.warning(f"âš ï¸ åŠ è½½LLMé…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        
        # åˆå¹¶é»˜è®¤é…ç½®
        for key, config in self.default_configs.items():
            if key not in self.available_configs:
                # ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
                config.api_key = self._get_api_key_from_env(config.provider)
                config.enabled = bool(config.api_key)  # æœ‰APIå¯†é’¥æ‰å¯ç”¨
                self.available_configs[key] = config

    def _get_api_key_from_env(self, provider: str) -> Optional[str]:
        """ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥"""
        env_key_map = {
            "openai": "OPENAI_API_KEY",
            "dashscope": "DASHSCOPE_API_KEY", 
            "deepseek": "DEEPSEEK_API_KEY",
            "google": "GOOGLE_API_KEY",
            "anthropic": "ANTHROPIC_API_KEY",
            "openrouter": "OPENROUTER_API_KEY",
            "kimi": "MOONSHOT_API_KEY",
            "glm": "GLM_API_KEY"
        }
        
        env_key = env_key_map.get(provider)
        return os.getenv(env_key) if env_key else None

    def save_configs(self):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        try:
            # åªä¿å­˜è‡ªå®šä¹‰é…ç½®
            custom_configs = {}
            for key, config in self.available_configs.items():
                if key not in self.default_configs:
                    custom_configs[key] = asdict(config)
            
            data = {
                'custom_configs': custom_configs,
                'current_config': self._get_current_config_key()
            }
            
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                
            logger.info(f"ğŸ’¾ LLMé…ç½®å·²ä¿å­˜åˆ°: {self.config_file}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜LLMé…ç½®å¤±è´¥: {e}")

    def get_available_models(self) -> Dict[str, Dict[str, Any]]:
        """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        models = {}
        for key, config in self.available_configs.items():
            models[key] = {
                'provider': config.provider,
                'model_name': config.model_name,
                'display_name': config.display_name or config.model_name,
                'description': config.description or f"{config.provider} {config.model_name}",
                'enabled': config.enabled,
                'has_api_key': bool(config.api_key)
            }
        return models

    def get_enabled_models(self) -> Dict[str, Dict[str, Any]]:
        """è·å–å·²å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        models = self.get_available_models()
        return {k: v for k, v in models.items() if v['enabled'] and v['has_api_key']}

    def set_current_model(self, model_key: str) -> bool:
        """
        è®¾ç½®å½“å‰ä½¿ç”¨çš„æ¨¡å‹
        
        Args:
            model_key: æ¨¡å‹é”®å€¼
            
        Returns:
            æ˜¯å¦è®¾ç½®æˆåŠŸ
        """
        if model_key not in self.available_configs:
            logger.error(f"âŒ æ¨¡å‹ {model_key} ä¸å­˜åœ¨")
            return False
        
        config = self.available_configs[model_key]
        
        if not config.enabled or not config.api_key:
            logger.error(f"âŒ æ¨¡å‹ {model_key} æœªå¯ç”¨æˆ–ç¼ºå°‘APIå¯†é’¥")
            return False
        
        try:
            # åˆ›å»ºLLMå®ä¾‹
            llm = self._create_llm_instance(config)
            
            self.current_config = config
            self.current_llm = llm
            
            logger.info(f"âœ… å·²åˆ‡æ¢åˆ°æ¨¡å‹: {config.display_name or config.model_name}")
            
            # ä¿å­˜é…ç½®
            self.save_configs()
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆ‡æ¢æ¨¡å‹å¤±è´¥: {e}")
            return False

    def _create_llm_instance(self, config: LLMConfig):
        """åˆ›å»ºLLMå®ä¾‹"""
        provider = config.provider
        
        if provider == "openai":
            return ChatOpenAI(
                model=config.model_name,
                api_key=config.api_key,
                base_url=config.base_url,
                max_tokens=config.max_tokens,
                temperature=config.temperature
            )
            
        elif provider == "dashscope":
            return ChatDashScope(
                model=config.model_name,
                dashscope_api_key=config.api_key,
                max_tokens=config.max_tokens,
                temperature=config.temperature
            )
            
        elif provider == "deepseek":
            return ChatOpenAI(
                model=config.model_name,
                api_key=config.api_key,
                base_url=config.base_url,
                max_tokens=config.max_tokens,
                temperature=config.temperature
            )
            
        elif provider == "google":
            return ChatGoogleGenerativeAI(
                model=config.model_name,
                google_api_key=config.api_key,
                max_tokens=config.max_tokens,
                temperature=config.temperature
            )
            
        elif provider == "anthropic":
            return ChatAnthropic(
                model=config.model_name,
                api_key=config.api_key,
                base_url=config.base_url,
                max_tokens=config.max_tokens,
                temperature=config.temperature
            )
            
        elif provider == "openrouter":
            return ChatOpenAI(
                model=config.model_name,
                api_key=config.api_key,
                base_url=config.base_url,
                max_tokens=config.max_tokens,
                temperature=config.temperature
            )
            
        elif provider == "kimi":
            return ChatOpenAI(
                model=config.model_name,
                api_key=config.api_key,
                base_url=config.base_url,
                max_tokens=config.max_tokens,
                temperature=config.temperature
            )
            
        elif provider == "glm":
            return ChatOpenAI(
                model=config.model_name,
                api_key=config.api_key,
                base_url=config.base_url,
                max_tokens=config.max_tokens,
                temperature=config.temperature
            )
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {provider}")

    def get_current_llm(self):
        """è·å–å½“å‰LLMå®ä¾‹"""
        if self.current_llm is None:
            # å°è¯•è‡ªåŠ¨é€‰æ‹©ä¸€ä¸ªå¯ç”¨çš„æ¨¡å‹
            enabled_models = self.get_enabled_models()
            if enabled_models:
                first_model = list(enabled_models.keys())[0]
                logger.info(f"ğŸ”„ è‡ªåŠ¨é€‰æ‹©æ¨¡å‹: {first_model}")
                self.set_current_model(first_model)
        
        return self.current_llm

    def get_current_config(self) -> Optional[LLMConfig]:
        """è·å–å½“å‰é…ç½®"""
        return self.current_config

    def _get_current_config_key(self) -> Optional[str]:
        """è·å–å½“å‰é…ç½®çš„é”®å€¼"""
        if not self.current_config:
            return None
        
        for key, config in self.available_configs.items():
            if config == self.current_config:
                return key
        return None

    def add_custom_model(self, key: str, config: LLMConfig) -> bool:
        """
        æ·»åŠ è‡ªå®šä¹‰æ¨¡å‹é…ç½®
        
        Args:
            key: é…ç½®é”®å€¼
            config: LLMé…ç½®
            
        Returns:
            æ˜¯å¦æ·»åŠ æˆåŠŸ
        """
        try:
            self.available_configs[key] = config
            self.save_configs()
            logger.info(f"âœ… å·²æ·»åŠ è‡ªå®šä¹‰æ¨¡å‹: {key}")
            return True
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ è‡ªå®šä¹‰æ¨¡å‹å¤±è´¥: {e}")
            return False

    def remove_custom_model(self, key: str) -> bool:
        """
        åˆ é™¤è‡ªå®šä¹‰æ¨¡å‹é…ç½®
        
        Args:
            key: é…ç½®é”®å€¼
            
        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        if key in self.default_configs:
            logger.error(f"âŒ æ— æ³•åˆ é™¤é»˜è®¤é…ç½®: {key}")
            return False
        
        if key not in self.available_configs:
            logger.error(f"âŒ é…ç½®ä¸å­˜åœ¨: {key}")
            return False
        
        try:
            del self.available_configs[key]
            
            # å¦‚æœåˆ é™¤çš„æ˜¯å½“å‰é…ç½®ï¼Œé‡ç½®
            if self.current_config == self.available_configs.get(key):
                self.current_config = None
                self.current_llm = None
            
            self.save_configs()
            logger.info(f"âœ… å·²åˆ é™¤è‡ªå®šä¹‰æ¨¡å‹: {key}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤è‡ªå®šä¹‰æ¨¡å‹å¤±è´¥: {e}")
            return False

    def test_model(self, model_key: str) -> Dict[str, Any]:
        """
        æµ‹è¯•æ¨¡å‹è¿æ¥
        
        Args:
            model_key: æ¨¡å‹é”®å€¼
            
        Returns:
            æµ‹è¯•ç»“æœ
        """
        if model_key not in self.available_configs:
            return {
                'success': False,
                'error': f"æ¨¡å‹ {model_key} ä¸å­˜åœ¨"
            }
        
        config = self.available_configs[model_key]
        
        try:
            # åˆ›å»ºä¸´æ—¶LLMå®ä¾‹
            llm = self._create_llm_instance(config)
            
            # å‘é€æµ‹è¯•æ¶ˆæ¯
            response = llm.invoke("Hello, this is a test message. Please respond with 'Test successful'.")
            
            return {
                'success': True,
                'model': config.display_name or config.model_name,
                'provider': config.provider,
                'response': str(response.content) if hasattr(response, 'content') else str(response)
            }
            
        except Exception as e:
            return {
                'success': False,
                'model': config.display_name or config.model_name,
                'provider': config.provider,
                'error': str(e)
            }


# å…¨å±€å®ä¾‹
_llm_manager = None

def get_llm_manager() -> DynamicLLMManager:
    """è·å–LLMç®¡ç†å™¨å•ä¾‹"""
    global _llm_manager
    if _llm_manager is None:
        _llm_manager = DynamicLLMManager()
    return _llm_manager


# ä¾¿æ·å‡½æ•°
def get_current_llm():
    """è·å–å½“å‰LLMå®ä¾‹"""
    return get_llm_manager().get_current_llm()

def set_current_model(model_key: str) -> bool:
    """è®¾ç½®å½“å‰æ¨¡å‹"""
    return get_llm_manager().set_current_model(model_key)

def get_available_models() -> Dict[str, Dict[str, Any]]:
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    return get_llm_manager().get_available_models()

def get_enabled_models() -> Dict[str, Dict[str, Any]]:
    """è·å–å·²å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    return get_llm_manager().get_enabled_models()


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    manager = get_llm_manager()
    
    print("å¯ç”¨æ¨¡å‹:")
    for key, info in manager.get_available_models().items():
        status = "âœ…" if info['enabled'] and info['has_api_key'] else "âŒ"
        print(f"  {status} {key}: {info['display_name']} ({info['provider']})")
    
    # æµ‹è¯•æ¨¡å‹åˆ‡æ¢
    enabled_models = manager.get_enabled_models()
    if enabled_models:
        first_model = list(enabled_models.keys())[0]
        print(f"\næµ‹è¯•åˆ‡æ¢åˆ°: {first_model}")
        success = manager.set_current_model(first_model)
        print(f"åˆ‡æ¢ç»“æœ: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
        
        if success:
            current = manager.get_current_config()
            print(f"å½“å‰æ¨¡å‹: {current.display_name} ({current.provider})")