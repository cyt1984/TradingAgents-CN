#!/usr/bin/env python3
"""
ç›¸ä¼¼è‚¡ç¥¨æ™ºèƒ½æ¨èå¼•æ“
åŸºäºå¤šç»´åº¦ç‰¹å¾åˆ†æï¼Œå‘ç°å’Œæ¨èç›¸ä¼¼è‚¡ç¥¨
"""

import numpy as np
from dataclasses import dataclass
from typing import Dict, List, Any, Optional, Tuple, Set
from datetime import datetime, timedelta
from enum import Enum
import statistics
import math
from collections import defaultdict
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

from tradingagents.utils.logging_manager import get_logger

logger = get_logger('agents')


class SimilarityDimension(Enum):
    """ç›¸ä¼¼æ€§ç»´åº¦"""
    FUNDAMENTAL = "fundamental"         # åŸºæœ¬é¢ç›¸ä¼¼æ€§
    TECHNICAL = "technical"             # æŠ€æœ¯é¢ç›¸ä¼¼æ€§
    INDUSTRY = "industry"               # è¡Œä¸šç›¸ä¼¼æ€§
    SIZE = "size"                       # è§„æ¨¡ç›¸ä¼¼æ€§
    GROWTH = "growth"                   # æˆé•¿æ€§ç›¸ä¼¼æ€§
    VALUATION = "valuation"             # ä¼°å€¼ç›¸ä¼¼æ€§
    SENTIMENT = "sentiment"             # æƒ…ç»ªç›¸ä¼¼æ€§
    PATTERN = "pattern"                 # æ¨¡å¼ç›¸ä¼¼æ€§
    RISK = "risk"                       # é£é™©ç›¸ä¼¼æ€§


@dataclass
class SimilarityScore:
    """ç›¸ä¼¼æ€§è¯„åˆ†"""
    dimension: SimilarityDimension      # ç›¸ä¼¼æ€§ç»´åº¦
    score: float                        # ç›¸ä¼¼æ€§è¯„åˆ† (0-1)
    weight: float                       # ç»´åº¦æƒé‡
    details: Dict[str, Any]             # è¯¦ç»†ä¿¡æ¯
    confidence: float                   # ç½®ä¿¡åº¦


@dataclass
class StockSimilarity:
    """è‚¡ç¥¨ç›¸ä¼¼æ€§ç»“æœ"""
    target_symbol: str                  # ç›®æ ‡è‚¡ç¥¨ä»£ç 
    similar_symbol: str                 # ç›¸ä¼¼è‚¡ç¥¨ä»£ç 
    overall_similarity: float           # ç»¼åˆç›¸ä¼¼åº¦ (0-1)
    dimension_scores: Dict[SimilarityDimension, SimilarityScore]  # å„ç»´åº¦è¯„åˆ†
    match_reasons: List[str]            # åŒ¹é…åŸå› 
    recommendation_strength: float      # æ¨èå¼ºåº¦
    risk_differential: float            # é£é™©å·®å¼‚
    expected_correlation: float         # é¢„æœŸç›¸å…³æ€§
    timestamp: datetime                 # åˆ†ææ—¶é—´


class SimilarityEngine:
    """ç›¸ä¼¼è‚¡ç¥¨æ¨èå¼•æ“"""

    def __init__(self, config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–ç›¸ä¼¼æ€§å¼•æ“
        
        Args:
            config: é…ç½®å‚æ•°
        """
        self.config = config or {}
        
        # ç›¸ä¼¼æ€§æƒé‡é…ç½®
        self.dimension_weights = {
            SimilarityDimension.FUNDAMENTAL: 0.25,    # åŸºæœ¬é¢ 25%
            SimilarityDimension.TECHNICAL: 0.20,      # æŠ€æœ¯é¢ 20%
            SimilarityDimension.INDUSTRY: 0.15,       # è¡Œä¸š 15%
            SimilarityDimension.SIZE: 0.10,           # è§„æ¨¡ 10%
            SimilarityDimension.GROWTH: 0.12,         # æˆé•¿æ€§ 12%
            SimilarityDimension.VALUATION: 0.10,      # ä¼°å€¼ 10%
            SimilarityDimension.SENTIMENT: 0.05,      # æƒ…ç»ª 5%
            SimilarityDimension.PATTERN: 0.03         # æ¨¡å¼ 3%
        }
        
        # ç›¸ä¼¼æ€§é˜ˆå€¼
        self.similarity_thresholds = {
            'high_similarity': 0.8,       # é«˜ç›¸ä¼¼åº¦é˜ˆå€¼
            'medium_similarity': 0.6,     # ä¸­ç­‰ç›¸ä¼¼åº¦é˜ˆå€¼
            'low_similarity': 0.4,        # ä½ç›¸ä¼¼åº¦é˜ˆå€¼
            'min_similarity': 0.3         # æœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼
        }
        
        # ç‰¹å¾æå–å™¨
        self.feature_extractors = {
            SimilarityDimension.FUNDAMENTAL: self._extract_fundamental_features,
            SimilarityDimension.TECHNICAL: self._extract_technical_features,
            SimilarityDimension.INDUSTRY: self._extract_industry_features,
            SimilarityDimension.SIZE: self._extract_size_features,
            SimilarityDimension.GROWTH: self._extract_growth_features,
            SimilarityDimension.VALUATION: self._extract_valuation_features,
            SimilarityDimension.SENTIMENT: self._extract_sentiment_features,
            SimilarityDimension.PATTERN: self._extract_pattern_features
        }
        
        # è‚¡ç¥¨ç‰¹å¾ç¼“å­˜
        self.feature_cache = {}
        
        # ç›¸ä¼¼æ€§è®¡ç®—ç¼“å­˜
        self.similarity_cache = {}
        
        # èšç±»æ¨¡å‹
        self.cluster_models = {}
        
        logger.info("ç›¸ä¼¼è‚¡ç¥¨æ¨èå¼•æ“åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   ç›¸ä¼¼æ€§ç»´åº¦: {len(SimilarityDimension)} ä¸ª")
        logger.info(f"   ç»´åº¦æƒé‡é…ç½®: {len(self.dimension_weights)} é¡¹")

    def find_similar_stocks(self, target_symbol: str, 
                          target_data: Dict[str, Any],
                          candidate_stocks: List[Dict[str, Any]],
                          top_k: int = 10) -> List[StockSimilarity]:
        """
        æ‰¾åˆ°ç›¸ä¼¼è‚¡ç¥¨
        
        Args:
            target_symbol: ç›®æ ‡è‚¡ç¥¨ä»£ç 
            target_data: ç›®æ ‡è‚¡ç¥¨æ•°æ®
            candidate_stocks: å€™é€‰è‚¡ç¥¨åˆ—è¡¨
            top_k: è¿”å›å‰Kä¸ªç›¸ä¼¼è‚¡ç¥¨
            
        Returns:
            ç›¸ä¼¼è‚¡ç¥¨åˆ—è¡¨ï¼ŒæŒ‰ç›¸ä¼¼åº¦æ’åº
        """
        try:
            logger.info(f"ğŸ” [ç›¸ä¼¼æ€§å¼•æ“] ä¸º {target_symbol} å¯»æ‰¾ç›¸ä¼¼è‚¡ç¥¨ï¼Œå€™é€‰æ•°: {len(candidate_stocks)}")
            
            # 1. æå–ç›®æ ‡è‚¡ç¥¨ç‰¹å¾
            target_features = self._extract_all_features(target_symbol, target_data)
            
            # 2. è®¡ç®—ä¸æ‰€æœ‰å€™é€‰è‚¡ç¥¨çš„ç›¸ä¼¼åº¦
            similarities = []
            
            for candidate in candidate_stocks:
                candidate_symbol = candidate.get('symbol', '')
                if candidate_symbol == target_symbol:
                    continue  # è·³è¿‡è‡ªå·±
                
                try:
                    # æå–å€™é€‰è‚¡ç¥¨ç‰¹å¾
                    candidate_features = self._extract_all_features(candidate_symbol, candidate)
                    
                    # è®¡ç®—ç›¸ä¼¼åº¦
                    similarity = self._calculate_similarity(
                        target_symbol, target_features,
                        candidate_symbol, candidate_features
                    )
                    
                    if similarity.overall_similarity >= self.similarity_thresholds['min_similarity']:
                        similarities.append(similarity)
                        
                except Exception as e:
                    logger.debug(f"âš ï¸ è®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥: {candidate_symbol} - {e}")
                    continue
            
            # 3. æ’åºå¹¶è¿”å›å‰Kä¸ª
            similarities.sort(key=lambda x: x.overall_similarity, reverse=True)
            top_similarities = similarities[:top_k]
            
            logger.info(f"ğŸ” [ç›¸ä¼¼æ€§å¼•æ“] æ‰¾åˆ° {len(top_similarities)} åªç›¸ä¼¼è‚¡ç¥¨")
            
            # 4. æ›´æ–°ç¼“å­˜
            cache_key = f"{target_symbol}_{datetime.now().strftime('%Y-%m-%d')}"
            self.similarity_cache[cache_key] = top_similarities
            
            return top_similarities
            
        except Exception as e:
            logger.error(f"âŒ [ç›¸ä¼¼æ€§å¼•æ“] æŸ¥æ‰¾å¤±è´¥: {target_symbol} - {e}")
            return []

    def _extract_all_features(self, symbol: str, stock_data: Dict[str, Any]) -> Dict[SimilarityDimension, np.ndarray]:
        """æå–è‚¡ç¥¨æ‰€æœ‰ç»´åº¦ç‰¹å¾"""
        try:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = f"{symbol}_{hash(str(sorted(stock_data.items())))}"
            if cache_key in self.feature_cache:
                return self.feature_cache[cache_key]
            
            features = {}
            
            # æå–å„ç»´åº¦ç‰¹å¾
            for dimension, extractor in self.feature_extractors.items():
                try:
                    feature_vector = extractor(symbol, stock_data)
                    features[dimension] = feature_vector
                except Exception as e:
                    logger.debug(f"âš ï¸ ç‰¹å¾æå–å¤±è´¥ {dimension.value}: {e}")
                    # ä½¿ç”¨é»˜è®¤ç‰¹å¾å‘é‡
                    features[dimension] = np.zeros(self._get_feature_dimension(dimension))
            
            # æ›´æ–°ç¼“å­˜
            self.feature_cache[cache_key] = features
            
            return features
            
        except Exception as e:
            logger.error(f"âŒ ç‰¹å¾æå–å¤±è´¥: {symbol} - {e}")
            # è¿”å›é»˜è®¤ç‰¹å¾
            return {dim: np.zeros(self._get_feature_dimension(dim)) for dim in SimilarityDimension}

    def _get_feature_dimension(self, dimension: SimilarityDimension) -> int:
        """è·å–ç‰¹å¾ç»´åº¦å¤§å°"""
        dimension_sizes = {
            SimilarityDimension.FUNDAMENTAL: 8,    # 8ä¸ªåŸºæœ¬é¢æŒ‡æ ‡
            SimilarityDimension.TECHNICAL: 6,      # 6ä¸ªæŠ€æœ¯æŒ‡æ ‡
            SimilarityDimension.INDUSTRY: 5,       # 5ä¸ªè¡Œä¸šç‰¹å¾
            SimilarityDimension.SIZE: 3,           # 3ä¸ªè§„æ¨¡æŒ‡æ ‡
            SimilarityDimension.GROWTH: 4,         # 4ä¸ªæˆé•¿æŒ‡æ ‡
            SimilarityDimension.VALUATION: 4,      # 4ä¸ªä¼°å€¼æŒ‡æ ‡
            SimilarityDimension.SENTIMENT: 3,      # 3ä¸ªæƒ…ç»ªæŒ‡æ ‡
            SimilarityDimension.PATTERN: 5         # 5ä¸ªæ¨¡å¼æŒ‡æ ‡
        }
        return dimension_sizes.get(dimension, 5)

    def _extract_fundamental_features(self, symbol: str, data: Dict[str, Any]) -> np.ndarray:
        """æå–åŸºæœ¬é¢ç‰¹å¾"""
        try:
            features = []
            
            # ROE (å‡€èµ„äº§æ”¶ç›Šç‡)
            roe = data.get('roe', 0.1)
            features.append(self._normalize_ratio(roe, 0, 0.3))
            
            # ROA (æ€»èµ„äº§æ”¶ç›Šç‡)
            roa = data.get('roa', 0.05)
            features.append(self._normalize_ratio(roa, 0, 0.2))
            
            # æ¯›åˆ©ç‡
            gross_margin = data.get('gross_margin', 0.3)
            features.append(self._normalize_ratio(gross_margin, 0, 1))
            
            # å‡€åˆ©ç‡
            net_margin = data.get('net_margin', 0.1)
            features.append(self._normalize_ratio(net_margin, 0, 0.5))
            
            # è´Ÿå€ºç‡
            debt_ratio = data.get('debt_ratio', 0.4)
            features.append(self._normalize_ratio(debt_ratio, 0, 1))
            
            # æµåŠ¨æ¯”ç‡
            current_ratio = data.get('current_ratio', 1.5)
            features.append(self._normalize_ratio(current_ratio, 0.5, 3))
            
            # èµ„äº§å‘¨è½¬ç‡
            asset_turnover = data.get('asset_turnover', 1.0)
            features.append(self._normalize_ratio(asset_turnover, 0.2, 3))
            
            # è¥æ”¶å¢é•¿ç‡
            revenue_growth = data.get('revenue_growth', 0.1)
            features.append(self._normalize_ratio(revenue_growth, -0.5, 1))
            
            return np.array(features)
            
        except Exception as e:
            logger.debug(f"åŸºæœ¬é¢ç‰¹å¾æå–å¤±è´¥: {e}")
            return np.zeros(8)

    def _extract_technical_features(self, symbol: str, data: Dict[str, Any]) -> np.ndarray:
        """æå–æŠ€æœ¯é¢ç‰¹å¾"""
        try:
            features = []
            
            # ä»·æ ¼åŠ¨é‡ (è¿‘æœŸæ¶¨è·Œå¹…)
            price_momentum = data.get('price_change_pct', 0) / 100
            features.append(self._normalize_ratio(price_momentum, -0.2, 0.2))
            
            # ç›¸å¯¹å¼ºåº¦ (ç›¸å¯¹å¤§ç›˜è¡¨ç°)
            relative_strength = data.get('relative_strength', 0)
            features.append(self._normalize_ratio(relative_strength, -0.1, 0.1))
            
            # æ³¢åŠ¨ç‡
            volatility = data.get('volatility', 0.2)
            features.append(self._normalize_ratio(volatility, 0.1, 0.5))
            
            # æˆäº¤é‡æ¯”ç‡
            volume_ratio = data.get('volume_ratio', 1.0)
            features.append(self._normalize_ratio(volume_ratio, 0.5, 3))
            
            # æŠ€æœ¯è¯„åˆ†
            technical_score = data.get('technical_score', 50) / 100
            features.append(technical_score)
            
            # è¶‹åŠ¿å¼ºåº¦
            trend_strength = data.get('trend_strength', 0.5)
            features.append(trend_strength)
            
            return np.array(features)
            
        except Exception as e:
            logger.debug(f"æŠ€æœ¯é¢ç‰¹å¾æå–å¤±è´¥: {e}")
            return np.zeros(6)

    def _extract_industry_features(self, symbol: str, data: Dict[str, Any]) -> np.ndarray:
        """æå–è¡Œä¸šç‰¹å¾"""
        try:
            features = []
            
            # è¡Œä¸šç¼–ç  (ç®€åŒ–ä¸ºå‡ ä¸ªä¸»è¦è¡Œä¸š)
            industry = data.get('industry', 'å…¶ä»–')
            industry_encoding = self._encode_industry(industry)
            features.extend(industry_encoding)
            
            # è¡Œä¸šç›¸å¯¹è¡¨ç°
            industry_performance = data.get('industry_performance', 0)
            features.append(self._normalize_ratio(industry_performance, -0.2, 0.2))
            
            # è¡Œä¸šæ’å (åœ¨è¡Œä¸šå†…çš„æ’åç™¾åˆ†ä½)
            industry_ranking = data.get('industry_ranking', 0.5)
            features.append(industry_ranking)
            
            return np.array(features)
            
        except Exception as e:
            logger.debug(f"è¡Œä¸šç‰¹å¾æå–å¤±è´¥: {e}")
            return np.zeros(5)

    def _extract_size_features(self, symbol: str, data: Dict[str, Any]) -> np.ndarray:
        """æå–è§„æ¨¡ç‰¹å¾"""
        try:
            features = []
            
            # å¸‚å€¼ (å¯¹æ•°ç¼©æ”¾)
            market_cap = data.get('market_cap', 1000000000)  # é»˜è®¤10äº¿
            log_market_cap = math.log10(max(market_cap, 1000000))  # æœ€å°100ä¸‡
            features.append(self._normalize_ratio(log_market_cap, 6, 12))  # 100ä¸‡åˆ°1ä¸‡äº¿
            
            # æµé€šè‚¡æœ¬ (å¯¹æ•°ç¼©æ”¾)
            shares_outstanding = data.get('shares_outstanding', 100000000)  # é»˜è®¤1äº¿è‚¡
            log_shares = math.log10(max(shares_outstanding, 1000000))
            features.append(self._normalize_ratio(log_shares, 6, 11))
            
            # å¹´è¥æ”¶ (å¯¹æ•°ç¼©æ”¾)
            revenue = data.get('revenue', 1000000000)  # é»˜è®¤10äº¿
            log_revenue = math.log10(max(revenue, 1000000))
            features.append(self._normalize_ratio(log_revenue, 6, 12))
            
            return np.array(features)
            
        except Exception as e:
            logger.debug(f"è§„æ¨¡ç‰¹å¾æå–å¤±è´¥: {e}")
            return np.zeros(3)

    def _extract_growth_features(self, symbol: str, data: Dict[str, Any]) -> np.ndarray:
        """æå–æˆé•¿æ€§ç‰¹å¾"""
        try:
            features = []
            
            # è¥æ”¶å¢é•¿ç‡
            revenue_growth = data.get('revenue_growth', 0.1)
            features.append(self._normalize_ratio(revenue_growth, -0.3, 1))
            
            # å‡€åˆ©æ¶¦å¢é•¿ç‡
            profit_growth = data.get('profit_growth', 0.1)
            features.append(self._normalize_ratio(profit_growth, -0.5, 1.5))
            
            # EPSå¢é•¿ç‡
            eps_growth = data.get('eps_growth', 0.1)
            features.append(self._normalize_ratio(eps_growth, -0.5, 1.5))
            
            # æˆé•¿æ€§è¯„åˆ†
            growth_score = data.get('growth_score', 50) / 100
            features.append(growth_score)
            
            return np.array(features)
            
        except Exception as e:
            logger.debug(f"æˆé•¿æ€§ç‰¹å¾æå–å¤±è´¥: {e}")
            return np.zeros(4)

    def _extract_valuation_features(self, symbol: str, data: Dict[str, Any]) -> np.ndarray:
        """æå–ä¼°å€¼ç‰¹å¾"""
        try:
            features = []
            
            # P/Eæ¯”ç‡ (å¯¹æ•°ç¼©æ”¾)
            pe_ratio = data.get('pe_ratio', 20)
            log_pe = math.log10(max(pe_ratio, 1))
            features.append(self._normalize_ratio(log_pe, 0, 2))  # 1åˆ°100å€
            
            # P/Bæ¯”ç‡ (å¯¹æ•°ç¼©æ”¾)
            pb_ratio = data.get('pb_ratio', 2)
            log_pb = math.log10(max(pb_ratio, 0.1))
            features.append(self._normalize_ratio(log_pb, -1, 1))  # 0.1åˆ°10å€
            
            # P/Sæ¯”ç‡
            ps_ratio = data.get('ps_ratio', 3)
            log_ps = math.log10(max(ps_ratio, 0.1))
            features.append(self._normalize_ratio(log_ps, -1, 1))
            
            # ä¼°å€¼è¯„åˆ†
            valuation_score = data.get('valuation_score', 50) / 100
            features.append(valuation_score)
            
            return np.array(features)
            
        except Exception as e:
            logger.debug(f"ä¼°å€¼ç‰¹å¾æå–å¤±è´¥: {e}")
            return np.zeros(4)

    def _extract_sentiment_features(self, symbol: str, data: Dict[str, Any]) -> np.ndarray:
        """æå–æƒ…ç»ªç‰¹å¾"""
        try:
            features = []
            
            # æ–°é—»æƒ…ç»ª
            news_sentiment = data.get('news_sentiment', 0)
            features.append(self._normalize_ratio(news_sentiment, -1, 1))
            
            # ç¤¾äº¤åª’ä½“æƒ…ç»ª
            social_sentiment = data.get('social_sentiment', 0)
            features.append(self._normalize_ratio(social_sentiment, -1, 1))
            
            # æƒ…ç»ªè¯„åˆ†
            sentiment_score = data.get('sentiment_score', 50) / 100
            features.append(sentiment_score)
            
            return np.array(features)
            
        except Exception as e:
            logger.debug(f"æƒ…ç»ªç‰¹å¾æå–å¤±è´¥: {e}")
            return np.zeros(3)

    def _extract_pattern_features(self, symbol: str, data: Dict[str, Any]) -> np.ndarray:
        """æå–æ¨¡å¼ç‰¹å¾"""
        try:
            features = []
            
            # è¶‹åŠ¿æ¨¡å¼å¼ºåº¦
            trend_pattern = data.get('trend_pattern_strength', 0.5)
            features.append(trend_pattern)
            
            # åŠ¨é‡æ¨¡å¼å¼ºåº¦
            momentum_pattern = data.get('momentum_pattern_strength', 0.5)
            features.append(momentum_pattern)
            
            # åè½¬æ¨¡å¼å¼ºåº¦
            reversal_pattern = data.get('reversal_pattern_strength', 0.5)
            features.append(reversal_pattern)
            
            # çªç ´æ¨¡å¼å¼ºåº¦
            breakout_pattern = data.get('breakout_pattern_strength', 0.5)
            features.append(breakout_pattern)
            
            # æ•´ä½“æ¨¡å¼ç›¸ä¼¼æ€§
            pattern_similarity = data.get('pattern_similarity', 0.5)
            features.append(pattern_similarity)
            
            return np.array(features)
            
        except Exception as e:
            logger.debug(f"æ¨¡å¼ç‰¹å¾æå–å¤±è´¥: {e}")
            return np.zeros(5)

    def _normalize_ratio(self, value: float, min_val: float, max_val: float) -> float:
        """å½’ä¸€åŒ–æ¯”ç‡åˆ° [0, 1] åŒºé—´"""
        try:
            if max_val <= min_val:
                return 0.5
            normalized = (value - min_val) / (max_val - min_val)
            return max(0, min(1, normalized))
        except:
            return 0.5

    def _encode_industry(self, industry: str) -> List[float]:
        """ç¼–ç è¡Œä¸šä¿¡æ¯ (One-Hotç¼–ç ç®€åŒ–ç‰ˆ)"""
        try:
            # ç®€åŒ–çš„ä¸»è¦è¡Œä¸šåˆ†ç±»
            major_industries = ['é‡‘è', 'ç§‘æŠ€', 'æ¶ˆè´¹', 'åŒ»è¯', 'åˆ¶é€ ']
            
            # One-hotç¼–ç  (ä½†é•¿åº¦å›ºå®šä¸º3ï¼Œå–å‰3ä¸ªåŒ¹é…)
            encoding = [0.0, 0.0, 0.0]
            
            for i, major in enumerate(major_industries[:3]):
                if major in industry:
                    encoding[i] = 1.0
                    break
            
            # å¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œä½¿ç”¨é»˜è®¤ç¼–ç 
            if sum(encoding) == 0:
                encoding[2] = 1.0  # é»˜è®¤ä¸ºç¬¬3ç±»
            
            return encoding
            
        except:
            return [0.0, 0.0, 1.0]  # é»˜è®¤ç¼–ç 

    def _calculate_similarity(self, target_symbol: str, target_features: Dict[SimilarityDimension, np.ndarray],
                            candidate_symbol: str, candidate_features: Dict[SimilarityDimension, np.ndarray]) -> StockSimilarity:
        """è®¡ç®—ä¸¤åªè‚¡ç¥¨çš„ç›¸ä¼¼åº¦"""
        try:
            dimension_scores = {}
            total_weighted_score = 0.0
            total_weight = 0.0
            
            # è®¡ç®—å„ç»´åº¦ç›¸ä¼¼åº¦
            for dimension in SimilarityDimension:
                if dimension in target_features and dimension in candidate_features:
                    target_vector = target_features[dimension]
                    candidate_vector = candidate_features[dimension]
                    
                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    similarity_score = self._calculate_cosine_similarity(target_vector, candidate_vector)
                    
                    # ç»´åº¦æƒé‡
                    weight = self.dimension_weights.get(dimension, 0.1)
                    
                    # ç½®ä¿¡åº¦åŸºäºç‰¹å¾å‘é‡çš„æœ‰æ•ˆæ€§
                    confidence = self._calculate_feature_confidence(target_vector, candidate_vector)
                    
                    dimension_scores[dimension] = SimilarityScore(
                        dimension=dimension,
                        score=similarity_score,
                        weight=weight,
                        details={
                            'target_features': target_vector.tolist(),
                            'candidate_features': candidate_vector.tolist()
                        },
                        confidence=confidence
                    )
                    
                    # ç´¯åŠ åŠ æƒå¾—åˆ†
                    effective_weight = weight * confidence
                    total_weighted_score += similarity_score * effective_weight
                    total_weight += effective_weight
            
            # è®¡ç®—ç»¼åˆç›¸ä¼¼åº¦
            overall_similarity = total_weighted_score / total_weight if total_weight > 0 else 0.0
            
            # ç”ŸæˆåŒ¹é…åŸå› 
            match_reasons = self._generate_match_reasons(dimension_scores)
            
            # è®¡ç®—æ¨èå¼ºåº¦
            recommendation_strength = self._calculate_recommendation_strength(overall_similarity, dimension_scores)
            
            # è®¡ç®—é£é™©å·®å¼‚ (ç®€åŒ–)
            risk_differential = self._calculate_risk_differential(target_features, candidate_features)
            
            # é¢„æœŸç›¸å…³æ€§
            expected_correlation = overall_similarity * 0.8  # ç®€åŒ–ä¼°è®¡
            
            return StockSimilarity(
                target_symbol=target_symbol,
                similar_symbol=candidate_symbol,
                overall_similarity=overall_similarity,
                dimension_scores=dimension_scores,
                match_reasons=match_reasons,
                recommendation_strength=recommendation_strength,
                risk_differential=risk_differential,
                expected_correlation=expected_correlation,
                timestamp=datetime.now()
            )
            
        except Exception as e:
            logger.error(f"âŒ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {target_symbol} vs {candidate_symbol} - {e}")
            return StockSimilarity(
                target_symbol=target_symbol,
                similar_symbol=candidate_symbol,
                overall_similarity=0.0,
                dimension_scores={},
                match_reasons=["è®¡ç®—å¤±è´¥"],
                recommendation_strength=0.0,
                risk_differential=0.0,
                expected_correlation=0.0,
                timestamp=datetime.now()
            )

    def _calculate_cosine_similarity(self, vector1: np.ndarray, vector2: np.ndarray) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        try:
            # å¤„ç†é›¶å‘é‡
            norm1 = np.linalg.norm(vector1)
            norm2 = np.linalg.norm(vector2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            cosine_sim = np.dot(vector1, vector2) / (norm1 * norm2)
            
            # è½¬æ¢åˆ° [0, 1] åŒºé—´
            return (cosine_sim + 1) / 2
            
        except Exception as e:
            logger.debug(f"ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0

    def _calculate_feature_confidence(self, vector1: np.ndarray, vector2: np.ndarray) -> float:
        """è®¡ç®—ç‰¹å¾ç½®ä¿¡åº¦"""
        try:
            # åŸºäºç‰¹å¾å‘é‡çš„å®Œæ•´æ€§å’Œæœ‰æ•ˆæ€§
            non_zero_count1 = np.count_nonzero(vector1)
            non_zero_count2 = np.count_nonzero(vector2)
            
            completeness = (non_zero_count1 + non_zero_count2) / (len(vector1) + len(vector2))
            
            # åŸºäºç‰¹å¾å€¼çš„æ–¹å·®
            variance1 = np.var(vector1)
            variance2 = np.var(vector2)
            avg_variance = (variance1 + variance2) / 2
            
            # ç»¼åˆç½®ä¿¡åº¦
            confidence = min(1.0, completeness + min(0.3, avg_variance))
            
            return max(0.1, confidence)  # æœ€ä½0.1çš„ç½®ä¿¡åº¦
            
        except Exception:
            return 0.5

    def _generate_match_reasons(self, dimension_scores: Dict[SimilarityDimension, SimilarityScore]) -> List[str]:
        """ç”ŸæˆåŒ¹é…åŸå› """
        try:
            reasons = []
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            sorted_dimensions = sorted(
                dimension_scores.items(),
                key=lambda x: x[1].score * x[1].weight,
                reverse=True
            )
            
            # æå–å‰å‡ ä¸ªæœ€ç›¸ä¼¼çš„ç»´åº¦
            for dimension, score in sorted_dimensions[:3]:
                if score.score > 0.7:
                    reason_map = {
                        SimilarityDimension.FUNDAMENTAL: f"åŸºæœ¬é¢é«˜åº¦ç›¸ä¼¼ ({score.score:.2%})",
                        SimilarityDimension.TECHNICAL: f"æŠ€æœ¯æŒ‡æ ‡ç›¸ä¼¼ ({score.score:.2%})",
                        SimilarityDimension.INDUSTRY: f"åŒè¡Œä¸šå…¬å¸ ({score.score:.2%})",
                        SimilarityDimension.SIZE: f"è§„æ¨¡ç›¸å½“ ({score.score:.2%})",
                        SimilarityDimension.GROWTH: f"æˆé•¿æ€§ç›¸ä¼¼ ({score.score:.2%})",
                        SimilarityDimension.VALUATION: f"ä¼°å€¼æ°´å¹³æ¥è¿‘ ({score.score:.2%})"
                    }
                    
                    reason = reason_map.get(dimension, f"{dimension.value}ç›¸ä¼¼ ({score.score:.2%})")
                    reasons.append(reason)
            
            return reasons if reasons else ["æ•´ä½“ç‰¹å¾ç›¸ä¼¼"]
            
        except Exception as e:
            logger.debug(f"åŒ¹é…åŸå› ç”Ÿæˆå¤±è´¥: {e}")
            return ["ç›¸ä¼¼æ€§åˆ†æ"]

    def _calculate_recommendation_strength(self, overall_similarity: float, 
                                        dimension_scores: Dict[SimilarityDimension, SimilarityScore]) -> float:
        """è®¡ç®—æ¨èå¼ºåº¦"""
        try:
            base_strength = overall_similarity
            
            # åŸºäºå…³é”®ç»´åº¦çš„åŠ æƒ
            key_dimensions = [SimilarityDimension.FUNDAMENTAL, SimilarityDimension.TECHNICAL, SimilarityDimension.GROWTH]
            
            key_dimension_boost = 0
            for dimension in key_dimensions:
                if dimension in dimension_scores:
                    score = dimension_scores[dimension]
                    if score.score > 0.8:
                        key_dimension_boost += 0.1 * score.confidence
            
            # ç»¼åˆæ¨èå¼ºåº¦
            recommendation_strength = min(1.0, base_strength + key_dimension_boost)
            
            return recommendation_strength
            
        except Exception:
            return overall_similarity

    def _calculate_risk_differential(self, target_features: Dict[SimilarityDimension, np.ndarray],
                                   candidate_features: Dict[SimilarityDimension, np.ndarray]) -> float:
        """è®¡ç®—é£é™©å·®å¼‚"""
        try:
            # ç®€åŒ–çš„é£é™©å·®å¼‚è®¡ç®—
            # åŸºäºåŸºæœ¬é¢å’ŒæŠ€æœ¯é¢ç‰¹å¾çš„å·®å¼‚
            risk_diff = 0.0
            
            if (SimilarityDimension.FUNDAMENTAL in target_features and 
                SimilarityDimension.FUNDAMENTAL in candidate_features):
                
                target_risk = np.mean(target_features[SimilarityDimension.FUNDAMENTAL][[4, 6]])  # è´Ÿå€ºç‡, èµ„äº§å‘¨è½¬ç‡
                candidate_risk = np.mean(candidate_features[SimilarityDimension.FUNDAMENTAL][[4, 6]])
                
                risk_diff += abs(target_risk - candidate_risk)
            
            if (SimilarityDimension.TECHNICAL in target_features and 
                SimilarityDimension.TECHNICAL in candidate_features):
                
                target_volatility = target_features[SimilarityDimension.TECHNICAL][2]  # æ³¢åŠ¨ç‡
                candidate_volatility = candidate_features[SimilarityDimension.TECHNICAL][2]
                
                risk_diff += abs(target_volatility - candidate_volatility)
            
            return min(1.0, risk_diff)
            
        except Exception:
            return 0.5

    def build_similarity_clusters(self, stock_data_list: List[Dict[str, Any]], 
                                n_clusters: int = 10) -> Dict[str, Any]:
        """æ„å»ºç›¸ä¼¼æ€§èšç±»"""
        try:
            logger.info(f"ğŸ”— [ç›¸ä¼¼æ€§å¼•æ“] æ„å»ºç›¸ä¼¼æ€§èšç±»ï¼Œè‚¡ç¥¨æ•°: {len(stock_data_list)}")
            
            # æå–æ‰€æœ‰è‚¡ç¥¨ç‰¹å¾
            stock_features = []
            stock_symbols = []
            
            for stock_data in stock_data_list:
                symbol = stock_data.get('symbol', '')
                if not symbol:
                    continue
                
                features = self._extract_all_features(symbol, stock_data)
                
                # å°†æ‰€æœ‰ç»´åº¦ç‰¹å¾è¿æ¥æˆä¸€ä¸ªå‘é‡
                combined_features = np.concatenate([
                    features.get(dim, np.zeros(self._get_feature_dimension(dim)))
                    for dim in SimilarityDimension
                ])
                
                stock_features.append(combined_features)
                stock_symbols.append(symbol)
            
            if len(stock_features) < n_clusters:
                n_clusters = max(2, len(stock_features) // 2)
            
            # æ ‡å‡†åŒ–ç‰¹å¾
            scaler = StandardScaler()
            normalized_features = scaler.fit_transform(stock_features)
            
            # K-meansèšç±»
            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
            cluster_labels = kmeans.fit_predict(normalized_features)
            
            # ç»„ç»‡èšç±»ç»“æœ
            clusters = defaultdict(list)
            for symbol, label in zip(stock_symbols, cluster_labels):
                clusters[f"cluster_{label}"].append(symbol)
            
            # ä¿å­˜èšç±»æ¨¡å‹
            self.cluster_models['stock_similarity'] = {
                'kmeans': kmeans,
                'scaler': scaler,
                'feature_dimension': len(combined_features)
            }
            
            cluster_summary = {
                'total_stocks': len(stock_symbols),
                'n_clusters': n_clusters,
                'clusters': dict(clusters),
                'cluster_sizes': {k: len(v) for k, v in clusters.items()},
                'timestamp': datetime.now()
            }
            
            logger.info(f"ğŸ”— [ç›¸ä¼¼æ€§å¼•æ“] èšç±»å®Œæˆï¼Œç”Ÿæˆ {n_clusters} ä¸ªé›†ç¾¤")
            return cluster_summary
            
        except Exception as e:
            logger.error(f"âŒ èšç±»æ„å»ºå¤±è´¥: {e}")
            return {'error': str(e)}

    def recommend_from_cluster(self, target_symbol: str, 
                             cluster_info: Dict[str, Any],
                             top_k: int = 5) -> List[str]:
        """åŸºäºèšç±»æ¨èç›¸ä¼¼è‚¡ç¥¨"""
        try:
            # æ‰¾åˆ°ç›®æ ‡è‚¡ç¥¨æ‰€åœ¨é›†ç¾¤
            target_cluster = None
            for cluster_name, symbols in cluster_info.get('clusters', {}).items():
                if target_symbol in symbols:
                    target_cluster = cluster_name
                    break
            
            if not target_cluster:
                return []
            
            # ä»åŒä¸€é›†ç¾¤ä¸­æ¨èå…¶ä»–è‚¡ç¥¨
            cluster_stocks = cluster_info['clusters'][target_cluster]
            similar_stocks = [s for s in cluster_stocks if s != target_symbol]
            
            return similar_stocks[:top_k]
            
        except Exception as e:
            logger.error(f"âŒ èšç±»æ¨èå¤±è´¥: {e}")
            return []

    def get_similarity_insights(self, similarities: List[StockSimilarity]) -> Dict[str, Any]:
        """è·å–ç›¸ä¼¼æ€§åˆ†ææ´å¯Ÿ"""
        try:
            if not similarities:
                return {'insights': 'æ— ç›¸ä¼¼è‚¡ç¥¨æ•°æ®'}
            
            # ç»Ÿè®¡åˆ†æ
            similarity_scores = [s.overall_similarity for s in similarities]
            
            # ç»´åº¦åˆ†æ
            dimension_stats = defaultdict(list)
            for similarity in similarities:
                for dim, score in similarity.dimension_scores.items():
                    dimension_stats[dim.value].append(score.score)
            
            # ç”Ÿæˆæ´å¯Ÿ
            insights = {
                'total_similar_stocks': len(similarities),
                'avg_similarity': statistics.mean(similarity_scores),
                'max_similarity': max(similarity_scores),
                'min_similarity': min(similarity_scores),
                'high_similarity_count': len([s for s in similarity_scores if s > 0.8]),
                'medium_similarity_count': len([s for s in similarity_scores if 0.6 <= s <= 0.8]),
                'dimension_analysis': {},
                'top_match_reasons': [],
                'similarity_distribution': {
                    'very_high': len([s for s in similarity_scores if s > 0.9]),
                    'high': len([s for s in similarity_scores if 0.8 <= s <= 0.9]),
                    'medium': len([s for s in similarity_scores if 0.6 <= s < 0.8]),
                    'low': len([s for s in similarity_scores if s < 0.6])
                }
            }
            
            # ç»´åº¦åˆ†æ
            for dim, scores in dimension_stats.items():
                if scores:
                    insights['dimension_analysis'][dim] = {
                        'avg_score': statistics.mean(scores),
                        'max_score': max(scores),
                        'contribution': statistics.mean(scores) * self.dimension_weights.get(
                            getattr(SimilarityDimension, dim.upper(), SimilarityDimension.FUNDAMENTAL), 0.1
                        )
                    }
            
            # æ”¶é›†æœ€å¸¸è§çš„åŒ¹é…åŸå› 
            all_reasons = []
            for similarity in similarities[:10]:  # å‰10ä¸ªæœ€ç›¸ä¼¼çš„
                all_reasons.extend(similarity.match_reasons)
            
            reason_counts = defaultdict(int)
            for reason in all_reasons:
                reason_counts[reason] += 1
            
            insights['top_match_reasons'] = sorted(
                reason_counts.items(),
                key=lambda x: x[1],
                reverse=True
            )[:5]
            
            return insights
            
        except Exception as e:
            logger.error(f"âŒ ç›¸ä¼¼æ€§æ´å¯Ÿåˆ†æå¤±è´¥: {e}")
            return {'error': str(e)}