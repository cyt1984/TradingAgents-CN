#!/usr/bin/env python3
"""
AIç­–ç•¥ç®¡ç†å™¨
ç»Ÿä¸€ç®¡ç†å’Œåè°ƒæ‰€æœ‰AIé€‰è‚¡å¼•æ“ï¼Œæä¾›æ™ºèƒ½é€‰è‚¡å†³ç­–
"""

import numpy as np
from dataclasses import dataclass
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
from enum import Enum
import asyncio
import statistics
import json
from concurrent.futures import ThreadPoolExecutor, as_completed

from tradingagents.utils.logging_manager import get_logger
from .expert_committee import AIExpertCommittee
from .adaptive_engine import AdaptiveEngine, MarketRegime, StrategyType
from .pattern_recognizer import PatternRecognizer, PatternType
from .similarity_engine import SimilarityEngine, SimilarityDimension

logger = get_logger('agents')


class AIMode(Enum):
    """AIé€‰è‚¡æ¨¡å¼"""
    BASIC = "basic"                     # åŸºç¡€æ¨¡å¼
    AI_ENHANCED = "ai_enhanced"         # AIå¢å¼ºæ¨¡å¼
    EXPERT_COMMITTEE = "expert_committee"  # ä¸“å®¶å§”å‘˜ä¼šæ¨¡å¼
    ADAPTIVE = "adaptive"               # è‡ªé€‚åº”æ¨¡å¼
    PATTERN_BASED = "pattern_based"     # æ¨¡å¼é©±åŠ¨æ¨¡å¼
    SIMILARITY_BASED = "similarity_based"  # ç›¸ä¼¼æ€§é©±åŠ¨æ¨¡å¼
    FULL_AI = "full_ai"                # å®Œæ•´AIæ¨¡å¼


@dataclass
class AIAnalysisResult:
    """AIåˆ†æç»“æœ"""
    symbol: str                         # è‚¡ç¥¨ä»£ç 
    overall_score: float                # ç»¼åˆAIè¯„åˆ† (0-100)
    confidence_level: float             # ç½®ä¿¡åº¦ (0-1)
    recommendation: str                 # AIå»ºè®®
    risk_assessment: str                # é£é™©è¯„ä¼°
    
    # å„å¼•æ“è¯„åˆ†
    expert_committee_score: Optional[float] = None
    adaptive_strategy_score: Optional[float] = None  
    pattern_recognition_score: Optional[float] = None
    similarity_score: Optional[float] = None
    
    # è¯¦ç»†åˆ†æ
    expert_analysis: Optional[Dict[str, Any]] = None
    market_regime: Optional[str] = None
    detected_patterns: Optional[List[str]] = None
    similar_stocks: Optional[List[str]] = None
    
    # å†³ç­–å› ç´ 
    key_factors: List[str] = None
    risk_factors: List[str] = None
    opportunity_factors: List[str] = None
    
    # å…ƒæ•°æ®
    processing_time: float = 0.0
    timestamp: datetime = None


@dataclass
class AISelectionConfig:
    """AIé€‰è‚¡é…ç½®"""
    ai_mode: AIMode = AIMode.AI_ENHANCED
    
    # å¼•æ“æƒé‡
    expert_committee_weight: float = 0.4
    adaptive_strategy_weight: float = 0.3
    pattern_recognition_weight: float = 0.2
    similarity_weight: float = 0.1
    
    # é˜ˆå€¼é…ç½®
    min_ai_score: float = 70.0
    min_confidence: float = 0.6
    max_risk_level: str = "ä¸­ç­‰"
    
    # æ€§èƒ½é…ç½®
    enable_caching: bool = True
    parallel_processing: bool = True
    timeout_seconds: float = 30.0
    
    # å¸‚åœºæ•°æ®é…ç½®
    market_data_required: bool = True
    news_analysis_enabled: bool = True
    pattern_analysis_enabled: bool = True
    similarity_analysis_enabled: bool = True


class AIStrategyManager:
    """AIç­–ç•¥ç®¡ç†å™¨"""
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–AIç­–ç•¥ç®¡ç†å™¨
        
        Args:
            config: é…ç½®å‚æ•°
        """
        self.config = config or {}
        
        # AIå¼•æ“å®ä¾‹
        self.expert_committee: Optional[AIExpertCommittee] = None
        self.adaptive_engine: Optional[AdaptiveEngine] = None
        self.pattern_recognizer: Optional[PatternRecognizer] = None
        self.similarity_engine: Optional[SimilarityEngine] = None
        
        # åˆ†æç¼“å­˜
        self.analysis_cache = {}
        self.cache_ttl = 300  # 5åˆ†é’Ÿç¼“å­˜
        
        # æ€§èƒ½ç»Ÿè®¡
        self.performance_stats = {
            'total_analyses': 0,
            'cache_hits': 0,
            'average_processing_time': 0.0,
            'engine_performance': {}
        }
        
        # çº¿ç¨‹æ± 
        self.thread_pool = ThreadPoolExecutor(max_workers=4)
        
        logger.info("AIç­–ç•¥ç®¡ç†å™¨åˆå§‹åŒ–å¼€å§‹")
        self._initialize_ai_engines()
        logger.info("AIç­–ç•¥ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        
    def _initialize_ai_engines(self):
        """åˆå§‹åŒ–AIå¼•æ“"""
        engines_status = {
            'expert_committee': False,
            'adaptive_engine': False,
            'pattern_recognizer': False,
            'similarity_engine': False
        }
        
        # 1. åˆå§‹åŒ–ä¸“å®¶å§”å‘˜ä¼š
        try:
            logger.info("æ­£åœ¨åˆå§‹åŒ–AIä¸“å®¶å§”å‘˜ä¼š...")
            self.expert_committee = AIExpertCommittee(self.config)
            engines_status['expert_committee'] = True
            logger.info("âœ… AIä¸“å®¶å§”å‘˜ä¼šåˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ AIä¸“å®¶å§”å‘˜ä¼šåˆå§‹åŒ–å¤±è´¥: {e}")
            logger.debug(f"ä¸“å®¶å§”å‘˜ä¼šé”™è¯¯è¯¦æƒ…: {e}", exc_info=True)
            self.expert_committee = None
            
        # 2. åˆå§‹åŒ–è‡ªé€‚åº”å¼•æ“
        try:
            logger.info("æ­£åœ¨åˆå§‹åŒ–è‡ªé€‚åº”ç­–ç•¥å¼•æ“...")
            self.adaptive_engine = AdaptiveEngine(self.config)
            engines_status['adaptive_engine'] = True
            logger.info("âœ… è‡ªé€‚åº”ç­–ç•¥å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ è‡ªé€‚åº”ç­–ç•¥å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.debug(f"è‡ªé€‚åº”å¼•æ“é”™è¯¯è¯¦æƒ…: {e}", exc_info=True)
            self.adaptive_engine = None
            
        # 3. åˆå§‹åŒ–æ¨¡å¼è¯†åˆ«å™¨
        try:
            logger.info("æ­£åœ¨åˆå§‹åŒ–æ¨¡å¼è¯†åˆ«å¼•æ“...")
            self.pattern_recognizer = PatternRecognizer(self.config)
            engines_status['pattern_recognizer'] = True
            logger.info("âœ… æ¨¡å¼è¯†åˆ«å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ æ¨¡å¼è¯†åˆ«å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.debug(f"æ¨¡å¼è¯†åˆ«é”™è¯¯è¯¦æƒ…: {e}", exc_info=True)
            self.pattern_recognizer = None
            
        # 4. åˆå§‹åŒ–ç›¸ä¼¼æ€§å¼•æ“
        try:
            logger.info("æ­£åœ¨åˆå§‹åŒ–ç›¸ä¼¼è‚¡ç¥¨æ¨èå¼•æ“...")
            self.similarity_engine = SimilarityEngine(self.config)
            engines_status['similarity_engine'] = True
            logger.info("âœ… ç›¸ä¼¼è‚¡ç¥¨æ¨èå¼•æ“åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ ç›¸ä¼¼è‚¡ç¥¨æ¨èå¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.debug(f"ç›¸ä¼¼æ€§å¼•æ“é”™è¯¯è¯¦æƒ…: {e}", exc_info=True)
            self.similarity_engine = None
            
        # è®°å½•å¼•æ“çŠ¶æ€
        successful_engines = [name for name, status in engines_status.items() if status]
        failed_engines = [name for name, status in engines_status.items() if not status]
        
        logger.info(f"ğŸ¤– AIå¼•æ“åˆå§‹åŒ–å®Œæˆ:")
        logger.info(f"   âœ… æˆåŠŸ: {successful_engines}")
        if failed_engines:
            logger.warning(f"   âŒ å¤±è´¥: {failed_engines}")
        
        # å­˜å‚¨çŠ¶æ€ç”¨äºç›‘æ§
        self.performance_stats['engine_initialization'] = engines_status
        
        # å¦‚æœæ‰€æœ‰å¼•æ“éƒ½å¤±è´¥ï¼Œç»™å‡ºè­¦å‘Š
        if not any(engines_status.values()):
            logger.error("âŒ æ‰€æœ‰AIå¼•æ“åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨åŸºç¡€æ¨¡å¼")
        elif len(successful_engines) < len(engines_status) / 2:
            logger.warning("âš ï¸ éƒ¨åˆ†AIå¼•æ“åˆå§‹åŒ–å¤±è´¥ï¼ŒåŠŸèƒ½å¯èƒ½å—é™")
            
    def analyze_stock_with_ai(self, symbol: str, 
                            stock_data: Dict[str, Any],
                            market_data: Dict[str, Any] = None,
                            config: AISelectionConfig = None) -> AIAnalysisResult:
        """
        ä½¿ç”¨AIåˆ†æè‚¡ç¥¨
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            stock_data: è‚¡ç¥¨æ•°æ®
            market_data: å¸‚åœºæ•°æ®
            config: AIé…ç½®
            
        Returns:
            AIåˆ†æç»“æœ
        """
        start_time = datetime.now()
        config = config or AISelectionConfig()
        
        try:
            logger.info(f"ğŸ¤– [AIç­–ç•¥ç®¡ç†å™¨] å¼€å§‹AIåˆ†æ: {symbol} (æ¨¡å¼: {config.ai_mode.value})")
            
            # æ˜¾ç¤ºAIå†³ç­–è¿‡ç¨‹å¼€å§‹
            logger.info(f"ğŸ” [AIå†³ç­–è¿‡ç¨‹] è‚¡ç¥¨ {symbol} - å¼€å§‹AIæ™ºèƒ½åˆ†æ")
            logger.info(f"ğŸ“Š [AIå†³ç­–è¿‡ç¨‹] åˆ†ææ¨¡å¼: {config.ai_mode.value}")
            logger.info(f"ğŸ“‹ [AIå†³ç­–è¿‡ç¨‹] å¯ç”¨AIå¼•æ“: {[name for name, available in self.get_performance_summary()['ai_engines_status'].items() if available]}")
            
            # æ£€æŸ¥ç¼“å­˜
            if config.enable_caching:
                cached_result = self._get_cached_analysis(symbol, stock_data)
                if cached_result:
                    self.performance_stats['cache_hits'] += 1
                    logger.info(f"ğŸ¤– [AIç­–ç•¥ç®¡ç†å™¨] ä½¿ç”¨ç¼“å­˜ç»“æœ: {symbol}")
                    logger.info(f"âš¡ [AIå†³ç­–è¿‡ç¨‹] è‚¡ç¥¨ {symbol} - ä½¿ç”¨ç¼“å­˜ç»“æœï¼Œè·³è¿‡é‡å¤åˆ†æ")
                    return cached_result
            
            # æ ¹æ®æ¨¡å¼é€‰æ‹©åˆ†æç­–ç•¥
            if config.ai_mode == AIMode.BASIC:
                logger.info(f"ğŸ“ [AIå†³ç­–è¿‡ç¨‹] è‚¡ç¥¨ {symbol} - æ‰§è¡ŒåŸºç¡€åˆ†ææ¨¡å¼")
                result = self._basic_analysis(symbol, stock_data)
            elif config.ai_mode == AIMode.EXPERT_COMMITTEE:
                logger.info(f"ğŸ‘¥ [AIå†³ç­–è¿‡ç¨‹] è‚¡ç¥¨ {symbol} - å¯åŠ¨ä¸“å®¶å§”å‘˜ä¼šåˆ†æ")
                result = self._expert_committee_analysis(symbol, stock_data, market_data)
            elif config.ai_mode == AIMode.ADAPTIVE:
                logger.info(f"ğŸ”„ [AIå†³ç­–è¿‡ç¨‹] è‚¡ç¥¨ {symbol} - å¯åŠ¨è‡ªé€‚åº”ç­–ç•¥åˆ†æ")
                result = self._adaptive_analysis(symbol, stock_data, market_data)
            elif config.ai_mode == AIMode.PATTERN_BASED:
                logger.info(f"ğŸ“ˆ [AIå†³ç­–è¿‡ç¨‹] è‚¡ç¥¨ {symbol} - å¯åŠ¨æ¨¡å¼è¯†åˆ«åˆ†æ")
                result = self._pattern_based_analysis(symbol, stock_data)
            elif config.ai_mode == AIMode.SIMILARITY_BASED:
                logger.info(f"ğŸ” [AIå†³ç­–è¿‡ç¨‹] è‚¡ç¥¨ {symbol} - å¯åŠ¨ç›¸ä¼¼æ€§åˆ†æ")
                result = self._similarity_based_analysis(symbol, stock_data)
            elif config.ai_mode == AIMode.FULL_AI:
                logger.info(f"ğŸ† [AIå†³ç­–è¿‡ç¨‹] è‚¡ç¥¨ {symbol} - å¯åŠ¨å®Œæ•´AIåˆ†æï¼ˆå…¨å¼•æ“ï¼‰")
                result = self._full_ai_analysis(symbol, stock_data, market_data, config)
            else:  # AI_ENHANCED
                logger.info(f"âš¡ [AIå†³ç­–è¿‡ç¨‹] è‚¡ç¥¨ {symbol} - å¯åŠ¨AIå¢å¼ºåˆ†æ")
                result = self._ai_enhanced_analysis(symbol, stock_data, market_data, config)
            
            # è®°å½•å¤„ç†æ—¶é—´
            processing_time = (datetime.now() - start_time).total_seconds()
            result.processing_time = processing_time
            result.timestamp = datetime.now()
            
            # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
            self._update_performance_stats(processing_time)
            
            # ç¼“å­˜ç»“æœ
            if config.enable_caching:
                self._cache_analysis(symbol, stock_data, result)
            
            # æ˜¾ç¤ºAIå†³ç­–è¿‡ç¨‹ç»“æœ
            logger.info(f"âœ… [AIå†³ç­–è¿‡ç¨‹] è‚¡ç¥¨ {symbol} - AIåˆ†æå®Œæˆ")
            logger.info(f"ğŸ¯ [AIå†³ç­–ç»“æœ] ç»¼åˆè¯„åˆ†: {result.overall_score:.1f}")
            logger.info(f"ğŸ’¡ [AIå†³ç­–ç»“æœ] æŠ•èµ„å»ºè®®: {result.recommendation}")
            logger.info(f"ğŸ”’ [AIå†³ç­–ç»“æœ] é£é™©è¯„ä¼°: {result.risk_assessment}")
            logger.info(f"ğŸ“Š [AIå†³ç­–ç»“æœ] ç½®ä¿¡åº¦: {result.confidence_level:.2f}")
            logger.info(f"â±ï¸ [AIå†³ç­–ç»“æœ] åˆ†æè€—æ—¶: {processing_time:.2f}ç§’")
            
            # æ˜¾ç¤ºAIå¼•æ“è´¡çŒ®
            engine_contributions = []
            if result.expert_committee_score is not None:
                engine_contributions.append(f"ä¸“å®¶å§”å‘˜ä¼š: {result.expert_committee_score:.1f}")
            if result.adaptive_strategy_score is not None:
                engine_contributions.append(f"è‡ªé€‚åº”ç­–ç•¥: {result.adaptive_strategy_score:.1f}")
            if result.pattern_recognition_score is not None:
                engine_contributions.append(f"æ¨¡å¼è¯†åˆ«: {result.pattern_recognition_score:.1f}")
            
            if engine_contributions:
                logger.info(f"ğŸ¤– [AIå¼•æ“è´¡çŒ®] {' | '.join(engine_contributions)}")
            
            # æ˜¾ç¤ºå…³é”®å†³ç­–å› ç´ 
            if result.key_factors:
                logger.info(f"ğŸ”‘ [AIå†³ç­–å› ç´ ] å…³é”®å› ç´ : {' | '.join(result.key_factors[:3])}")
            
            logger.info(f"ğŸ¤– [AIç­–ç•¥ç®¡ç†å™¨] AIåˆ†æå®Œæˆ: {symbol} - è¯„åˆ†: {result.overall_score:.1f} (è€—æ—¶: {processing_time:.2f}s)")
            return result
            
        except Exception as e:
            logger.error(f"âŒ [AIç­–ç•¥ç®¡ç†å™¨] AIåˆ†æå¤±è´¥: {symbol} - {e}")
            logger.error(f"âŒ [AIå†³ç­–è¿‡ç¨‹] è‚¡ç¥¨ {symbol} - AIåˆ†æå¤±è´¥: {e}")
            return self._create_error_result(symbol, str(e), start_time)
    
    def _full_ai_analysis(self, symbol: str, 
                         stock_data: Dict[str, Any],
                         market_data: Dict[str, Any] = None,
                         config: AISelectionConfig = None) -> AIAnalysisResult:
        """å®Œæ•´AIåˆ†æ - ä½¿ç”¨æ‰€æœ‰AIå¼•æ“"""
        try:
            logger.debug(f"ğŸ”¬ [å®Œæ•´AIåˆ†æ] å¼€å§‹: {symbol}")
            
            # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰AIå¼•æ“åˆ†æ
            future_tasks = []
            
            # 1. ä¸“å®¶å§”å‘˜ä¼šåˆ†æ
            if self.expert_committee:
                future_tasks.append(
                    self.thread_pool.submit(
                        self.expert_committee.analyze_stock_committee,
                        symbol, stock_data, market_data.get('news_data', []) if market_data else []
                    )
                )
            
            # 2. è‡ªé€‚åº”ç­–ç•¥åˆ†æ
            if self.adaptive_engine and market_data:
                future_tasks.append(
                    self.thread_pool.submit(
                        self.adaptive_engine.get_adaptive_recommendations,
                        market_data, [{'symbol': symbol, **stock_data}]
                    )
                )
            
            # 3. æ¨¡å¼è¯†åˆ«åˆ†æ
            if self.pattern_recognizer and 'price_data' in stock_data:
                price_data = stock_data['price_data']
                volume_data = stock_data.get('volume_data', [])
                future_tasks.append(
                    self.thread_pool.submit(
                        self.pattern_recognizer.recognize_patterns,
                        symbol, price_data, volume_data
                    )
                )
            
            # æ”¶é›†ç»“æœ
            results = {
                'expert_result': None,
                'adaptive_result': None, 
                'pattern_result': None
            }
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ (å¸¦è¶…æ—¶)
            timeout = config.timeout_seconds if config else 30.0
            for i, future in enumerate(as_completed(future_tasks, timeout=timeout)):
                try:
                    if i == 0:  # ä¸“å®¶å§”å‘˜ä¼š
                        results['expert_result'] = future.result()
                    elif i == 1:  # è‡ªé€‚åº”ç­–ç•¥
                        results['adaptive_result'] = future.result()
                    elif i == 2:  # æ¨¡å¼è¯†åˆ«
                        results['pattern_result'] = future.result()
                except Exception as e:
                    logger.warning(f"âš ï¸ AIå¼•æ“åˆ†æå¤±è´¥: {e}")
                    continue
            
            # èåˆæ‰€æœ‰ç»“æœ
            return self._fuse_analysis_results(symbol, results, config)
            
        except Exception as e:
            logger.error(f"âŒ å®Œæ•´AIåˆ†æå¤±è´¥: {symbol} - {e}")
            return self._create_error_result(symbol, str(e))
    
    def _ai_enhanced_analysis(self, symbol: str, 
                            stock_data: Dict[str, Any],
                            market_data: Dict[str, Any] = None,
                            config: AISelectionConfig = None) -> AIAnalysisResult:
        """AIå¢å¼ºåˆ†æ - å¹³è¡¡æ€§èƒ½å’Œæ™ºèƒ½åº¦"""
        try:
            logger.debug(f"âš¡ [AIå¢å¼ºåˆ†æ] å¼€å§‹: {symbol}")
            
            # é€‰æ‹©æœ€é‡è¦çš„AIå¼•æ“è¿›è¡Œåˆ†æ
            results = {}
            
            # 1. ä¼˜å…ˆä¸“å®¶å§”å‘˜ä¼šåˆ†æ (æƒé‡æœ€é«˜)
            if self.expert_committee:
                try:
                    expert_result = self.expert_committee.analyze_stock_committee(
                        symbol, stock_data, 
                        market_data.get('news_data', []) if market_data else []
                    )
                    results['expert_result'] = expert_result
                except Exception as e:
                    logger.warning(f"âš ï¸ ä¸“å®¶å§”å‘˜ä¼šåˆ†æå¤±è´¥: {e}")
            
            # 2. å¦‚æœæœ‰å¸‚åœºæ•°æ®ï¼Œè¿›è¡Œè‡ªé€‚åº”åˆ†æ
            if self.adaptive_engine and market_data:
                try:
                    adaptive_result = self.adaptive_engine.get_adaptive_recommendations(
                        market_data, [{'symbol': symbol, **stock_data}]
                    )
                    results['adaptive_result'] = adaptive_result
                except Exception as e:
                    logger.warning(f"âš ï¸ è‡ªé€‚åº”ç­–ç•¥åˆ†æå¤±è´¥: {e}")
            
            # 3. è½»é‡çº§æ¨¡å¼è¯†åˆ«
            if self.pattern_recognizer and 'price_data' in stock_data:
                try:
                    pattern_result = self.pattern_recognizer.recognize_patterns(
                        symbol, stock_data['price_data'][-20:],  # åªåˆ†ææœ€è¿‘20å¤©
                        stock_data.get('volume_data', [])[-20:] if stock_data.get('volume_data') else None
                    )
                    results['pattern_result'] = pattern_result
                except Exception as e:
                    logger.warning(f"âš ï¸ æ¨¡å¼è¯†åˆ«åˆ†æå¤±è´¥: {e}")
            
            return self._fuse_analysis_results(symbol, results, config)
            
        except Exception as e:
            logger.error(f"âŒ AIå¢å¼ºåˆ†æå¤±è´¥: {symbol} - {e}")
            return self._create_error_result(symbol, str(e))
    
    def _expert_committee_analysis(self, symbol: str, 
                                 stock_data: Dict[str, Any],
                                 market_data: Dict[str, Any] = None) -> AIAnalysisResult:
        """ä¸“å®¶å§”å‘˜ä¼šåˆ†ææ¨¡å¼"""
        try:
            if not self.expert_committee:
                logger.warning(f"âš ï¸ [ä¸“å®¶å§”å‘˜ä¼š] è‚¡ç¥¨ {symbol} - ä¸“å®¶å§”å‘˜ä¼šæœªåˆå§‹åŒ–")
                return self._create_error_result(symbol, "ä¸“å®¶å§”å‘˜ä¼šæœªåˆå§‹åŒ–")
            
            logger.info(f"ğŸ‘¥ [ä¸“å®¶å§”å‘˜ä¼š] è‚¡ç¥¨ {symbol} - å¼€å§‹ä¸“å®¶å§”å‘˜ä¼šåˆ†æ")
            logger.info(f"ğŸ” [ä¸“å®¶å§”å‘˜ä¼š] è‚¡ç¥¨ {symbol} - å¯åŠ¨6åAIä¸“å®¶åˆ†æå¸ˆ")
            
            expert_result = self.expert_committee.analyze_stock_committee(
                symbol, stock_data, 
                market_data.get('news_data', []) if market_data else []
            )
            
            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            committee_decision = expert_result.get('committee_decision', {})
            
            # æ˜¾ç¤ºä¸“å®¶å§”å‘˜ä¼šå†³ç­–è¿‡ç¨‹
            logger.info(f"ğŸ“Š [ä¸“å®¶å§”å‘˜ä¼š] è‚¡ç¥¨ {symbol} - ä¸“å®¶å§”å‘˜ä¼šå†³ç­–å®Œæˆ")
            logger.info(f"ğŸ¯ [ä¸“å®¶å§”å‘˜ä¼š] è‚¡ç¥¨ {symbol} - å§”å‘˜ä¼šç»¼åˆè¯„åˆ†: {committee_decision.get('score', 50):.1f}")
            logger.info(f"ğŸ’¡ [ä¸“å®¶å§”å‘˜ä¼š] è‚¡ç¥¨ {symbol} - æŠ•èµ„å»ºè®®: {committee_decision.get('recommendation', 'è§‚æœ›')}")
            logger.info(f"ğŸ¤ [ä¸“å®¶å§”å‘˜ä¼š] è‚¡ç¥¨ {symbol} - ä¸“å®¶ä¸€è‡´æ€§: {committee_decision.get('consensus_level', 'æœªçŸ¥')}")
            logger.info(f"ğŸ“Š [ä¸“å®¶å§”å‘˜ä¼š] è‚¡ç¥¨ {symbol} - ç½®ä¿¡åº¦: {committee_decision.get('confidence', 0.5):.2f}")
            
            # æ˜¾ç¤ºå„ä¸“å®¶æ„è§
            expert_opinions = expert_result.get('expert_opinions', {})
            if expert_opinions:
                logger.info(f"ğŸ‘¨â€ğŸ’¼ [ä¸“å®¶æ„è§] è‚¡ç¥¨ {symbol} - å„ä¸“å®¶è¯„åˆ†:")
                for expert_name, opinion in expert_opinions.items():
                    score = opinion.get('score', 0)
                    recommendation = opinion.get('recommendation', 'è§‚æœ›')
                    confidence = opinion.get('confidence', 0)
                    logger.info(f"   â€¢ {expert_name}: {score:.1f}åˆ† ({recommendation}, ç½®ä¿¡åº¦: {confidence:.2f})")
            
            # æ˜¾ç¤ºå†³ç­–å› ç´ 
            decision_factors = committee_decision.get('decision_factors', [])
            if decision_factors:
                logger.info(f"ğŸ”‘ [ä¸“å®¶å§”å‘˜ä¼š] è‚¡ç¥¨ {symbol} - å…³é”®å†³ç­–å› ç´ : {' | '.join(decision_factors[:3])}")
            
            # æ˜¾ç¤ºé£é™©å› ç´ 
            risk_factors = expert_result.get('final_recommendation', {}).get('risk_factors', [])
            if risk_factors:
                logger.info(f"âš ï¸ [ä¸“å®¶å§”å‘˜ä¼š] è‚¡ç¥¨ {symbol} - é£é™©å› ç´ : {' | '.join(risk_factors[:2])}")
            
            # æ˜¾ç¤ºæœºä¼šå› ç´ 
            opportunity_factors = expert_result.get('final_recommendation', {}).get('key_reasons', [])
            if opportunity_factors:
                logger.info(f"ğŸŒŸ [ä¸“å®¶å§”å‘˜ä¼š] è‚¡ç¥¨ {symbol} - æœºä¼šå› ç´ : {' | '.join(opportunity_factors[:2])}")
            
            result = AIAnalysisResult(
                symbol=symbol,
                overall_score=committee_decision.get('score', 50),
                confidence_level=committee_decision.get('confidence', 0.5),
                recommendation=committee_decision.get('recommendation', 'è§‚æœ›'),
                risk_assessment=self._assess_risk_from_expert_analysis(expert_result),
                expert_committee_score=committee_decision.get('score', 50),
                expert_analysis=expert_result,
                key_factors=committee_decision.get('decision_factors', [])[:3],
                risk_factors=[f"ä¸“å®¶ä¸€è‡´æ€§: {committee_decision.get('consensus_level', 'æœªçŸ¥')}"],
                opportunity_factors=expert_result.get('final_recommendation', {}).get('key_reasons', [])[:2]
            )
            
            logger.info(f"âœ… [ä¸“å®¶å§”å‘˜ä¼š] è‚¡ç¥¨ {symbol} - ä¸“å®¶å§”å‘˜ä¼šåˆ†æå®Œæˆ")
            return result
            
        except Exception as e:
            logger.error(f"âŒ ä¸“å®¶å§”å‘˜ä¼šåˆ†æå¤±è´¥: {symbol} - {e}")
            return self._create_error_result(symbol, str(e))
    
    def _adaptive_analysis(self, symbol: str, 
                         stock_data: Dict[str, Any],
                         market_data: Dict[str, Any] = None) -> AIAnalysisResult:
        """è‡ªé€‚åº”ç­–ç•¥åˆ†ææ¨¡å¼"""
        try:
            if not self.adaptive_engine or not market_data:
                return self._create_error_result(symbol, "è‡ªé€‚åº”å¼•æ“æœªåˆå§‹åŒ–æˆ–ç¼ºå°‘å¸‚åœºæ•°æ®")
            
            adaptive_result = self.adaptive_engine.get_adaptive_recommendations(
                market_data, [{'symbol': symbol, **stock_data}]
            )
            
            # ä»æ¨èç»“æœä¸­æå–ä¿¡æ¯
            recommended_stocks = adaptive_result.get('recommended_stocks', [])
            target_stock = next((s for s in recommended_stocks if s.get('symbol') == symbol), {})
            
            if not target_stock:
                return self._create_error_result(symbol, "æœªåœ¨è‡ªé€‚åº”æ¨èä¸­æ‰¾åˆ°ç›®æ ‡è‚¡ç¥¨")
            
            adaptive_score = target_stock.get('adaptive_score', 50)
            
            return AIAnalysisResult(
                symbol=symbol,
                overall_score=adaptive_score,
                confidence_level=adaptive_result.get('selected_strategy', {}).get('confidence', 0.6),
                recommendation=self._convert_score_to_recommendation(adaptive_score),
                risk_assessment=adaptive_result.get('risk_assessment', {}).get('overall_risk', 'ä¸­ç­‰é£é™©'),
                adaptive_strategy_score=adaptive_score,
                market_regime=adaptive_result.get('market_regime', 'æœªçŸ¥'),
                key_factors=[
                    f"å¸‚åœºç¯å¢ƒ: {adaptive_result.get('market_regime', 'æœªçŸ¥')}",
                    f"ç­–ç•¥ç±»å‹: {adaptive_result.get('selected_strategy', {}).get('type', 'æœªçŸ¥')}"
                ],
                risk_factors=[adaptive_result.get('risk_assessment', {}).get('overall_risk', 'ä¸­ç­‰é£é™©')],
                opportunity_factors=[adaptive_result.get('strategy_reasoning', '').split('\n')[0][:50] + '...']
            )
            
        except Exception as e:
            logger.error(f"âŒ è‡ªé€‚åº”ç­–ç•¥åˆ†æå¤±è´¥: {symbol} - {e}")
            return self._create_error_result(symbol, str(e))
    
    def _pattern_based_analysis(self, symbol: str, stock_data: Dict[str, Any]) -> AIAnalysisResult:
        """åŸºäºæ¨¡å¼è¯†åˆ«çš„åˆ†æ"""
        try:
            if not self.pattern_recognizer or 'price_data' not in stock_data:
                return self._create_error_result(symbol, "æ¨¡å¼è¯†åˆ«å™¨æœªåˆå§‹åŒ–æˆ–ç¼ºå°‘ä»·æ ¼æ•°æ®")
            
            patterns = self.pattern_recognizer.recognize_patterns(
                symbol, stock_data['price_data'],
                stock_data.get('volume_data', [])
            )
            
            if not patterns:
                return AIAnalysisResult(
                    symbol=symbol,
                    overall_score=50,
                    confidence_level=0.3,
                    recommendation='è§‚æœ›',
                    risk_assessment='æ•°æ®ä¸è¶³',
                    pattern_recognition_score=50,
                    detected_patterns=['æ— æ˜æ˜¾æ¨¡å¼'],
                    key_factors=['æœªæ£€æµ‹åˆ°æ˜æ˜¾æ¨¡å¼'],
                    risk_factors=['æ¨¡å¼ä¸æ˜ç¡®'],
                    opportunity_factors=['éœ€è¦æ›´å¤šæ•°æ®']
                )
            
            # è®¡ç®—æ¨¡å¼ç»¼åˆè¯„åˆ†
            pattern_score = self._calculate_pattern_score(patterns)
            
            # è·å–æœ€å¼ºæ¨¡å¼
            strongest_pattern = max(patterns, key=lambda p: p.confidence * p.strength / 100)
            
            return AIAnalysisResult(
                symbol=symbol,
                overall_score=pattern_score,
                confidence_level=strongest_pattern.confidence,
                recommendation=strongest_pattern.expected_direction,
                risk_assessment=strongest_pattern.risk_level,
                pattern_recognition_score=pattern_score,
                detected_patterns=[p.description for p in patterns[:3]],
                key_factors=[f"ä¸»è¦æ¨¡å¼: {strongest_pattern.pattern_type.value}"],
                risk_factors=[f"é£é™©çº§åˆ«: {strongest_pattern.risk_level}"],
                opportunity_factors=[strongest_pattern.description]
            )
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å¼è¯†åˆ«åˆ†æå¤±è´¥: {symbol} - {e}")
            return self._create_error_result(symbol, str(e))
    
    def _similarity_based_analysis(self, symbol: str, stock_data: Dict[str, Any]) -> AIAnalysisResult:
        """åŸºäºç›¸ä¼¼æ€§çš„åˆ†æ"""
        try:
            if not self.similarity_engine:
                return self._create_error_result(symbol, "ç›¸ä¼¼æ€§å¼•æ“æœªåˆå§‹åŒ–")
            
            # ç®€åŒ–çš„ç›¸ä¼¼æ€§åˆ†æ (éœ€è¦å€™é€‰è‚¡ç¥¨åˆ—è¡¨æ‰èƒ½å®Œæ•´è¿è¡Œ)
            # è¿™é‡Œè¿”å›åŸºäºè‚¡ç¥¨æ•°æ®çš„åŸºç¡€è¯„åˆ†
            
            basic_score = self._calculate_basic_similarity_score(stock_data)
            
            return AIAnalysisResult(
                symbol=symbol,
                overall_score=basic_score,
                confidence_level=0.6,
                recommendation=self._convert_score_to_recommendation(basic_score),
                risk_assessment='ä¸­ç­‰',
                similarity_score=basic_score,
                similar_stocks=[],  # éœ€è¦å€™é€‰åˆ—è¡¨æ‰èƒ½è®¡ç®—
                key_factors=['åŸºäºè‚¡ç¥¨åŸºæœ¬ç‰¹å¾è¯„åˆ†'],
                risk_factors=['éœ€è¦æ›´å¤šç›¸ä¼¼è‚¡ç¥¨å¯¹æ¯”'],
                opportunity_factors=['å…·å¤‡ç›¸ä¼¼æ€§åˆ†ææ½œåŠ›']
            )
            
        except Exception as e:
            logger.error(f"âŒ ç›¸ä¼¼æ€§åˆ†æå¤±è´¥: {symbol} - {e}")
            return self._create_error_result(symbol, str(e))
    
    def _basic_analysis(self, symbol: str, stock_data: Dict[str, Any]) -> AIAnalysisResult:
        """åŸºç¡€åˆ†ææ¨¡å¼"""
        try:
            # åŸºäºè‚¡ç¥¨æ•°æ®çš„ç®€å•è¯„åˆ†
            basic_score = self._calculate_basic_score(stock_data)
            
            return AIAnalysisResult(
                symbol=symbol,
                overall_score=basic_score,
                confidence_level=0.4,
                recommendation=self._convert_score_to_recommendation(basic_score),
                risk_assessment='æœªè¯„ä¼°',
                key_factors=['åŸºäºåŸºç¡€æ•°æ®è®¡ç®—'],
                risk_factors=['æœªè¿›è¡Œæ·±åº¦é£é™©åˆ†æ'],
                opportunity_factors=['åŸºç¡€è¯„åˆ†ç»“æœ']
            )
            
        except Exception as e:
            logger.error(f"âŒ åŸºç¡€åˆ†æå¤±è´¥: {symbol} - {e}")
            return self._create_error_result(symbol, str(e))
    
    def _fuse_analysis_results(self, symbol: str, 
                             results: Dict[str, Any],
                             config: AISelectionConfig = None) -> AIAnalysisResult:
        """èåˆå¤šä¸ªAIå¼•æ“çš„åˆ†æç»“æœ"""
        try:
            config = config or AISelectionConfig()
            
            # åˆå§‹åŒ–èåˆç»“æœ
            fused_result = AIAnalysisResult(
                symbol=symbol,
                overall_score=0.0,
                confidence_level=0.0,
                recommendation='è§‚æœ›',
                risk_assessment='æœªè¯„ä¼°',
                key_factors=[],
                risk_factors=[],
                opportunity_factors=[]
            )
            
            total_weight = 0.0
            total_score = 0.0
            confidence_scores = []
            
            # èåˆä¸“å®¶å§”å‘˜ä¼šç»“æœ
            if 'expert_result' in results and results['expert_result']:
                expert_result = results['expert_result']
                committee_decision = expert_result.get('committee_decision', {})
                
                score = committee_decision.get('score', 50)
                confidence = committee_decision.get('confidence', 0.5)
                weight = config.expert_committee_weight
                
                total_score += score * weight
                total_weight += weight
                confidence_scores.append(confidence * weight)
                
                fused_result.expert_committee_score = score
                fused_result.expert_analysis = expert_result
                fused_result.key_factors.extend(committee_decision.get('decision_factors', [])[:2])
                
            # èåˆè‡ªé€‚åº”ç­–ç•¥ç»“æœ
            if 'adaptive_result' in results and results['adaptive_result']:
                adaptive_result = results['adaptive_result']
                recommended_stocks = adaptive_result.get('recommended_stocks', [])
                
                if recommended_stocks:
                    target_stock = recommended_stocks[0] if recommended_stocks else {}
                    score = target_stock.get('adaptive_score', 50)
                    confidence = adaptive_result.get('selected_strategy', {}).get('confidence', 0.6)
                    weight = config.adaptive_strategy_weight
                    
                    total_score += score * weight
                    total_weight += weight
                    confidence_scores.append(confidence * weight)
                    
                    fused_result.adaptive_strategy_score = score
                    fused_result.market_regime = adaptive_result.get('market_regime', 'æœªçŸ¥')
                    fused_result.key_factors.append(f"å¸‚åœºç¯å¢ƒ: {adaptive_result.get('market_regime', 'æœªçŸ¥')}")
                    
            # èåˆæ¨¡å¼è¯†åˆ«ç»“æœ
            if 'pattern_result' in results and results['pattern_result']:
                patterns = results['pattern_result']
                if patterns:
                    pattern_score = self._calculate_pattern_score(patterns)
                    strongest_pattern = max(patterns, key=lambda p: p.confidence * p.strength / 100)
                    weight = config.pattern_recognition_weight
                    
                    total_score += pattern_score * weight
                    total_weight += weight
                    confidence_scores.append(strongest_pattern.confidence * weight)
                    
                    fused_result.pattern_recognition_score = pattern_score
                    fused_result.detected_patterns = [p.pattern_type.value for p in patterns[:3]]
                    fused_result.key_factors.append(f"ä¸»è¦æ¨¡å¼: {strongest_pattern.pattern_type.value}")
                    fused_result.risk_factors.append(f"æ¨¡å¼é£é™©: {strongest_pattern.risk_level}")
                    
            # è®¡ç®—ç»¼åˆè¯„åˆ†å’Œç½®ä¿¡åº¦
            if total_weight > 0:
                fused_result.overall_score = round(total_score / total_weight, 1)
                fused_result.confidence_level = round(sum(confidence_scores) / total_weight, 3)
            else:
                fused_result.overall_score = 50.0
                fused_result.confidence_level = 0.3
                
            # ç”Ÿæˆç»¼åˆå»ºè®®
            fused_result.recommendation = self._generate_fused_recommendation(fused_result, results)
            fused_result.risk_assessment = self._generate_fused_risk_assessment(fused_result, results)
            
            # æ¸…ç†å’Œä¼˜åŒ–å…³é”®å› ç´ 
            fused_result.key_factors = list(set(fused_result.key_factors))[:5]
            fused_result.risk_factors = list(set(fused_result.risk_factors))[:3]
            fused_result.opportunity_factors = list(set(fused_result.opportunity_factors))[:3]
            
            return fused_result
            
        except Exception as e:
            logger.error(f"âŒ ç»“æœèåˆå¤±è´¥: {symbol} - {e}")
            return self._create_error_result(symbol, str(e))
    
    def _generate_fused_recommendation(self, fused_result: AIAnalysisResult, 
                                     results: Dict[str, Any]) -> str:
        """ç”Ÿæˆèåˆå»ºè®®"""
        try:
            score = fused_result.overall_score
            confidence = fused_result.confidence_level
            
            # åŸºäºè¯„åˆ†å’Œç½®ä¿¡åº¦ç”Ÿæˆå»ºè®®
            if score >= 80 and confidence >= 0.7:
                return "å¼ºçƒˆæ¨è"
            elif score >= 70 and confidence >= 0.6:
                return "æ¨è"
            elif score >= 60 and confidence >= 0.5:
                return "è°¨æ…æ¨è"
            elif score >= 50:
                return "è§‚æœ›"
            elif score >= 40:
                return "è°¨æ…"
            else:
                return "ä¸æ¨è"
                
        except Exception:
            return "è§‚æœ›"
    
    def _generate_fused_risk_assessment(self, fused_result: AIAnalysisResult,
                                      results: Dict[str, Any]) -> str:
        """ç”Ÿæˆèåˆé£é™©è¯„ä¼°"""
        try:
            # ç»¼åˆå„å¼•æ“çš„é£é™©è¯„ä¼°
            risk_indicators = []
            
            if fused_result.confidence_level < 0.5:
                risk_indicators.append("ç½®ä¿¡åº¦è¾ƒä½")
            
            if fused_result.overall_score < 60:
                risk_indicators.append("è¯„åˆ†åä½")
            
            if len(fused_result.risk_factors) > 2:
                risk_indicators.append("é£é™©å› ç´ è¾ƒå¤š")
            
            # æ ¹æ®é£é™©æŒ‡æ ‡æ•°é‡åˆ¤æ–­æ•´ä½“é£é™©
            risk_count = len(risk_indicators)
            if risk_count >= 3:
                return "é«˜é£é™©"
            elif risk_count == 2:
                return "ä¸­é«˜é£é™©"
            elif risk_count == 1:
                return "ä¸­ç­‰é£é™©"
            else:
                return "ä½é£é™©"
                
        except Exception:
            return "é£é™©æœªè¯„ä¼°"
    
    def _calculate_pattern_score(self, patterns: List[Any]) -> float:
        """è®¡ç®—æ¨¡å¼ç»¼åˆè¯„åˆ†"""
        try:
            if not patterns:
                return 50.0
                
            total_score = 0.0
            total_weight = 0.0
            
            for pattern in patterns:
                # æ¨¡å¼è¯„åˆ† = ç½®ä¿¡åº¦ * å¼ºåº¦
                pattern_score = pattern.confidence * pattern.strength / 100 * 100
                weight = pattern.confidence
                
                total_score += pattern_score * weight
                total_weight += weight
                
            return round(total_score / total_weight if total_weight > 0 else 50.0, 1)
            
        except Exception:
            return 50.0
    
    def _calculate_basic_score(self, stock_data: Dict[str, Any]) -> float:
        """è®¡ç®—åŸºç¡€è¯„åˆ†"""
        try:
            score = 50.0  # åŸºç¡€åˆ†
            
            # ä»·æ ¼å˜åŠ¨
            price_change = stock_data.get('price_change_pct', 0)
            if price_change > 0:
                score += min(10, price_change * 2)
            else:
                score += max(-10, price_change * 2)
                
            # å¸‚ç›ˆç‡
            pe_ratio = stock_data.get('pe_ratio', 20)
            if 10 <= pe_ratio <= 25:
                score += 5
            elif pe_ratio > 50:
                score -= 10
                
            # å¸‚å‡€ç‡  
            pb_ratio = stock_data.get('pb_ratio', 2)
            if 1 <= pb_ratio <= 3:
                score += 5
            elif pb_ratio > 5:
                score -= 5
                
            return max(0, min(100, score))
            
        except Exception:
            return 50.0
            
    def _calculate_basic_similarity_score(self, stock_data: Dict[str, Any]) -> float:
        """è®¡ç®—åŸºç¡€ç›¸ä¼¼æ€§è¯„åˆ†"""
        try:
            # åŸºäºè‚¡ç¥¨åŸºæœ¬ç‰¹å¾çš„è¯„åˆ†
            score = 50.0
            
            # åŸºæœ¬é¢æŒ‡æ ‡æƒé‡è¯„åˆ†
            fundamentals = [
                ('roe', 0.15, 15),  # ROE 15%æƒé‡
                ('pe_ratio', -0.002, 20),  # PEé€‚ä¸­åŠ åˆ†
                ('revenue_growth', 0.3, 15),  # è¥æ”¶å¢é•¿
                ('market_cap', 0.00000001, 1000000000)  # å¸‚å€¼è§„æ¨¡
            ]
            
            for key, weight, baseline in fundamentals:
                value = stock_data.get(key, baseline)
                if key == 'pe_ratio':
                    # PEé€‚ä¸­ä¸ºå¥½
                    score += max(-10, min(10, (25 - abs(value - 15)) * weight))
                else:
                    score += min(10, max(-10, (value - baseline) * weight))
            
            return max(0, min(100, score))
            
        except Exception:
            return 50.0
    
    def _convert_score_to_recommendation(self, score: float) -> str:
        """å°†è¯„åˆ†è½¬æ¢ä¸ºæŠ•èµ„å»ºè®®"""
        if score >= 85:
            return "å¼ºçƒˆæ¨è"
        elif score >= 75:
            return "æ¨è"  
        elif score >= 65:
            return "è°¨æ…æ¨è"
        elif score >= 55:
            return "è§‚æœ›"
        elif score >= 45:
            return "è°¨æ…"
        else:
            return "ä¸æ¨è"
    
    def _assess_risk_from_expert_analysis(self, expert_result: Dict[str, Any]) -> str:
        """ä»ä¸“å®¶åˆ†æä¸­è¯„ä¼°é£é™©"""
        try:
            final_recommendation = expert_result.get('final_recommendation', {})
            risk_assessment = final_recommendation.get('risk_assessment', 'ä¸­ç­‰é£é™©')
            
            # åŸºäºä¸“å®¶ä¸€è‡´æ€§è°ƒæ•´é£é™©è¯„ä¼°
            consensus_level = expert_result.get('committee_decision', {}).get('consensus_level', 'åŸºæœ¬ä¸€è‡´')
            
            if consensus_level == 'æ„è§åˆ†åŒ–':
                return 'é«˜é£é™©'
            elif consensus_level == 'å­˜åœ¨åˆ†æ­§':
                return 'ä¸­é«˜é£é™©'
            else:
                return risk_assessment
                
        except Exception:
            return 'é£é™©æœªè¯„ä¼°'
    
    def _get_cached_analysis(self, symbol: str, stock_data: Dict[str, Any]) -> Optional[AIAnalysisResult]:
        """è·å–ç¼“å­˜çš„åˆ†æç»“æœ"""
        try:
            cache_key = f"{symbol}_{hash(str(sorted(stock_data.items())))}"
            
            if cache_key in self.analysis_cache:
                cached_item = self.analysis_cache[cache_key]
                
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                if (datetime.now() - cached_item['timestamp']).total_seconds() < self.cache_ttl:
                    return cached_item['result']
                else:
                    # åˆ é™¤è¿‡æœŸç¼“å­˜
                    del self.analysis_cache[cache_key]
            
            return None
            
        except Exception:
            return None
    
    def _cache_analysis(self, symbol: str, stock_data: Dict[str, Any], result: AIAnalysisResult):
        """ç¼“å­˜åˆ†æç»“æœ"""
        try:
            cache_key = f"{symbol}_{hash(str(sorted(stock_data.items())))}"
            
            self.analysis_cache[cache_key] = {
                'result': result,
                'timestamp': datetime.now()
            }
            
            # é™åˆ¶ç¼“å­˜å¤§å°
            if len(self.analysis_cache) > 1000:
                # åˆ é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
                oldest_key = min(self.analysis_cache.keys(), 
                               key=lambda k: self.analysis_cache[k]['timestamp'])
                del self.analysis_cache[oldest_key]
                
        except Exception as e:
            logger.debug(f"ç¼“å­˜å¤±è´¥: {e}")
    
    def _update_performance_stats(self, processing_time: float):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        try:
            self.performance_stats['total_analyses'] += 1
            
            # æ›´æ–°å¹³å‡å¤„ç†æ—¶é—´
            total_time = (self.performance_stats['average_processing_time'] * 
                         (self.performance_stats['total_analyses'] - 1) + processing_time)
            self.performance_stats['average_processing_time'] = total_time / self.performance_stats['total_analyses']
            
        except Exception:
            pass
    
    def _create_error_result(self, symbol: str, error_msg: str, start_time: datetime = None) -> AIAnalysisResult:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        processing_time = (datetime.now() - start_time).total_seconds() if start_time else 0.0
        
        return AIAnalysisResult(
            symbol=symbol,
            overall_score=30.0,  # é”™è¯¯æƒ…å†µä¸‹ç»™è¾ƒä½è¯„åˆ†
            confidence_level=0.1,
            recommendation='æ•°æ®ä¸è¶³',
            risk_assessment='åˆ†æå¤±è´¥',
            key_factors=[f'åˆ†æé”™è¯¯: {error_msg}'],
            risk_factors=['AIåˆ†æå¤±è´¥'],
            opportunity_factors=[],
            processing_time=processing_time,
            timestamp=datetime.now()
        )
    
    def batch_analyze_stocks(self, stock_list: List[Dict[str, Any]], 
                           market_data: Dict[str, Any] = None,
                           config: AISelectionConfig = None) -> List[AIAnalysisResult]:
        """æ‰¹é‡AIåˆ†æè‚¡ç¥¨"""
        try:
            logger.info(f"ğŸ¤– [AIç­–ç•¥ç®¡ç†å™¨] å¼€å§‹æ‰¹é‡AIåˆ†æï¼Œè‚¡ç¥¨æ•°é‡: {len(stock_list)}")
            logger.info(f"ğŸ” [æ‰¹é‡åˆ†æ] å¼€å§‹å¯¹ {len(stock_list)} åªè‚¡ç¥¨è¿›è¡ŒAIæ™ºèƒ½åˆ†æ")
            logger.info(f"ğŸ“Š [æ‰¹é‡åˆ†æ] åˆ†ææ¨¡å¼: {config.ai_mode.value if config else 'AIå¢å¼º'}")
            
            results = []
            config = config or AISelectionConfig()
            
            # æ˜¾ç¤ºæ‰¹é‡åˆ†æè¿›åº¦
            total_stocks = len(stock_list)
            processed_count = 0
            successful_count = 0
            failed_count = 0
            
            # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å¹¶è¡Œå¤„ç†
            if config.parallel_processing and len(stock_list) > 1:
                logger.info(f"âš¡ [æ‰¹é‡åˆ†æ] ä½¿ç”¨å¹¶è¡Œå¤„ç†æ¨¡å¼ï¼Œå¹¶å‘æ•°: {min(4, total_stocks)}")
                
                # å¹¶è¡Œå¤„ç†
                future_tasks = []
                
                for stock_info in stock_list:
                    symbol = stock_info.get('symbol', stock_info.get('ts_code', ''))
                    if symbol:
                        future_task = self.thread_pool.submit(
                            self.analyze_stock_with_ai,
                            symbol, stock_info, market_data, config
                        )
                        future_tasks.append((symbol, future_task))
                
                # æ”¶é›†ç»“æœ
                for symbol, future_task in future_tasks:
                    try:
                        processed_count += 1
                        result = future_task.result(timeout=config.timeout_seconds)
                        results.append(result)
                        successful_count += 1
                        
                        # æ˜¾ç¤ºè¿›åº¦
                        if processed_count % 5 == 0 or processed_count == total_stocks:
                            logger.info(f"ğŸ“Š [æ‰¹é‡è¿›åº¦] å·²å¤„ç† {processed_count}/{total_stocks} åªè‚¡ç¥¨ (æˆåŠŸ: {successful_count}, å¤±è´¥: {failed_count})")
                        
                    except Exception as e:
                        processed_count += 1
                        failed_count += 1
                        logger.warning(f"âš ï¸ æ‰¹é‡åˆ†æå¤±è´¥: {symbol} - {e}")
                        results.append(self._create_error_result(symbol, str(e)))
                        
                        # æ˜¾ç¤ºè¿›åº¦
                        if processed_count % 5 == 0 or processed_count == total_stocks:
                            logger.info(f"ğŸ“Š [æ‰¹é‡è¿›åº¦] å·²å¤„ç† {processed_count}/{total_stocks} åªè‚¡ç¥¨ (æˆåŠŸ: {successful_count}, å¤±è´¥: {failed_count})")
                        
            else:
                logger.info(f"ğŸ”„ [æ‰¹é‡åˆ†æ] ä½¿ç”¨é¡ºåºå¤„ç†æ¨¡å¼")
                
                # é¡ºåºå¤„ç†
                for stock_info in stock_list:
                    symbol = stock_info.get('symbol', stock_info.get('ts_code', ''))
                    if symbol:
                        try:
                            processed_count += 1
                            result = self.analyze_stock_with_ai(symbol, stock_info, market_data, config)
                            results.append(result)
                            successful_count += 1
                            
                            # æ˜¾ç¤ºè¿›åº¦
                            if processed_count % 5 == 0 or processed_count == total_stocks:
                                logger.info(f"ğŸ“Š [æ‰¹é‡è¿›åº¦] å·²å¤„ç† {processed_count}/{total_stocks} åªè‚¡ç¥¨ (æˆåŠŸ: {successful_count}, å¤±è´¥: {failed_count})")
                            
                        except Exception as e:
                            processed_count += 1
                            failed_count += 1
                            logger.warning(f"âš ï¸ åˆ†æå¤±è´¥: {symbol} - {e}")
                            results.append(self._create_error_result(symbol, str(e)))
                            
                            # æ˜¾ç¤ºè¿›åº¦
                            if processed_count % 5 == 0 or processed_count == total_stocks:
                                logger.info(f"ğŸ“Š [æ‰¹é‡è¿›åº¦] å·²å¤„ç† {processed_count}/{total_stocks} åªè‚¡ç¥¨ (æˆåŠŸ: {successful_count}, å¤±è´¥: {failed_count})")
            
            # æ˜¾ç¤ºæ‰¹é‡åˆ†æç»“æœç»Ÿè®¡
            logger.info(f"ğŸ‰ [æ‰¹é‡åˆ†æ] æ‰¹é‡AIåˆ†æå®Œæˆ")
            logger.info(f"ğŸ“Š [æ‰¹é‡ç»Ÿè®¡] æ€»è‚¡ç¥¨æ•°: {total_stocks}")
            logger.info(f"âœ… [æ‰¹é‡ç»Ÿè®¡] æˆåŠŸåˆ†æ: {successful_count}")
            logger.info(f"âŒ [æ‰¹é‡ç»Ÿè®¡] åˆ†æå¤±è´¥: {failed_count}")
            logger.info(f"ğŸ“ˆ [æ‰¹é‡ç»Ÿè®¡] æˆåŠŸç‡: {successful_count/total_stocks*100:.1f}%")
            
            # æ˜¾ç¤ºAIè¯„åˆ†ç»Ÿè®¡
            if results:
                ai_scores = [r.overall_score for r in results if r.overall_score > 0]
                if ai_scores:
                    avg_score = sum(ai_scores) / len(ai_scores)
                    max_score = max(ai_scores)
                    min_score = min(ai_scores)
                    logger.info(f"ğŸ¯ [æ‰¹é‡ç»Ÿè®¡] AIè¯„åˆ†ç»Ÿè®¡ - å¹³å‡: {avg_score:.1f}, æœ€é«˜: {max_score:.1f}, æœ€ä½: {min_score:.1f}")
                    
                    # è¯„åˆ†åˆ†å¸ƒ
                    high_score_count = len([s for s in ai_scores if s >= 80])
                    medium_score_count = len([s for s in ai_scores if 60 <= s < 80])
                    low_score_count = len([s for s in ai_scores if s < 60])
                    logger.info(f"ğŸ“Š [æ‰¹é‡ç»Ÿè®¡] è¯„åˆ†åˆ†å¸ƒ - ä¼˜ç§€(â‰¥80): {high_score_count}, è‰¯å¥½(60-79): {medium_score_count}, ä¸€èˆ¬(<60): {low_score_count}")
            
            logger.info(f"ğŸ¤– [AIç­–ç•¥ç®¡ç†å™¨] æ‰¹é‡AIåˆ†æå®Œæˆï¼ŒæˆåŠŸåˆ†æ: {len(results)} åªè‚¡ç¥¨")
            return results
            
        except Exception as e:
            logger.error(f"âŒ [AIç­–ç•¥ç®¡ç†å™¨] æ‰¹é‡åˆ†æå¤±è´¥: {e}")
            return []
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–AIå¼•æ“æ€§èƒ½æ‘˜è¦"""
        try:
            ai_engines_status = {
                'expert_committee': self.expert_committee is not None,
                'adaptive_engine': self.adaptive_engine is not None,
                'pattern_recognizer': self.pattern_recognizer is not None,
                'similarity_engine': self.similarity_engine is not None
            }
            
            # è®¡ç®—å¯ç”¨å¼•æ“æ•°é‡
            available_engines = sum(ai_engines_status.values())
            total_engines = len(ai_engines_status)
            availability_rate = (available_engines / total_engines * 100) if total_engines > 0 else 0
            
            return {
                'total_analyses': self.performance_stats['total_analyses'],
                'cache_hit_rate': (self.performance_stats['cache_hits'] / 
                                 max(1, self.performance_stats['total_analyses']) * 100),
                'average_processing_time': self.performance_stats['average_processing_time'],
                'cache_size': len(self.analysis_cache),
                'ai_engines_status': ai_engines_status,
                'engine_availability': {
                    'available_count': available_engines,
                    'total_count': total_engines,
                    'availability_rate': availability_rate
                },
                'engine_initialization': self.performance_stats.get('engine_initialization', {}),
                'ai_enabled': available_engines > 0,
                'fully_operational': available_engines == total_engines,
                'timestamp': datetime.now()
            }
            
        except Exception as e:
            logger.error(f"âŒ æ€§èƒ½æ‘˜è¦è·å–å¤±è´¥: {e}")
            return {
                'error': str(e),
                'ai_enabled': False,
                'ai_engines_status': {
                    'expert_committee': False,
                    'adaptive_engine': False,
                    'pattern_recognizer': False,
                    'similarity_engine': False
                }
            }
    
    def clear_cache(self):
        """æ¸…ç†åˆ†æç¼“å­˜"""
        try:
            self.analysis_cache.clear()
            logger.info("ğŸ§¹ AIåˆ†æç¼“å­˜å·²æ¸…ç†")
        except Exception as e:
            logger.error(f"âŒ ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")
    
    def __del__(self):
        """ææ„å‡½æ•°"""
        try:
            if hasattr(self, 'thread_pool'):
                self.thread_pool.shutdown(wait=False)
        except Exception:
            pass


# å…¨å±€AIç­–ç•¥ç®¡ç†å™¨å®ä¾‹
_ai_strategy_manager = None

def get_ai_strategy_manager(config: Dict[str, Any] = None) -> AIStrategyManager:
    """è·å–å…¨å±€AIç­–ç•¥ç®¡ç†å™¨å®ä¾‹"""
    global _ai_strategy_manager
    if _ai_strategy_manager is None:
        _ai_strategy_manager = AIStrategyManager(config)
    return _ai_strategy_manager