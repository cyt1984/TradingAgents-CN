#!/usr/bin/env python3
"""
æ™ºèƒ½é€‰è‚¡æ ¸å¿ƒå¼•æ“
åŸºäºå¤šæºæ•°æ®èåˆå’Œç»¼åˆè¯„åˆ†ç³»ç»Ÿçš„Aè‚¡é€‰è‚¡å¼•æ“
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Any, Union, Tuple
from datetime import datetime, timedelta
import time
from dataclasses import dataclass, field
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed

# å¯¼å…¥ç›¸å…³æ¨¡å—
from .filter_conditions import (
    FilterCondition, FilterGroup, FilterLogic, STOCK_FILTER_FIELDS,
    NumericFilter, EnumFilter, BooleanFilter, FilterOperator
)
from ..analytics.comprehensive_scoring_system import get_comprehensive_scoring_system, ComprehensiveScore
from ..analytics.data_fusion_engine import get_fusion_engine
from ..dataflows.enhanced_data_manager import EnhancedDataManager
from ..utils.logging_manager import get_logger
from .ai_strategies.ai_strategy_manager import get_ai_strategy_manager, AIMode, AISelectionConfig
from .intelligent_sampling import get_intelligent_sampler, SamplingConfig, SamplingStrategy
from .batch_ai_processor import get_batch_ai_processor, BatchConfig, ProcessingStrategy
from ..analytics.longhubang_analyzer import get_longhubang_analyzer, LongHuBangAnalysisResult
from ..dataflows.longhubang_utils import get_longhubang_provider, RankingType

logger = get_logger('agents')


@dataclass
class SelectionCriteria:
    """é€‰è‚¡æ ‡å‡†"""
    filters: List[Union[FilterCondition, FilterGroup]] = field(default_factory=list)  # ç­›é€‰æ¡ä»¶
    sort_by: Optional[str] = 'overall_score'                     # æ’åºå­—æ®µ
    sort_ascending: bool = False                                 # æ’åºæ–¹å‘
    limit: Optional[int] = 100                                   # ç»“æœæ•°é‡é™åˆ¶
    include_scores: bool = True                                  # æ˜¯å¦åŒ…å«è¯„åˆ†æ•°æ®
    include_basic_info: bool = True                              # æ˜¯å¦åŒ…å«åŸºæœ¬ä¿¡æ¯
    
    # AIå¢å¼ºé€‰é¡¹
    ai_mode: AIMode = AIMode.BASIC                               # AIæ¨¡å¼
    ai_config: Optional[AISelectionConfig] = None               # AIé…ç½®
    
    # æ™ºèƒ½é‡‡æ ·é€‰é¡¹
    enable_smart_sampling: bool = True                           # å¯ç”¨æ™ºèƒ½é‡‡æ ·
    sampling_config: Optional[SamplingConfig] = None            # é‡‡æ ·é…ç½®
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'filters': [f.to_dict() if hasattr(f, 'to_dict') else str(f) for f in self.filters],
            'sort_by': self.sort_by,
            'sort_ascending': self.sort_ascending,
            'limit': self.limit,
            'include_scores': self.include_scores,
            'include_basic_info': self.include_basic_info
        }


@dataclass  
class SelectionResult:
    """é€‰è‚¡ç»“æœ"""
    symbols: List[str]                          # è‚¡ç¥¨ä»£ç åˆ—è¡¨
    data: pd.DataFrame                          # è¯¦ç»†æ•°æ®
    summary: Dict[str, Any]                     # ç»Ÿè®¡æ‘˜è¦
    criteria: SelectionCriteria                 # é€‰è‚¡æ ‡å‡†
    execution_time: float                       # æ‰§è¡Œæ—¶é—´
    total_candidates: int                       # å€™é€‰è‚¡ç¥¨æ€»æ•°
    filtered_count: int                         # ç­›é€‰åæ•°é‡
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'symbols': self.symbols,
            'data': self.data.to_dict('records') if not self.data.empty else [],
            'summary': self.summary,
            'criteria': self.criteria.to_dict(),
            'execution_time': self.execution_time,
            'total_candidates': self.total_candidates,
            'filtered_count': self.filtered_count
        }


class StockSelector:
    """æ™ºèƒ½é€‰è‚¡å¼•æ“"""
    
    def __init__(self, cache_enabled: bool = True):
        """
        åˆå§‹åŒ–é€‰è‚¡å¼•æ“
        
        Args:
            cache_enabled: æ˜¯å¦å¯ç”¨ç¼“å­˜æœºåˆ¶
        """
        self.cache_enabled = cache_enabled
        self.data_manager = None
        self.scoring_system = None
        self.fusion_engine = None
        self._stock_cache = {}
        self._cache_timestamp = None
        self._cache_ttl = 300  # ç¼“å­˜5åˆ†é’Ÿ
        
        # AIç­–ç•¥ç®¡ç†å™¨
        self.ai_strategy_manager = None
        
        # æ™ºèƒ½é‡‡æ ·å™¨
        self.intelligent_sampler = None
        
        # AIæ‰¹æ¬¡å¤„ç†å™¨
        self.batch_processor = None
        
        # é¾™è™æ¦œåˆ†æå™¨
        self.longhubang_analyzer = None
        
        # é¾™è™æ¦œæ•°æ®æä¾›å™¨
        self.longhubang_provider = None
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._init_components()
    
    def _init_components(self):
        """åˆå§‹åŒ–ç›¸å…³ç»„ä»¶"""
        try:
            # åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
            self.data_manager = EnhancedDataManager()
            logger.info("âœ… æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            
            # åˆå§‹åŒ–ç»¼åˆè¯„åˆ†ç³»ç»Ÿ
            self.scoring_system = get_comprehensive_scoring_system()
            logger.info("âœ… ç»¼åˆè¯„åˆ†ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
            
            # åˆå§‹åŒ–æ•°æ®èåˆå¼•æ“
            self.fusion_engine = get_fusion_engine()
            logger.info("âœ… æ•°æ®èåˆå¼•æ“åˆå§‹åŒ–æˆåŠŸ")
            
            
            # åˆå§‹åŒ–AIç­–ç•¥ç®¡ç†å™¨
            try:
                self.ai_strategy_manager = get_ai_strategy_manager()
                logger.info("âœ… AIç­–ç•¥ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as ai_error:
                logger.warning(f"âš ï¸ AIç­–ç•¥ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨åŸºç¡€æ¨¡å¼: {ai_error}")
                self.ai_strategy_manager = None
            
            # åˆå§‹åŒ–æ™ºèƒ½é‡‡æ ·å™¨
            try:
                self.intelligent_sampler = get_intelligent_sampler()
                logger.info("âœ… æ™ºèƒ½é‡‡æ ·å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as sampling_error:
                logger.warning(f"âš ï¸ æ™ºèƒ½é‡‡æ ·å™¨åˆå§‹åŒ–å¤±è´¥: {sampling_error}")
                self.intelligent_sampler = None
            
            # åˆå§‹åŒ–AIæ‰¹æ¬¡å¤„ç†å™¨
            try:
                batch_config = BatchConfig(
                    batch_size=20,
                    max_workers=8,
                    strategy=ProcessingStrategy.HYBRID,
                    enable_progress_tracking=True,
                    enable_auto_scaling=True,
                    cache_results=True
                )
                self.batch_processor = get_batch_ai_processor(batch_config)
                logger.info("âœ… AIæ‰¹æ¬¡å¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as batch_error:
                logger.warning(f"âš ï¸ AIæ‰¹æ¬¡å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {batch_error}")
                self.batch_processor = None
            
            # åˆå§‹åŒ–é¾™è™æ¦œåˆ†æå™¨
            try:
                self.longhubang_analyzer = get_longhubang_analyzer()
                logger.info("âœ… é¾™è™æ¦œåˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as longhubang_error:
                logger.warning(f"âš ï¸ é¾™è™æ¦œåˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {longhubang_error}")
                self.longhubang_analyzer = None
            
            # åˆå§‹åŒ–é¾™è™æ¦œæ•°æ®æä¾›å™¨
            try:
                self.longhubang_provider = get_longhubang_provider()
                logger.info("âœ… é¾™è™æ¦œæ•°æ®æä¾›å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as provider_error:
                logger.warning(f"âš ï¸ é¾™è™æ¦œæ•°æ®æä¾›å™¨åˆå§‹åŒ–å¤±è´¥: {provider_error}")
                self.longhubang_provider = None
            
        except Exception as e:
            logger.error(f"âŒ ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def get_available_fields(self) -> Dict[str, Dict[str, Any]]:
        """è·å–å¯ç”¨çš„ç­›é€‰å­—æ®µ"""
        return STOCK_FILTER_FIELDS.copy()
    
    def create_numeric_filter(
        self, 
        field_name: str, 
        operator: FilterOperator, 
        value: Union[float, int, Tuple[float, float]]
    ) -> NumericFilter:
        """åˆ›å»ºæ•°å€¼ç­›é€‰æ¡ä»¶"""
        field_info = STOCK_FILTER_FIELDS.get(field_name, {})
        return NumericFilter(
            field_name=field_name,
            field_display_name=field_info.get('name', field_name),
            operator=operator,
            value=value,
            description=field_info.get('description', '')
        )
    
    def create_enum_filter(
        self, 
        field_name: str, 
        operator: FilterOperator, 
        value: Union[str, List[str]]
    ) -> EnumFilter:
        """åˆ›å»ºæšä¸¾ç­›é€‰æ¡ä»¶"""
        field_info = STOCK_FILTER_FIELDS.get(field_name, {})
        return EnumFilter(
            field_name=field_name,
            field_display_name=field_info.get('name', field_name),
            operator=operator,
            value=value,
            allowed_values=field_info.get('allowed_values'),
            description=field_info.get('description', '')
        )
    
    def create_boolean_filter(
        self, 
        field_name: str, 
        operator: FilterOperator, 
        value: bool
    ) -> BooleanFilter:
        """åˆ›å»ºå¸ƒå°”ç­›é€‰æ¡ä»¶"""
        field_info = STOCK_FILTER_FIELDS.get(field_name, {})
        return BooleanFilter(
            field_name=field_name,
            field_display_name=field_info.get('name', field_name),
            operator=operator,
            value=value,
            description=field_info.get('description', '')
        )
    
    def _get_stock_list(self) -> pd.DataFrame:
        """è·å–è‚¡ç¥¨åˆ—è¡¨ - ä»…ä½¿ç”¨å…è´¹æ•°æ®æº"""
        # æ£€æŸ¥ç¼“å­˜
        if (self.cache_enabled and 
            self._stock_cache is not None and 
            self._cache_timestamp and 
            time.time() - self._cache_timestamp < self._cache_ttl):
            logger.info("ä»ç¼“å­˜è·å–è‚¡ç¥¨åˆ—è¡¨")
            return self._stock_cache.copy()
        
        try:
            # ä»…ä½¿ç”¨å¢å¼ºæ•°æ®ç®¡ç†å™¨è·å–Aè‚¡æ•°æ®ï¼Œé¿å…Tushareæƒé™é—®é¢˜
            if self.data_manager and hasattr(self.data_manager, 'get_stock_list'):
                stock_data = self.data_manager.get_stock_list('A')
                if stock_data and len(stock_data) > 0:
                    # å°†åˆ—è¡¨è½¬æ¢ä¸ºDataFrame
                    stock_list = pd.DataFrame(stock_data)
                    # é‡å‘½ååˆ—ä»¥ä¿æŒä¸€è‡´æ€§
                    if 'symbol' in stock_list.columns and 'name' in stock_list.columns:
                        stock_list = stock_list.rename(columns={'symbol': 'ts_code', 'name': 'name'})
                    logger.info(f"ä»å¢å¼ºæ•°æ®ç®¡ç†å™¨è·å–Aè‚¡åˆ—è¡¨æˆåŠŸ: {len(stock_list)}åªè‚¡ç¥¨")
                    
                    # æ›´æ–°ç¼“å­˜
                    if self.cache_enabled:
                        self._stock_cache = stock_list.copy()
                        self._cache_timestamp = time.time()
                    
                    return stock_list
            
            logger.warning("è·å–çš„è‚¡ç¥¨åˆ—è¡¨ä¸ºç©º")
            return pd.DataFrame()
            
        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _enrich_stock_data(self, stock_data: pd.DataFrame, include_scores: bool = True) -> pd.DataFrame:
        """ä¸°å¯Œè‚¡ç¥¨æ•°æ®"""
        if stock_data.empty:
            return stock_data
        
        enriched_data = stock_data.copy()
        
        # æ·»åŠ ç»¼åˆè¯„åˆ†æ•°æ®
        if include_scores and self.scoring_system:
            try:
                logger.info("ğŸ”„ æ­£åœ¨è·å–ç»¼åˆè¯„åˆ†æ•°æ®...")
                
                # æ‰¹é‡è·å–è¯„åˆ†ï¼ˆé™åˆ¶å¹¶å‘æ•°é‡é¿å…APIé™åˆ¶ï¼‰
                batch_size = 10
                symbols = enriched_data['ts_code'].tolist()
                all_scores = []
                
                for i in range(0, len(symbols), batch_size):
                    batch_symbols = symbols[i:i + batch_size]
                    batch_scores = []
                    
                    for symbol in batch_symbols:
                        try:
                            # è·å–åŸºç¡€æ•°æ®ç”¨äºè¯„åˆ†
                            basic_data = self.data_manager.get_latest_price_data(symbol)
                            if basic_data:
                                score = self.scoring_system.calculate_comprehensive_score(symbol, basic_data)
                                batch_scores.append({
                                    'ts_code': symbol,
                                    'overall_score': score.overall_score,
                                    'grade': score.grade,
                                    'technical_score': score.category_scores.get('technical', {}).get('score', 0),
                                    'fundamental_score': score.category_scores.get('fundamental', {}).get('score', 0),
                                    'sentiment_score': score.category_scores.get('sentiment', {}).get('score', 0),
                                    'quality_score': score.category_scores.get('quality', {}).get('score', 0),
                                    'risk_score': score.category_scores.get('risk', {}).get('score', 0)
                                })
                        except Exception as e:
                            logger.warning(f"âš ï¸ è·å– {symbol} è¯„åˆ†å¤±è´¥: {e}")
                            continue
                    
                    all_scores.extend(batch_scores)
                    
                    # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                    if i + batch_size < len(symbols):
                        time.sleep(0.1)
                
                if all_scores:
                    scores_df = pd.DataFrame(all_scores)
                    enriched_data = enriched_data.merge(scores_df, on='ts_code', how='left')
                    logger.info(f"âœ… æ·»åŠ è¯„åˆ†æ•°æ®æˆåŠŸ: {len(scores_df)}æ¡è®°å½•")
                else:
                    logger.warning("âš ï¸ æœªè·å–åˆ°æœ‰æ•ˆçš„è¯„åˆ†æ•°æ®")
                    
            except Exception as e:
                logger.error(f"âŒ è·å–ç»¼åˆè¯„åˆ†æ•°æ®å¤±è´¥: {e}")
        
        return enriched_data
    
    def _enrich_stock_data_with_ai(self, stock_data: pd.DataFrame, 
                                  ai_config: AISelectionConfig = None) -> pd.DataFrame:
        """ä½¿ç”¨AIå¢å¼ºè‚¡ç¥¨æ•°æ®"""
        if stock_data.empty:
            logger.warning("âš ï¸ è‚¡ç¥¨æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡AIå¢å¼º")
            return stock_data
        
        if not self.ai_strategy_manager:
            logger.warning("âš ï¸ AIç­–ç•¥ç®¡ç†å™¨æœªåˆå§‹åŒ–ï¼Œè·³è¿‡AIå¢å¼º")
            return stock_data
            
        try:
            logger.info(f"ğŸ¤– å¼€å§‹AIå¢å¼ºæ•°æ®å¤„ç†ï¼Œè‚¡ç¥¨æ•°é‡: {len(stock_data)}")
            
            ai_config = ai_config or AISelectionConfig()
            enriched_data = stock_data.copy()
            
            # æ£€æŸ¥AIå¼•æ“å¯ç”¨æ€§
            ai_status = self.ai_strategy_manager.get_performance_summary()
            available_engines = ai_status.get('engine_availability', {}).get('available_count', 0)
            
            if available_engines == 0:
                logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„AIå¼•æ“ï¼Œä½¿ç”¨åŸºç¡€è¯„åˆ†")
                # æ·»åŠ åŸºç¡€AIè¯„åˆ†åˆ—ä»¥ä¿æŒå…¼å®¹æ€§
                enriched_data['ai_overall_score'] = enriched_data.get('overall_score', 50)
                enriched_data['ai_confidence'] = 0.3
                enriched_data['ai_recommendation'] = 'æ•°æ®ä¸è¶³'
                enriched_data['ai_risk_assessment'] = 'æœªè¯„ä¼°'
                return enriched_data
            
            logger.info(f"ğŸ¤– å‘ç° {available_engines} ä¸ªå¯ç”¨AIå¼•æ“")
            
            # å‡†å¤‡è‚¡ç¥¨ä»£ç åˆ—è¡¨
            stock_symbols = []
            stock_symbol_to_data = {}
            
            for _, row in stock_data.iterrows():
                symbol = row.get('ts_code', '')
                if symbol:
                    stock_symbols.append(symbol)
                    stock_info = row.to_dict()
                    
                    # è·å–é¢å¤–çš„è‚¡ç¥¨æ•°æ®
                    try:
                        if hasattr(self, 'data_manager') and self.data_manager:
                            basic_data = self.data_manager.get_latest_price_data(symbol)
                            if basic_data:
                                stock_info.update(basic_data)
                    except Exception as e:
                        logger.debug(f"è·å– {symbol} åŸºç¡€æ•°æ®å¤±è´¥: {e}")
                    
                    stock_symbol_to_data[symbol] = stock_info
            
            # ä½¿ç”¨æ–°çš„æ‰¹æ¬¡å¤„ç†å™¨è¿›è¡ŒAIåˆ†æ
            ai_results = []
            if self.batch_processor and stock_symbols:
                logger.info(f"ğŸš€ ä½¿ç”¨AIæ‰¹æ¬¡å¤„ç†å™¨åˆ†æ {len(stock_symbols)} åªè‚¡ç¥¨...")
                
                # å®šä¹‰åˆ†æå›è°ƒå‡½æ•°
                def ai_analysis_callback(symbol: str) -> Dict[str, Any]:
                    """AIåˆ†æå›è°ƒå‡½æ•°"""
                    try:
                        stock_info = stock_symbol_to_data.get(symbol, {})
                        
                        # å‡†å¤‡å¸‚åœºæ•°æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰
                        market_data = None
                        if ai_config.market_data_required:
                            try:
                                market_data = {
                                    'market_type': 'Aè‚¡',
                                    'timestamp': datetime.now(),
                                    'news_data': []
                                }
                            except Exception:
                                market_data = None
                        
                        # è°ƒç”¨AIç­–ç•¥ç®¡ç†å™¨è¿›è¡Œåˆ†æ
                        batch_results = self.ai_strategy_manager.batch_analyze_stocks(
                            [stock_info], market_data=market_data, config=ai_config
                        )
                        
                        if batch_results and len(batch_results) > 0:
                            return {
                                'symbol': symbol,
                                'ai_result': batch_results[0],
                                'success': True
                            }
                        else:
                            return {
                                'symbol': symbol,
                                'ai_result': None,
                                'success': False,
                                'error': 'AIåˆ†æè¿”å›ç©ºç»“æœ'
                            }
                            
                    except Exception as e:
                        logger.debug(f"AIåˆ†æå›è°ƒå¤±è´¥ {symbol}: {e}")
                        return {
                            'symbol': symbol,
                            'ai_result': None,
                            'success': False,
                            'error': str(e)
                        }
                
                # æ‰§è¡Œæ‰¹é‡å¤„ç†
                processing_report = self.batch_processor.process_stocks(
                    stock_symbols, 
                    analysis_callback=ai_analysis_callback
                )
                
                # å¤„ç†æ‰¹æ¬¡å¤„ç†ç»“æœ
                for symbol in stock_symbols:
                    # ä»æ‰¹å¤„ç†å™¨çš„ç¼“å­˜ä¸­è·å–ç»“æœ
                    if hasattr(self.batch_processor, '_results_cache') and symbol in self.batch_processor._results_cache:
                        batch_result = self.batch_processor._results_cache[symbol]
                        if batch_result.success and batch_result.analysis_result:
                            # æå–ç¤¾äº¤ä¿¡å·æ•°æ®
                            if 'social_signals' in batch_result.analysis_result:
                                idx = enriched_data[enriched_data['ts_code'] == symbol].index
                                if len(idx) > 0:
                                    enriched_data.at[idx[0], 'social_signals'] = ','.join(batch_result.analysis_result['social_signals'])
                            
                            # æå–ç¤¾äº¤è¯„åˆ†æ•°æ®
                            if 'analyst_results' in batch_result.analysis_result and 'social' in batch_result.analysis_result['analyst_results']:
                                social_data = batch_result.analysis_result['analyst_results']['social']
                                idx = enriched_data[enriched_data['ts_code'] == symbol].index
                                if len(idx) > 0:
                                    enriched_data.at[idx[0], 'social_score'] = social_data.get('social_score', 50)
                                    xueqiu_data = social_data.get('xueqiu_sentiment', {})
                                    enriched_data.at[idx[0], 'social_heat'] = xueqiu_data.get('total_discussions', 0)
                                    enriched_data.at[idx[0], 'social_sentiment'] = xueqiu_data.get('sentiment_score', 0)
                            
                            ai_result_data = batch_result.analysis_result.get('ai_result')
                            if ai_result_data:
                                ai_results.append(ai_result_data)
                            else:
                                # åˆ›å»ºé»˜è®¤ç»“æœ
                                default_result = self._create_default_ai_result(symbol, enriched_data)
                                ai_results.append(default_result)
                        else:
                            # åˆ›å»ºé»˜è®¤ç»“æœ
                            default_result = self._create_default_ai_result(symbol, enriched_data)
                            ai_results.append(default_result)
                    else:
                        # åˆ›å»ºé»˜è®¤ç»“æœ
                        default_result = self._create_default_ai_result(symbol, enriched_data)
                        ai_results.append(default_result)
                
                # è®°å½•å¤„ç†ç»Ÿè®¡
                logger.info(f"ğŸ¤– AIæ‰¹æ¬¡å¤„ç†å®Œæˆ:")
                logger.info(f"ğŸ“Š æ€»è‚¡ç¥¨æ•°: {processing_report.total_stocks}")
                logger.info(f"âœ… æˆåŠŸå¤„ç†: {processing_report.successful_stocks}")
                logger.info(f"âŒ å¤±è´¥æ•°é‡: {processing_report.failed_stocks}")
                logger.info(f"â±ï¸ æ€»è€—æ—¶: {processing_report.total_time:.2f}ç§’")
                logger.info(f"ğŸš€ å¤„ç†ååé‡: {processing_report.throughput:.2f}è‚¡ç¥¨/ç§’")
                logger.info(f"ğŸ’¾ å†…å­˜å³°å€¼: {processing_report.memory_peak:.1f}%")
                
            else:
                # å›é€€åˆ°åŸæœ‰çš„æ‰¹å¤„ç†æ–¹å¼
                logger.warning("âš ï¸ AIæ‰¹æ¬¡å¤„ç†å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿæ‰¹å¤„ç†æ–¹å¼")
                ai_results = self._fallback_ai_processing(stock_symbol_to_data, ai_config, enriched_data)
            
            # å°†AIåˆ†æç»“æœåˆå¹¶åˆ°æ•°æ®ä¸­
            if ai_results:
                ai_scores = []
                for ai_result in ai_results:
                    # å®‰å…¨åœ°è·å–å±æ€§å€¼
                    ai_score_entry = {
                        'ts_code': getattr(ai_result, 'symbol', ''),
                        'ai_overall_score': float(getattr(ai_result, 'overall_score', 50.0)),
                        'ai_confidence': float(getattr(ai_result, 'confidence_level', 0.5)),
                        'ai_recommendation': str(getattr(ai_result, 'recommendation', 'è§‚æœ›')),
                        'ai_risk_assessment': str(getattr(ai_result, 'risk_assessment', 'æœªè¯„ä¼°')),
                    }
                    
                    # å¯é€‰çš„AIå¼•æ“ç‰¹å®šè¯„åˆ†
                    if hasattr(ai_result, 'expert_committee_score') and ai_result.expert_committee_score is not None:
                        ai_score_entry['expert_committee_score'] = float(ai_result.expert_committee_score)
                    
                    if hasattr(ai_result, 'adaptive_strategy_score') and ai_result.adaptive_strategy_score is not None:
                        ai_score_entry['adaptive_strategy_score'] = float(ai_result.adaptive_strategy_score)
                        
                    if hasattr(ai_result, 'pattern_recognition_score') and ai_result.pattern_recognition_score is not None:
                        ai_score_entry['pattern_recognition_score'] = float(ai_result.pattern_recognition_score)
                    
                    if hasattr(ai_result, 'market_regime') and ai_result.market_regime:
                        ai_score_entry['market_regime'] = str(ai_result.market_regime)
                    
                    # è½¬æ¢åˆ—è¡¨ä¸ºå­—ç¬¦ä¸²
                    detected_patterns = getattr(ai_result, 'detected_patterns', [])
                    if detected_patterns and isinstance(detected_patterns, (list, tuple)):
                        ai_score_entry['detected_patterns'] = str(detected_patterns)
                    elif detected_patterns:
                        ai_score_entry['detected_patterns'] = str(detected_patterns)
                    
                    key_factors = getattr(ai_result, 'key_factors', [])
                    if key_factors and isinstance(key_factors, (list, tuple)):
                        ai_score_entry['key_factors'] = str(key_factors)
                    elif key_factors:
                        ai_score_entry['key_factors'] = str(key_factors)
                    
                    ai_scores.append(ai_score_entry)
                
                if ai_scores:
                    ai_scores_df = pd.DataFrame(ai_scores)
                    # ä½¿ç”¨å·¦è¿æ¥ç¡®ä¿ä¸ä¸¢å¤±åŸå§‹æ•°æ®
                    enriched_data = enriched_data.merge(ai_scores_df, on='ts_code', how='left')
                    
                    logger.info(f"âœ… AIå¢å¼ºæ•°æ®å¤„ç†å®Œæˆ: {len(ai_scores_df)} æ¡AIåˆ†æç»“æœå·²åˆå¹¶")
                    
                    # å¡«å……ç¼ºå¤±çš„AIåˆ†ææ•°æ®
                    ai_columns = ['ai_overall_score', 'ai_confidence', 'ai_recommendation', 'ai_risk_assessment']
                    for col in ai_columns:
                        if col in enriched_data.columns:
                            if col == 'ai_overall_score':
                                enriched_data[col].fillna(enriched_data.get('overall_score', 50), inplace=True)
                            elif col == 'ai_confidence':
                                enriched_data[col].fillna(0.3, inplace=True)
                            elif col == 'ai_recommendation':
                                enriched_data[col].fillna('æ•°æ®ä¸è¶³', inplace=True)
                            elif col == 'ai_risk_assessment':
                                enriched_data[col].fillna('æœªè¯„ä¼°', inplace=True)
                else:
                    logger.warning("âš ï¸ AIåˆ†æç»“æœå¤„ç†åä¸ºç©º")
            else:
                logger.warning("âš ï¸ æœªè·å–åˆ°ä»»ä½•AIåˆ†æç»“æœï¼Œæ·»åŠ é»˜è®¤AIåˆ—")
                # æ·»åŠ é»˜è®¤AIåˆ—ä»¥ä¿æŒå…¼å®¹æ€§
                enriched_data['ai_overall_score'] = enriched_data.get('overall_score', 50)
                enriched_data['ai_confidence'] = 0.3
                enriched_data['ai_recommendation'] = 'æ•°æ®ä¸è¶³'
                enriched_data['ai_risk_assessment'] = 'æœªåˆ†æ'
            
            return enriched_data
            
        except Exception as e:
            logger.error(f"âŒ AIå¢å¼ºæ•°æ®å¤„ç†å¤±è´¥: {e}")
            return stock_data
    
    def _create_default_ai_result(self, symbol: str, enriched_data: pd.DataFrame):
        """åˆ›å»ºé»˜è®¤AIåˆ†æç»“æœ"""
        try:
            # å°è¯•ä»ç°æœ‰æ•°æ®ä¸­è·å–åŸºç¡€è¯„åˆ†
            default_score = 50.0
            if 'overall_score' in enriched_data.columns:
                symbol_data = enriched_data[enriched_data.get('ts_code', '') == symbol]
                if not symbol_data.empty:
                    default_score = float(symbol_data['overall_score'].iloc[0])
        except:
            default_score = 50.0
        
        return type('AIResult', (), {
            'symbol': symbol,
            'overall_score': default_score,
            'confidence_level': 0.2,
            'recommendation': 'æ•°æ®ä¸è¶³',
            'risk_assessment': 'AIåˆ†æå¤±è´¥',
            'expert_committee_score': None,
            'adaptive_strategy_score': None,
            'pattern_recognition_score': None,
            'market_regime': None,
            'detected_patterns': [],
            'key_factors': ['AIåˆ†æå¤±è´¥'],
            'processing_time': 0.0
        })()
    
    def _fallback_ai_processing(self, stock_symbol_to_data: Dict[str, Dict], 
                               ai_config, enriched_data: pd.DataFrame) -> List:
        """å›é€€çš„AIå¤„ç†æ–¹å¼ï¼ˆåŸæœ‰çš„æ‰¹å¤„ç†é€»è¾‘ï¼‰"""
        try:
            stock_list = list(stock_symbol_to_data.values())
            ai_results = []
            
            # ç®€åŒ–çš„æ‰¹å¤„ç†
            batch_size = 15
            for i in range(0, len(stock_list), batch_size):
                batch = stock_list[i:i + batch_size]
                
                try:
                    # å‡†å¤‡å¸‚åœºæ•°æ®
                    market_data = None
                    if ai_config.market_data_required:
                        market_data = {
                            'market_type': 'Aè‚¡',
                            'timestamp': datetime.now(),
                            'news_data': []
                        }
                    
                    batch_results = self.ai_strategy_manager.batch_analyze_stocks(
                        batch, market_data=market_data, config=ai_config
                    )
                    
                    if batch_results:
                        ai_results.extend(batch_results)
                    else:
                        # åˆ›å»ºé»˜è®¤ç»“æœ
                        for stock_info in batch:
                            symbol = stock_info.get('ts_code', '')
                            default_result = self._create_default_ai_result(symbol, enriched_data)
                            ai_results.append(default_result)
                
                except Exception as e:
                    logger.warning(f"âŒ å›é€€æ‰¹å¤„ç†å¤±è´¥: {e}")
                    # åˆ›å»ºé»˜è®¤ç»“æœ
                    for stock_info in batch:
                        symbol = stock_info.get('ts_code', '')
                        default_result = self._create_default_ai_result(symbol, enriched_data)
                        ai_results.append(default_result)
                
                # æ·»åŠ å»¶è¿Ÿ
                if i + batch_size < len(stock_list):
                    time.sleep(0.2)
            
            return ai_results
            
        except Exception as e:
            logger.error(f"âŒ å›é€€AIå¤„ç†å¤±è´¥: {e}")
            return []
    
    def _apply_filters(self, data: pd.DataFrame, filters: List[Union[FilterCondition, FilterGroup]]) -> pd.DataFrame:
        """åº”ç”¨ç­›é€‰æ¡ä»¶"""
        if not filters or data.empty:
            return data
        
        # åˆ›å»ºæ€»çš„ç­›é€‰ç»„
        main_group = FilterGroup(conditions=filters, logic=FilterLogic.AND)
        
        try:
            # åº”ç”¨ç­›é€‰
            filter_mask = main_group.apply_filter(data)
            filtered_data = data[filter_mask].copy()
            
            logger.info(f"ğŸ” ç­›é€‰å®Œæˆ: {len(data)} -> {len(filtered_data)}åªè‚¡ç¥¨")
            return filtered_data
            
        except Exception as e:
            logger.error(f"âŒ åº”ç”¨ç­›é€‰æ¡ä»¶å¤±è´¥: {e}")
            return data
    
    def _generate_summary(self, data: pd.DataFrame, total_candidates: int) -> Dict[str, Any]:
        """ç”Ÿæˆç»Ÿè®¡æ‘˜è¦"""
        if data.empty:
            return {
                'total_candidates': total_candidates,
                'filtered_count': 0,
                'success_rate': 0.0,
                'statistics': {}
            }
        
        summary = {
            'total_candidates': total_candidates,
            'filtered_count': len(data),
            'success_rate': len(data) / max(total_candidates, 1) * 100,
            'statistics': {}
        }
        
        # åŸºç¡€ç»Ÿè®¡
        numeric_columns = data.select_dtypes(include=[np.number]).columns
        for col in numeric_columns:
            if col in data.columns and not data[col].isna().all():
                summary['statistics'][col] = {
                    'mean': float(data[col].mean()),
                    'median': float(data[col].median()),
                    'std': float(data[col].std()),
                    'min': float(data[col].min()),
                    'max': float(data[col].max()),
                    'count': int(data[col].count())
                }
        
        # è¡Œä¸šåˆ†å¸ƒ
        if 'industry' in data.columns:
            industry_counts = data['industry'].value_counts().head(10)
            summary['industry_distribution'] = industry_counts.to_dict()
        
        # è¯„çº§åˆ†å¸ƒ
        if 'grade' in data.columns:
            grade_counts = data['grade'].value_counts()
            summary['grade_distribution'] = grade_counts.to_dict()
        
        return summary
    
    def select_stocks(self, criteria: SelectionCriteria) -> SelectionResult:
        """
        æ‰§è¡Œé€‰è‚¡æ“ä½œ
        
        Args:
            criteria: é€‰è‚¡æ ‡å‡†
            
        Returns:
            SelectionResult: é€‰è‚¡ç»“æœ
        """
        start_time = time.time()
        logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œæ™ºèƒ½é€‰è‚¡...")
        
        try:
            # 1. è·å–åŸºç¡€è‚¡ç¥¨åˆ—è¡¨
            logger.info("ğŸ“‹ è·å–è‚¡ç¥¨åˆ—è¡¨...")
            stock_data = self._get_stock_list()
            
            if stock_data.empty:
                logger.warning("âš ï¸ æœªè·å–åˆ°è‚¡ç¥¨æ•°æ®")
                return SelectionResult(
                    symbols=[],
                    data=pd.DataFrame(),
                    summary={'error': 'æœªè·å–åˆ°è‚¡ç¥¨æ•°æ®'},
                    criteria=criteria,
                    execution_time=time.time() - start_time,
                    total_candidates=0,
                    filtered_count=0
                )
            
            total_candidates = len(stock_data)
            logger.info(f"ğŸ“Š å€™é€‰è‚¡ç¥¨æ€»æ•°: {total_candidates}")
            
            # 2. æ™ºèƒ½é‡‡æ · (å¤§å¹…å‡å°‘éœ€è¦è¯¦ç»†åˆ†æçš„è‚¡ç¥¨æ•°é‡)
            if criteria.enable_smart_sampling and self.intelligent_sampler and len(stock_data) > (criteria.limit or 100) * 2:
                logger.info(f"ğŸ¯ å¯ç”¨æ™ºèƒ½é‡‡æ ·ï¼Œä¼˜åŒ–æ•°æ®è·å–æµç¨‹...")
                
                # åˆ›å»ºé‡‡æ ·é…ç½®
                sampling_config = criteria.sampling_config or SamplingConfig(
                    strategy=SamplingStrategy.HYBRID,
                    max_candidates=min(800, max((criteria.limit or 100) * 4, len(stock_data) // 4)),  # æ™ºèƒ½ç¡®å®šé‡‡æ ·æ•°é‡
                    min_market_cap=5.0,  # 5äº¿æœ€å°å¸‚å€¼
                    min_daily_volume=5000000,  # 500ä¸‡æœ€å°æˆäº¤é¢
                    min_price=1.0,
                    max_price=300.0,
                    exclude_st_stocks=True,
                    activity_days=30,
                    enable_cache=True
                )
                
                logger.info(f"ğŸ“ˆ é‡‡æ ·ç›®æ ‡: {len(stock_data)} -> {sampling_config.max_candidates}")
                
                # æ‰§è¡Œæ™ºèƒ½é‡‡æ ·
                sampling_result = self.intelligent_sampler.smart_sample(stock_data, sampling_config)
                
                if sampling_result.sampled_stocks:
                    # è¿‡æ»¤åˆ°é‡‡æ ·çš„è‚¡ç¥¨
                    sampled_symbols = set(sampling_result.sampled_stocks)
                    if 'ts_code' in stock_data.columns:
                        stock_data = stock_data[stock_data['ts_code'].isin(sampled_symbols)].reset_index(drop=True)
                    
                    logger.info(f"âœ… æ™ºèƒ½é‡‡æ ·å®Œæˆ: {sampling_result.original_count} -> {len(stock_data)} åªè‚¡ç¥¨")
                    logger.info(f"ğŸ¯ é‡‡æ ·ç­–ç•¥: {sampling_result.strategy_used.value}")
                    logger.info(f"â±ï¸ é‡‡æ ·è€—æ—¶: {sampling_result.execution_time:.2f}ç§’")
                    logger.info(f"ğŸŒŸ è´¨é‡è¯„åˆ†: {sampling_result.quality_score:.2f}")
                else:
                    logger.warning("âš ï¸ æ™ºèƒ½é‡‡æ ·æœªè¿”å›æœ‰æ•ˆç»“æœï¼Œç»§ç»­ä½¿ç”¨å…¨é‡æ•°æ®")
            else:
                if not criteria.enable_smart_sampling:
                    logger.info("ğŸ“Š æ™ºèƒ½é‡‡æ ·å·²ç¦ç”¨ï¼Œä½¿ç”¨å…¨é‡æ•°æ®")
                elif not self.intelligent_sampler:
                    logger.warning("âš ï¸ æ™ºèƒ½é‡‡æ ·å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨å…¨é‡æ•°æ®")
                else:
                    logger.info(f"ğŸ“Š æ•°æ®é‡è¾ƒå°({len(stock_data)})ï¼Œæ— éœ€æ™ºèƒ½é‡‡æ ·")
            
            # 3. ä¸°å¯Œè‚¡ç¥¨æ•°æ®
            if criteria.include_scores or criteria.include_basic_info:
                logger.info("ğŸ”„ æ­£åœ¨ä¸°å¯Œè‚¡ç¥¨æ•°æ®...")
                
                # åŸºç¡€æ•°æ®ä¸°å¯Œ
                stock_data = self._enrich_stock_data(
                    stock_data, 
                    include_scores=criteria.include_scores
                )
                
                # AIå¢å¼ºæ•°æ®ä¸°å¯Œ
                if criteria.ai_mode != AIMode.BASIC and self.ai_strategy_manager:
                    logger.info(f"ğŸ¤– å¯ç”¨AIå¢å¼ºæ¨¡å¼: {criteria.ai_mode.value}")
                    stock_data = self._enrich_stock_data_with_ai(
                        stock_data,
                        criteria.ai_config
                    )
            
            # 3. åº”ç”¨ç­›é€‰æ¡ä»¶
            if criteria.filters:
                logger.info("ğŸ” æ­£åœ¨åº”ç”¨ç­›é€‰æ¡ä»¶...")
                filtered_data = self._apply_filters(stock_data, criteria.filters)
            else:
                filtered_data = stock_data.copy()
            
            # æ›´æ–°å®é™…å€™é€‰æ•°é‡ï¼ˆè€ƒè™‘æ™ºèƒ½é‡‡æ ·åçš„æ•°é‡ï¼‰
            actual_candidates = len(stock_data)
            
            # 4. æ™ºèƒ½æ’åº - ç»“åˆAIè¯„åˆ†å’Œä¼ ç»Ÿè¯„åˆ†
            sort_column = criteria.sort_by
            
            # åˆ›å»ºç»¼åˆè¯„åˆ†åˆ—ç”¨äºæ’åº
            if criteria.ai_mode != AIMode.BASIC and 'ai_overall_score' in filtered_data.columns:
                logger.info("ğŸ¤– ä½¿ç”¨AIå¢å¼ºæ’åºç­–ç•¥")
                
                # è®¡ç®—ç»¼åˆæ™ºèƒ½è¯„åˆ†ï¼ˆåŒ…å«ç¤¾äº¤æ•°æ®ï¼‰
                ai_weight = 0.5  # AIè¯„åˆ†æƒé‡50%
                traditional_weight = 0.3  # ä¼ ç»Ÿè¯„åˆ†æƒé‡30%
                social_weight = 0.2  # ç¤¾äº¤è¯„åˆ†æƒé‡20%
                
                # æ ‡å‡†åŒ–è¯„åˆ†åˆ°0-100èŒƒå›´
                ai_scores = filtered_data['ai_overall_score'].fillna(50)
                traditional_scores = filtered_data.get('overall_score', pd.Series([50] * len(filtered_data)))
                social_scores = filtered_data.get('social_score', pd.Series([50] * len(filtered_data)))
                
                # å¦‚æœæœ‰ç½®ä¿¡åº¦ï¼Œæ ¹æ®ç½®ä¿¡åº¦è°ƒæ•´æƒé‡
                if 'ai_confidence' in filtered_data.columns:
                    confidence_scores = filtered_data['ai_confidence'].fillna(0.5)
                    # é«˜ç½®ä¿¡åº¦æ—¶å¢åŠ AIæƒé‡ï¼Œä½ç½®ä¿¡åº¦æ—¶é™ä½AIæƒé‡
                    # åŠ¨æ€è°ƒæ•´æƒé‡åŸºäºç½®ä¿¡åº¦
                    confidence_factor = confidence_scores.mean()
                    
                    # æ ¹æ®ç½®ä¿¡åº¦è°ƒæ•´å„éƒ¨åˆ†æƒé‡
                    dynamic_ai_weight = ai_weight * (0.5 + confidence_factor * 0.5)  # 0.25-0.5
                    dynamic_traditional_weight = traditional_weight * (1.5 - confidence_factor * 0.5)  # 0.3-0.45
                    dynamic_social_weight = social_weight  # ç¤¾äº¤æƒé‡ä¿æŒç¨³å®š
                    
                    # å½’ä¸€åŒ–æƒé‡
                    total_weight = dynamic_ai_weight + dynamic_traditional_weight + dynamic_social_weight
                    dynamic_ai_weight /= total_weight
                    dynamic_traditional_weight /= total_weight
                    dynamic_social_weight /= total_weight
                    
                    filtered_data['intelligent_score'] = (
                        ai_scores * dynamic_ai_weight + 
                        traditional_scores * dynamic_traditional_weight +
                        social_scores * dynamic_social_weight
                    )
                    logger.info(f"ğŸ¯ ä½¿ç”¨åŠ¨æ€æƒé‡æ™ºèƒ½è¯„åˆ† (AI:{dynamic_ai_weight:.2f}, ä¼ ç»Ÿ:{dynamic_traditional_weight:.2f}, ç¤¾äº¤:{dynamic_social_weight:.2f})")
                else:
                    # å›ºå®šæƒé‡
                    filtered_data['intelligent_score'] = (
                        ai_scores * ai_weight + 
                        traditional_scores * traditional_weight +
                        social_scores * social_weight
                    )
                    logger.info(f"âš–ï¸ ä½¿ç”¨å›ºå®šæƒé‡æ™ºèƒ½è¯„åˆ† (AI:{ai_weight:.1f}, ä¼ ç»Ÿ:{traditional_weight:.1f}, ç¤¾äº¤:{social_weight:.1f})")
                
                # æ·»åŠ ç¤¾äº¤ä¿¡å·åŠ æˆ
                if 'social_signals' in filtered_data.columns:
                    # å¯¹æœ‰å¼ºçƒˆç¤¾äº¤ä¿¡å·çš„è‚¡ç¥¨è¿›è¡ŒåŠ å‡åˆ†
                    for idx, row in filtered_data.iterrows():
                        signals = str(row.get('social_signals', '')).split(',')
                        if 'STRONG_BULLISH' in signals or 'SENTIMENT_SURGE' in signals:
                            filtered_data.at[idx, 'intelligent_score'] = min(100, filtered_data.at[idx, 'intelligent_score'] + 5)
                        elif 'HIGH_HEAT_WARNING' in signals:
                            filtered_data.at[idx, 'intelligent_score'] = max(0, filtered_data.at[idx, 'intelligent_score'] - 5)
                
                # ä¼˜å…ˆä½¿ç”¨æ™ºèƒ½è¯„åˆ†æ’åº
                if sort_column in ['overall_score', 'ai_overall_score'] or not sort_column:
                    sort_column = 'intelligent_score'
                    logger.info("ğŸ§  ä½¿ç”¨æ™ºèƒ½ç»¼åˆè¯„åˆ†æ’åº")
            
            # æ‰§è¡Œæ’åº
            if sort_column and sort_column in filtered_data.columns:
                logger.info(f"ğŸ“ˆ æŒ‰ {sort_column} æ’åº...")
                filtered_data = filtered_data.sort_values(
                    by=sort_column, 
                    ascending=criteria.sort_ascending
                )
            elif 'ai_overall_score' in filtered_data.columns:
                # å¦‚æœæŒ‡å®šçš„æ’åºå­—æ®µä¸å­˜åœ¨ï¼Œä½†æœ‰AIè¯„åˆ†ï¼Œåˆ™ä½¿ç”¨AIè¯„åˆ†
                logger.info("ğŸ¤– é»˜è®¤ä½¿ç”¨AIç»¼åˆè¯„åˆ†æ’åº")
                filtered_data = filtered_data.sort_values(
                    by='ai_overall_score',
                    ascending=False
                )
            elif 'overall_score' in filtered_data.columns:
                # æœ€åä½¿ç”¨ä¼ ç»Ÿè¯„åˆ†
                logger.info("ğŸ“Š ä½¿ç”¨ä¼ ç»Ÿç»¼åˆè¯„åˆ†æ’åº")
                filtered_data = filtered_data.sort_values(
                    by='overall_score',
                    ascending=False
                )
            
            # 5. é™åˆ¶ç»“æœæ•°é‡
            if criteria.limit and len(filtered_data) > criteria.limit:
                filtered_data = filtered_data.head(criteria.limit)
                logger.info(f"âœ‚ï¸ é™åˆ¶ç»“æœæ•°é‡ä¸º: {criteria.limit}")
            
            # 6. ç”Ÿæˆç»“æœ
            symbols = filtered_data['ts_code'].tolist() if 'ts_code' in filtered_data.columns else []
            summary = self._generate_summary(filtered_data, total_candidates)
            
            # åœ¨æ‘˜è¦ä¸­æ·»åŠ æ™ºèƒ½é‡‡æ ·ä¿¡æ¯
            if criteria.enable_smart_sampling and actual_candidates != total_candidates:
                summary['intelligent_sampling'] = {
                    'enabled': True,
                    'original_candidates': total_candidates,
                    'sampled_candidates': actual_candidates,
                    'sampling_ratio': actual_candidates / max(total_candidates, 1),
                    'sampling_efficiency': f"{100 * (1 - actual_candidates / max(total_candidates, 1)):.1f}% æ•°æ®é‡å‡å°‘"
                }
            
            execution_time = time.time() - start_time
            
            result = SelectionResult(
                symbols=symbols,
                data=filtered_data,
                summary=summary,
                criteria=criteria,
                execution_time=execution_time,
                total_candidates=total_candidates,
                filtered_count=len(filtered_data)
            )
            
            logger.info(f"âœ… é€‰è‚¡å®Œæˆ: {len(symbols)}åªè‚¡ç¥¨, è€—æ—¶ {execution_time:.2f}ç§’")
            return result
            
        except Exception as e:
            logger.error(f"âŒ é€‰è‚¡æ“ä½œå¤±è´¥: {e}")
            return SelectionResult(
                symbols=[],
                data=pd.DataFrame(),
                summary={'error': str(e)},
                criteria=criteria,
                execution_time=time.time() - start_time,
                total_candidates=0,
                filtered_count=0
            )
    
    def quick_select(
        self, 
        min_score: float = 70.0, 
        min_market_cap: float = 50.0,
        max_pe_ratio: float = 30.0,
        grades: List[str] = None,
        limit: int = 50
    ) -> SelectionResult:
        """
        å¿«é€Ÿé€‰è‚¡ - ä½¿ç”¨é¢„è®¾çš„å¸¸ç”¨æ¡ä»¶
        
        Args:
            min_score: æœ€å°ç»¼åˆè¯„åˆ†
            min_market_cap: æœ€å°å¸‚å€¼ï¼ˆäº¿å…ƒï¼‰
            max_pe_ratio: æœ€å¤§å¸‚ç›ˆç‡
            grades: æŠ•èµ„ç­‰çº§åˆ—è¡¨
            limit: ç»“æœæ•°é‡é™åˆ¶
            
        Returns:
            SelectionResult: é€‰è‚¡ç»“æœ
        """
        filters = []
        
        # ç»¼åˆè¯„åˆ†ç­›é€‰
        filters.append(self.create_numeric_filter(
            'overall_score', FilterOperator.GREATER_EQUAL, min_score
        ))
        
        # å¸‚å€¼ç­›é€‰
        if min_market_cap > 0:
            filters.append(self.create_numeric_filter(
                'market_cap', FilterOperator.GREATER_EQUAL, min_market_cap
            ))
        
        # å¸‚ç›ˆç‡ç­›é€‰
        if max_pe_ratio > 0:
            filters.append(self.create_numeric_filter(
                'pe_ratio', FilterOperator.LESS_EQUAL, max_pe_ratio
            ))
        
        # æŠ•èµ„ç­‰çº§ç­›é€‰
        if grades:
            filters.append(self.create_enum_filter(
                'grade', FilterOperator.IN, grades
            ))
        
        # æ£€æŸ¥AIæ˜¯å¦å¯ç”¨ï¼Œè‡ªåŠ¨å¯ç”¨AIå¢å¼ºæ¨¡å¼
        ai_mode = AIMode.BASIC
        if self.ai_strategy_manager:
            ai_status = self.ai_strategy_manager.get_performance_summary()
            if ai_status.get('ai_enabled', False):
                ai_mode = AIMode.AI_ENHANCED
                logger.info("ğŸ¤– å¿«é€Ÿé€‰è‚¡è‡ªåŠ¨å¯ç”¨AIå¢å¼ºæ¨¡å¼")
        
        criteria = SelectionCriteria(
            filters=filters,
            sort_by='overall_score',  # å°†åœ¨AIå¢å¼ºæ—¶è‡ªåŠ¨è½¬æ¢ä¸ºæ™ºèƒ½è¯„åˆ†
            sort_ascending=False,
            limit=limit,
            include_scores=True,
            include_basic_info=True,
            ai_mode=ai_mode,
            ai_config=AISelectionConfig(
                ai_mode=ai_mode,
                min_ai_score=min_score,
                min_confidence=0.6,
                parallel_processing=True,
                enable_caching=True
            ) if ai_mode != AIMode.BASIC else None,
            # å¯ç”¨æ™ºèƒ½é‡‡æ ·
            enable_smart_sampling=True,
            sampling_config=SamplingConfig(
                strategy=SamplingStrategy.HYBRID,
                max_candidates=min(1000, limit * 10),  # é‡‡æ ·æ•°é‡ä¸ºç›®æ ‡çš„10å€
                min_market_cap=min_market_cap if min_market_cap > 0 else 5.0,
                min_daily_volume=10000000,  # 1000ä¸‡æœ€å°æˆäº¤é¢
                min_price=2.0,
                exclude_st_stocks=True,
                enable_cache=True
            )
        )
        
        return self.select_stocks(criteria)
    
    def ai_enhanced_select(self,
                          ai_mode: AIMode = AIMode.AI_ENHANCED,
                          min_ai_score: float = 70.0,
                          min_confidence: float = 0.6,
                          max_risk_level: str = "ä¸­ç­‰",
                          limit: int = 50) -> SelectionResult:
        """
        AIå¢å¼ºé€‰è‚¡ - ä½¿ç”¨AIç­–ç•¥è¿›è¡Œæ™ºèƒ½é€‰è‚¡
        
        Args:
            ai_mode: AIæ¨¡å¼
            min_ai_score: æœ€å°AIè¯„åˆ†
            min_confidence: æœ€å°ç½®ä¿¡åº¦
            max_risk_level: æœ€å¤§é£é™©çº§åˆ«
            limit: ç»“æœæ•°é‡é™åˆ¶
            
        Returns:
            SelectionResult: AIé€‰è‚¡ç»“æœ
        """
        if not self.ai_strategy_manager:
            logger.warning("âš ï¸ AIç­–ç•¥ç®¡ç†å™¨æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨åŸºç¡€é€‰è‚¡æ¨¡å¼")
            return self.quick_select(min_score=min_ai_score, limit=limit)
        
        # åˆ›å»ºAIé…ç½®
        ai_config = AISelectionConfig(
            ai_mode=ai_mode,
            min_ai_score=min_ai_score,
            min_confidence=min_confidence,
            max_risk_level=max_risk_level,
            enable_caching=True,
            parallel_processing=True
        )
        
        # åŸºç¡€ç­›é€‰æ¡ä»¶ (ä½¿ç”¨AIè¯„åˆ†)
        filters = []
        
        # åŸºç¡€æ¡ä»¶ä¿è¯æ•°æ®è´¨é‡
        filters.append(self.create_numeric_filter(
            'market_cap', FilterOperator.GREATER_EQUAL, 10.0  # è‡³å°‘10äº¿å¸‚å€¼
        ))
        
        criteria = SelectionCriteria(
            filters=filters,
            sort_by='ai_overall_score',  # ä½¿ç”¨AIè¯„åˆ†æ’åº
            sort_ascending=False,
            limit=limit,
            include_scores=True,
            include_basic_info=True,
            ai_mode=ai_mode,
            ai_config=ai_config,
            # AIå¢å¼ºé€‰è‚¡å¯ç”¨æ›´æ¿€è¿›çš„æ™ºèƒ½é‡‡æ ·
            enable_smart_sampling=True,
            sampling_config=SamplingConfig(
                strategy=SamplingStrategy.HYBRID,
                max_candidates=min(1500, limit * 15),  # AIæ¨¡å¼ä½¿ç”¨æ›´å¤§çš„é‡‡æ ·æ± 
                min_market_cap=8.0,  # 8äº¿æœ€å°å¸‚å€¼
                min_daily_volume=15000000,  # 1500ä¸‡æœ€å°æˆäº¤é¢
                min_price=3.0,
                exclude_st_stocks=True,
                enable_momentum_filter=True,
                momentum_threshold=-15.0,  # è¿‡æ»¤æ‰è·Œå¹…è¿‡å¤§çš„
                enable_cache=True
            )
        )
        
        return self.select_stocks(criteria)
    
    def expert_committee_select(self,
                              min_expert_score: float = 75.0,
                              min_consensus: str = "åŸºæœ¬ä¸€è‡´",
                              limit: int = 30) -> SelectionResult:
        """
        ä¸“å®¶å§”å‘˜ä¼šé€‰è‚¡ - åŸºäºAIä¸“å®¶å§”å‘˜ä¼šçš„é›†ä½“å†³ç­–
        
        Args:
            min_expert_score: ä¸“å®¶å§”å‘˜ä¼šæœ€å°è¯„åˆ†
            min_consensus: æœ€å°ä¸€è‡´æ€§è¦æ±‚
            limit: ç»“æœæ•°é‡é™åˆ¶
            
        Returns:
            SelectionResult: ä¸“å®¶å§”å‘˜ä¼šé€‰è‚¡ç»“æœ
        """
        ai_config = AISelectionConfig(
            ai_mode=AIMode.EXPERT_COMMITTEE,
            expert_committee_weight=1.0,
            min_ai_score=min_expert_score,
            min_confidence=0.7,
            enable_caching=True
        )
        
        criteria = SelectionCriteria(
            filters=[],
            sort_by='expert_committee_score',
            sort_ascending=False,
            limit=limit,
            include_scores=True,
            include_basic_info=True,
            ai_mode=AIMode.EXPERT_COMMITTEE,
            ai_config=ai_config
        )
        
        return self.select_stocks(criteria)
    
    def adaptive_strategy_select(self,
                               market_data: Dict[str, Any] = None,
                               limit: int = 40) -> SelectionResult:
        """
        è‡ªé€‚åº”ç­–ç•¥é€‰è‚¡ - æ ¹æ®å¸‚åœºç¯å¢ƒè‡ªåŠ¨è°ƒæ•´é€‰è‚¡ç­–ç•¥
        
        Args:
            market_data: å¸‚åœºæ•°æ® (ç”¨äºç¯å¢ƒæ£€æµ‹)
            limit: ç»“æœæ•°é‡é™åˆ¶
            
        Returns:
            SelectionResult: è‡ªé€‚åº”ç­–ç•¥é€‰è‚¡ç»“æœ
        """
        ai_config = AISelectionConfig(
            ai_mode=AIMode.ADAPTIVE,
            adaptive_strategy_weight=1.0,
            min_ai_score=65.0,
            min_confidence=0.6,
            market_data_required=True
        )
        
        criteria = SelectionCriteria(
            filters=[],
            sort_by='adaptive_strategy_score',
            sort_ascending=False,
            limit=limit,
            include_scores=True,
            include_basic_info=True,
            ai_mode=AIMode.ADAPTIVE,
            ai_config=ai_config
        )
        
        return self.select_stocks(criteria)
    
    def pattern_based_select(self,
                           pattern_types: List[str] = None,
                           min_pattern_score: float = 70.0,
                           limit: int = 35) -> SelectionResult:
        """
        åŸºäºæ¨¡å¼è¯†åˆ«çš„é€‰è‚¡ - å¯»æ‰¾å…·æœ‰ç‰¹å®šæŠ€æœ¯æ¨¡å¼çš„è‚¡ç¥¨
        
        Args:
            pattern_types: æœŸæœ›çš„æ¨¡å¼ç±»å‹åˆ—è¡¨
            min_pattern_score: æœ€å°æ¨¡å¼è¯„åˆ†
            limit: ç»“æœæ•°é‡é™åˆ¶
            
        Returns:
            SelectionResult: æ¨¡å¼é©±åŠ¨é€‰è‚¡ç»“æœ
        """
        ai_config = AISelectionConfig(
            ai_mode=AIMode.PATTERN_BASED,
            pattern_recognition_weight=1.0,
            min_ai_score=min_pattern_score,
            min_confidence=0.65,
            pattern_analysis_enabled=True
        )
        
        criteria = SelectionCriteria(
            filters=[],
            sort_by='pattern_recognition_score',
            sort_ascending=False,
            limit=limit,
            include_scores=True,
            include_basic_info=True,
            ai_mode=AIMode.PATTERN_BASED,
            ai_config=ai_config
        )
        
        return self.select_stocks(criteria)
    
    def full_ai_select(self,
                      min_overall_score: float = 80.0,
                      min_confidence: float = 0.7,
                      risk_tolerance: str = "ä¸­ç­‰",
                      limit: int = 20) -> SelectionResult:
        """
        å®Œæ•´AIé€‰è‚¡ - ä½¿ç”¨æ‰€æœ‰AIå¼•æ“è¿›è¡Œå…¨é¢åˆ†æ
        
        Args:
            min_overall_score: æœ€å°ç»¼åˆè¯„åˆ†
            min_confidence: æœ€å°ç½®ä¿¡åº¦
            risk_tolerance: é£é™©æ‰¿å—åº¦
            limit: ç»“æœæ•°é‡é™åˆ¶
            
        Returns:
            SelectionResult: å®Œæ•´AIé€‰è‚¡ç»“æœ
        """
        ai_config = AISelectionConfig(
            ai_mode=AIMode.FULL_AI,
            min_ai_score=min_overall_score,
            min_confidence=min_confidence,
            max_risk_level=risk_tolerance,
            parallel_processing=True,
            timeout_seconds=60.0,  # å®Œæ•´AIåˆ†æéœ€è¦æ›´å¤šæ—¶é—´
            market_data_required=True,
            news_analysis_enabled=True,
            pattern_analysis_enabled=True,
            similarity_analysis_enabled=True
        )
        
        criteria = SelectionCriteria(
            filters=[],
            sort_by='ai_overall_score',
            sort_ascending=False,
            limit=limit,
            include_scores=True,
            include_basic_info=True,
            ai_mode=AIMode.FULL_AI,
            ai_config=ai_config
        )
        
        return self.select_stocks(criteria)
    
    def get_ai_performance_summary(self) -> Dict[str, Any]:
        """
        è·å–AIé€‰è‚¡å¼•æ“æ€§èƒ½æ‘˜è¦
        
        Returns:
            AIå¼•æ“æ€§èƒ½æ•°æ®
        """
        if self.ai_strategy_manager:
            return self.ai_strategy_manager.get_performance_summary()
        else:
            return {
                'status': 'AIç­–ç•¥ç®¡ç†å™¨æœªåˆå§‹åŒ–',
                'ai_enabled': False
            }
    
    def clear_ai_cache(self):
        """
        æ¸…ç†AIåˆ†æç¼“å­˜
        """
        if self.ai_strategy_manager:
            self.ai_strategy_manager.clear_cache()
            logger.info("ğŸ§¹ AIåˆ†æç¼“å­˜å·²æ¸…ç†")
        else:
            logger.warning("âš ï¸ AIç­–ç•¥ç®¡ç†å™¨æœªåˆå§‹åŒ–")

    def switch_ai_model(self, model_key: str) -> bool:
        """
        åˆ‡æ¢AIæ¨¡å‹
        
        Args:
            model_key: æ¨¡å‹é”®å€¼
            
        Returns:
            æ˜¯å¦åˆ‡æ¢æˆåŠŸ
        """
        try:
            logger.info(f"ğŸ”„ [é€‰è‚¡å¼•æ“] åˆ‡æ¢AIæ¨¡å‹: {model_key}")
            
            if self.ai_strategy_manager:
                success = self.ai_strategy_manager.switch_ai_model(model_key)
                if success:
                    logger.info(f"âœ… [é€‰è‚¡å¼•æ“] AIæ¨¡å‹åˆ‡æ¢æˆåŠŸ: {model_key}")
                    # æ¸…ç†ç¼“å­˜ä»¥ä½¿æ–°æ¨¡å‹ç”Ÿæ•ˆ
                    self.clear_ai_cache()
                    return True
                else:
                    logger.error(f"âŒ [é€‰è‚¡å¼•æ“] AIæ¨¡å‹åˆ‡æ¢å¤±è´¥: {model_key}")
                    return False
            else:
                # å¦‚æœAIç­–ç•¥ç®¡ç†å™¨æœªåˆå§‹åŒ–ï¼Œç›´æ¥è®¾ç½®å…¨å±€æ¨¡å‹
                from tradingagents.llm_adapters.dynamic_llm_manager import get_llm_manager
                llm_manager = get_llm_manager()
                success = llm_manager.set_current_model(model_key)
                if success:
                    logger.info(f"âœ… [é€‰è‚¡å¼•æ“] å…¨å±€AIæ¨¡å‹è®¾ç½®æˆåŠŸ: {model_key}")
                    return True
                else:
                    logger.error(f"âŒ [é€‰è‚¡å¼•æ“] å…¨å±€AIæ¨¡å‹è®¾ç½®å¤±è´¥: {model_key}")
                    return False
                    
        except Exception as e:
            logger.error(f"âŒ [é€‰è‚¡å¼•æ“] AIæ¨¡å‹åˆ‡æ¢å¼‚å¸¸: {e}")
            return False

    def get_available_ai_models(self) -> Dict[str, Dict[str, Any]]:
        """è·å–å¯ç”¨çš„AIæ¨¡å‹åˆ—è¡¨"""
        try:
            if self.ai_strategy_manager:
                return self.ai_strategy_manager.get_available_ai_models()
            else:
                from tradingagents.llm_adapters.dynamic_llm_manager import get_llm_manager
                llm_manager = get_llm_manager()
                return llm_manager.get_enabled_models()
        except Exception as e:
            logger.error(f"âŒ [é€‰è‚¡å¼•æ“] è·å–å¯ç”¨æ¨¡å‹å¤±è´¥: {e}")
            return {}

    def get_current_ai_model_info(self) -> Optional[Dict[str, Any]]:
        """è·å–å½“å‰AIæ¨¡å‹ä¿¡æ¯"""
        try:
            if self.ai_strategy_manager:
                return self.ai_strategy_manager.get_current_ai_model_info()
            else:
                from tradingagents.llm_adapters.dynamic_llm_manager import get_llm_manager
                llm_manager = get_llm_manager()
                current_config = llm_manager.get_current_config()
                if current_config:
                    return {
                        'provider': current_config.provider,
                        'model_name': current_config.model_name,
                        'display_name': current_config.display_name,
                        'description': current_config.description,
                        'temperature': current_config.temperature,
                        'max_tokens': current_config.max_tokens
                    }
                return None
        except Exception as e:
            logger.error(f"âŒ [é€‰è‚¡å¼•æ“] è·å–å½“å‰æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
            return None

    def longhubang_enhanced_select(self, 
                                  date: str = None,
                                  ranking_type: RankingType = RankingType.DAILY,
                                  min_longhubang_score: float = 60.0,
                                  enable_ai_analysis: bool = True,
                                  ai_mode: AIMode = AIMode.AI_ENHANCED,
                                  limit: int = 50) -> SelectionResult:
        """
        é¾™è™æ¦œå¢å¼ºé€‰è‚¡ - åŸºäºé¾™è™æ¦œæ•°æ®è¿›è¡Œè‚¡ç¥¨é€‰æ‹©å’Œåˆ†æ
        è¿™æ˜¯è§£å†³5000+è‚¡ç¥¨æ‰«ææ€§èƒ½é—®é¢˜çš„æ ¸å¿ƒè§£å†³æ–¹æ¡ˆ
        
        Args:
            date: æŸ¥è¯¢æ—¥æœŸï¼Œé»˜è®¤ä¸ºä»Šå¤©
            ranking_type: é¾™è™æ¦œç±»å‹
            min_longhubang_score: æœ€å°é¾™è™æ¦œç»¼åˆè¯„åˆ†
            enable_ai_analysis: æ˜¯å¦å¯ç”¨AIåˆ†æ
            ai_mode: AIåˆ†ææ¨¡å¼
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
            
        Returns:
            SelectionResult: é¾™è™æ¦œå¢å¼ºé€‰è‚¡ç»“æœ
        """
        start_time = time.time()
        logger.info("ğŸ‰ å¼€å§‹é¾™è™æ¦œå¢å¼ºé€‰è‚¡...")
        
        try:
            if not self.longhubang_analyzer or not self.longhubang_provider:
                logger.error("âŒ é¾™è™æ¦œåˆ†æå™¨æˆ–æ•°æ®æä¾›å™¨æœªåˆå§‹åŒ–")
                return SelectionResult(
                    symbols=[],
                    data=pd.DataFrame(),
                    summary={'error': 'é¾™è™æ¦œç»„ä»¶æœªåˆå§‹åŒ–'},
                    criteria=SelectionCriteria(),
                    execution_time=time.time() - start_time,
                    total_candidates=0,
                    filtered_count=0
                )
            
            # 1. è·å–é¾™è™æ¦œæ•°æ® (50-200åªè‚¡ç¥¨ï¼Œç›¸æ¯”5000+å¤§å¹…å‡å°‘)
            logger.info(f"ğŸ“‹ è·å–{date or 'ä»Šæ—¥'}é¾™è™æ¦œæ•°æ®...")
            longhubang_results = self.longhubang_analyzer.get_top_ranking_stocks(
                date=date,
                ranking_type=ranking_type,
                min_score=min_longhubang_score,
                limit=limit * 3  # è·å–3å€æ•°é‡ä»¥ä¾¿ç­›é€‰
            )
            
            if not longhubang_results:
                logger.warning("âš ï¸ æœªè·å–åˆ°ç¬¦åˆæ¡ä»¶çš„é¾™è™æ¦œè‚¡ç¥¨")
                return SelectionResult(
                    symbols=[],
                    data=pd.DataFrame(),
                    summary={'error': 'æœªè·å–åˆ°é¾™è™æ¦œæ•°æ®'},
                    criteria=SelectionCriteria(),
                    execution_time=time.time() - start_time,
                    total_candidates=0,
                    filtered_count=0
                )
            
            total_candidates = len(longhubang_results)
            logger.info(f"ğŸ¯ è·å–åˆ°{total_candidates}åªé¾™è™æ¦œè‚¡ç¥¨ï¼Œç›¸æ¯”å…¨å¸‚åœº5000+è‚¡ç¥¨å¤§å¹…å‡å°‘")
            
            # 2. è½¬æ¢ä¸ºDataFrameæ ¼å¼
            longhubang_data_list = []
            for result in longhubang_results:
                try:
                    # åŸºç¡€è‚¡ç¥¨ä¿¡æ¯
                    stock_info = {
                        'ts_code': result.symbol,
                        'name': result.name,
                        'current_price': result.longhubang_data.current_price,
                        'change_pct': result.longhubang_data.change_pct,
                        'turnover': result.longhubang_data.turnover,
                        'turnover_rate': result.longhubang_data.turnover_rate,
                        
                        # é¾™è™æ¦œè¯„åˆ†ä¿¡æ¯
                        'longhubang_overall_score': result.score.overall_score,
                        'longhubang_seat_quality_score': result.score.seat_quality_score,
                        'longhubang_capital_flow_score': result.score.capital_flow_score,
                        'longhubang_follow_potential_score': result.score.follow_potential_score,
                        'longhubang_risk_score': result.score.risk_score,
                        'longhubang_confidence': result.score.confidence,
                        
                        # å¸‚åœºæƒ…ç»ªå’Œæ“ä½œæ¨¡å¼
                        'market_sentiment': result.market_sentiment.value,
                        'operation_pattern': result.operation_pattern.value,
                        
                        # æŠ•èµ„å»ºè®®
                        'investment_suggestion': result.investment_suggestion,
                        'risk_warning': result.risk_warning,
                        'follow_recommendation': result.follow_recommendation,
                        
                        # å¸­ä½åˆ†ææ‘˜è¦
                        'buy_seat_count': len(result.longhubang_data.buy_seats),
                        'sell_seat_count': len(result.longhubang_data.sell_seats),
                        'net_inflow': result.longhubang_data.get_net_flow(),
                        
                        # æ•°æ®è´¨é‡
                        'data_quality': result.data_quality,
                        'analysis_timestamp': result.analysis_timestamp
                    }
                    
                    # å¸­ä½åˆ†æè¯¦æƒ…
                    if result.seat_analysis:
                        battle_analysis = result.seat_analysis.get('battle_analysis', {})
                        stock_info.update({
                            'battle_result': battle_analysis.get('battle_result', ''),
                            'battle_winner': battle_analysis.get('winner', ''),
                            'battle_confidence': battle_analysis.get('confidence', 0),
                            'buy_power': battle_analysis.get('buy_power', 0),
                            'sell_power': battle_analysis.get('sell_power', 0)
                        })
                        
                        # ååŒäº¤æ˜“æ£€æµ‹
                        coordination = result.seat_analysis.get('coordination_analysis', {})
                        stock_info.update({
                            'coordinated_trading': coordination.get('coordinated', False),
                            'coordination_confidence': coordination.get('confidence', 0),
                            'coordination_signals': '; '.join(coordination.get('signals', []))
                        })
                    
                    longhubang_data_list.append(stock_info)
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ å¤„ç†é¾™è™æ¦œæ•°æ®å¤±è´¥ {result.symbol}: {e}")
                    continue
            
            if not longhubang_data_list:
                logger.warning("âš ï¸ é¾™è™æ¦œæ•°æ®å¤„ç†åä¸ºç©º")
                return SelectionResult(
                    symbols=[],
                    data=pd.DataFrame(),
                    summary={'error': 'é¾™è™æ¦œæ•°æ®å¤„ç†å¤±è´¥'},
                    criteria=SelectionCriteria(),
                    execution_time=time.time() - start_time,
                    total_candidates=total_candidates,
                    filtered_count=0
                )
            
            # åˆ›å»ºDataFrame
            stock_data = pd.DataFrame(longhubang_data_list)
            logger.info(f"âœ… é¾™è™æ¦œæ•°æ®è½¬æ¢å®Œæˆ: {len(stock_data)}åªè‚¡ç¥¨")
            
            # 3. AIå¢å¼ºåˆ†æ (å¯é€‰ï¼Œä»…å¯¹é¾™è™æ¦œè‚¡ç¥¨è¿›è¡ŒAIåˆ†æ)
            if enable_ai_analysis and self.ai_strategy_manager:
                logger.info(f"ğŸ¤– å¯¹{len(stock_data)}åªé¾™è™æ¦œè‚¡ç¥¨è¿›è¡ŒAIå¢å¼ºåˆ†æ...")
                
                ai_config = AISelectionConfig(
                    ai_mode=ai_mode,
                    min_ai_score=50.0,  # é¾™è™æ¦œè‚¡ç¥¨å·²ç»æ˜¯é«˜è´¨é‡çš„ï¼Œå¯ä»¥æ”¾å®½AIè¯„åˆ†
                    min_confidence=0.5,
                    parallel_processing=True,
                    enable_caching=True,
                    timeout_seconds=30.0  # é¾™è™æ¦œè‚¡ç¥¨æ•°é‡å°‘ï¼Œå¯ä»¥ç»™æ¯åªè‚¡ç¥¨æ›´å¤šåˆ†ææ—¶é—´
                )
                
                stock_data = self._enrich_stock_data_with_ai(stock_data, ai_config)
                logger.info("âœ… é¾™è™æ¦œè‚¡ç¥¨AIå¢å¼ºåˆ†æå®Œæˆ")
            
            # 4. åº”ç”¨ç­›é€‰å’Œæ’åº
            # æŒ‰é¾™è™æ¦œç»¼åˆè¯„åˆ†æ’åº
            stock_data = stock_data.sort_values(
                by='longhubang_overall_score', 
                ascending=False
            )
            
            # å¦‚æœå¯ç”¨äº†AIåˆ†æï¼Œä½¿ç”¨æ™ºèƒ½ç»¼åˆè¯„åˆ†
            if enable_ai_analysis and 'ai_overall_score' in stock_data.columns:
                logger.info("ğŸ§  ä½¿ç”¨é¾™è™æ¦œ+AIç»¼åˆè¯„åˆ†æ’åº")
                
                # é¾™è™æ¦œè¯„åˆ†æƒé‡60%ï¼ŒAIè¯„åˆ†æƒé‡40%
                longhubang_weight = 0.6
                ai_weight = 0.4
                
                longhubang_scores = stock_data['longhubang_overall_score'].fillna(50)
                ai_scores = stock_data['ai_overall_score'].fillna(50)
                
                stock_data['longhubang_ai_combined_score'] = (
                    longhubang_scores * longhubang_weight + 
                    ai_scores * ai_weight
                )
                
                # æŒ‰ç»¼åˆè¯„åˆ†é‡æ–°æ’åº
                stock_data = stock_data.sort_values(
                    by='longhubang_ai_combined_score', 
                    ascending=False
                )
                logger.info(f"ğŸ“Š ä½¿ç”¨é¾™è™æ¦œ({longhubang_weight:.1f})+AI({ai_weight:.1f})ç»¼åˆè¯„åˆ†æ’åº")
            
            # 5. é™åˆ¶ç»“æœæ•°é‡
            if len(stock_data) > limit:
                stock_data = stock_data.head(limit)
                logger.info(f"âœ‚ï¸ é™åˆ¶ç»“æœæ•°é‡ä¸º: {limit}")
            
            filtered_count = len(stock_data)
            symbols = stock_data['ts_code'].tolist()
            
            # 6. ç”Ÿæˆå¢å¼ºæ‘˜è¦
            summary = self._generate_longhubang_summary(stock_data, total_candidates, filtered_count)
            
            execution_time = time.time() - start_time
            
            # åˆ›å»ºé€‰è‚¡æ ‡å‡†å¯¹è±¡
            criteria = SelectionCriteria(
                filters=[],
                sort_by='longhubang_ai_combined_score' if enable_ai_analysis else 'longhubang_overall_score',
                sort_ascending=False,
                limit=limit,
                include_scores=True,
                include_basic_info=True,
                ai_mode=ai_mode if enable_ai_analysis else AIMode.BASIC
            )
            
            result = SelectionResult(
                symbols=symbols,
                data=stock_data,
                summary=summary,
                criteria=criteria,
                execution_time=execution_time,
                total_candidates=total_candidates,
                filtered_count=filtered_count
            )
            
            logger.info(f"ğŸ‰ é¾™è™æ¦œå¢å¼ºé€‰è‚¡å®Œæˆ: {filtered_count}åªè‚¡ç¥¨, è€—æ—¶ {execution_time:.2f}ç§’")
            logger.info(f"ğŸš€ æ€§èƒ½æå‡: ç›¸æ¯”å…¨å¸‚åœº5000+è‚¡ç¥¨æ‰«æï¼Œå¤„ç†æ—¶é—´å¤§å¹…å‡å°‘")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ é¾™è™æ¦œå¢å¼ºé€‰è‚¡å¤±è´¥: {e}")
            return SelectionResult(
                symbols=[],
                data=pd.DataFrame(),
                summary={'error': str(e)},
                criteria=SelectionCriteria(),
                execution_time=time.time() - start_time,
                total_candidates=0,
                filtered_count=0
            )
    
    def _generate_longhubang_summary(self, data: pd.DataFrame, total_candidates: int, filtered_count: int) -> Dict[str, Any]:
        """ç”Ÿæˆé¾™è™æ¦œé€‰è‚¡ç»Ÿè®¡æ‘˜è¦"""
        if data.empty:
            return {
                'total_candidates': total_candidates,
                'filtered_count': 0,
                'success_rate': 0.0,
                'longhubang_statistics': {},
                'selection_type': 'longhubang_enhanced'
            }
        
        summary = {
            'total_candidates': total_candidates,
            'filtered_count': filtered_count,
            'success_rate': filtered_count / max(total_candidates, 1) * 100,
            'selection_type': 'longhubang_enhanced',
            'data_source': 'é¾™è™æ¦œ',
            'longhubang_statistics': {}
        }
        
        # é¾™è™æ¦œç‰¹æœ‰ç»Ÿè®¡
        if 'longhubang_overall_score' in data.columns:
            summary['longhubang_statistics']['average_longhubang_score'] = float(data['longhubang_overall_score'].mean())
            summary['longhubang_statistics']['max_longhubang_score'] = float(data['longhubang_overall_score'].max())
            summary['longhubang_statistics']['min_longhubang_score'] = float(data['longhubang_overall_score'].min())
        
        # å¸‚åœºæƒ…ç»ªåˆ†å¸ƒ
        if 'market_sentiment' in data.columns:
            sentiment_counts = data['market_sentiment'].value_counts()
            summary['sentiment_distribution'] = sentiment_counts.to_dict()
        
        # æ“ä½œæ¨¡å¼åˆ†å¸ƒ
        if 'operation_pattern' in data.columns:
            pattern_counts = data['operation_pattern'].value_counts()
            summary['operation_pattern_distribution'] = pattern_counts.to_dict()
        
        # å®åŠ›å¯¹æ¯”ç»Ÿè®¡
        if 'battle_winner' in data.columns:
            battle_counts = data['battle_winner'].value_counts()
            summary['battle_result_distribution'] = battle_counts.to_dict()
        
        # ååŒäº¤æ˜“ç»Ÿè®¡
        if 'coordinated_trading' in data.columns:
            coordinated_count = data['coordinated_trading'].sum()
            summary['coordinated_trading_ratio'] = coordinated_count / len(data) if len(data) > 0 else 0
        
        # èµ„é‡‘æµå‘ç»Ÿè®¡
        if 'net_inflow' in data.columns:
            net_inflow_positive = len(data[data['net_inflow'] > 0])
            summary['net_inflow_positive_ratio'] = net_inflow_positive / len(data) if len(data) > 0 else 0
            summary['average_net_inflow'] = float(data['net_inflow'].mean())
        
        # è·Ÿéšå»ºè®®åˆ†å¸ƒ
        if 'follow_recommendation' in data.columns:
            follow_counts = data['follow_recommendation'].value_counts()
            summary['follow_recommendation_distribution'] = follow_counts.to_dict()
        
        return summary
    
    def get_longhubang_statistics(self, date: str = None) -> Dict[str, Any]:
        """
        è·å–é¾™è™æ¦œå¸‚åœºç»Ÿè®¡ä¿¡æ¯
        
        Args:
            date: æŸ¥è¯¢æ—¥æœŸ
            
        Returns:
            é¾™è™æ¦œå¸‚åœºç»Ÿè®¡ä¿¡æ¯
        """
        try:
            if not self.longhubang_provider:
                return {'error': 'é¾™è™æ¦œæ•°æ®æä¾›å™¨æœªåˆå§‹åŒ–'}
            
            return self.longhubang_provider.get_statistics(date)
            
        except Exception as e:
            logger.error(f"âŒ è·å–é¾™è™æ¦œç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def search_seat_activity(self, seat_name: str, days: int = 7) -> List[Dict[str, Any]]:
        """
        æœç´¢ç‰¹å®šå¸­ä½çš„æ´»åŠ¨è®°å½•
        
        Args:
            seat_name: å¸­ä½åç§°(æ”¯æŒæ¨¡ç³ŠåŒ¹é…)
            days: æœç´¢æœ€è¿‘å‡ å¤©
            
        Returns:
            å¸­ä½æ´»åŠ¨è®°å½•åˆ—è¡¨
        """
        try:
            if not self.longhubang_provider:
                logger.error("âŒ é¾™è™æ¦œæ•°æ®æä¾›å™¨æœªåˆå§‹åŒ–")
                return []
            
            longhubang_stocks = self.longhubang_provider.search_stocks_by_seat(seat_name, days)
            
            # è½¬æ¢ä¸ºç®€åŒ–æ ¼å¼
            activity_records = []
            for stock_data in longhubang_stocks:
                record = {
                    'symbol': stock_data.symbol,
                    'name': stock_data.name,
                    'date': stock_data.date,
                    'change_pct': stock_data.change_pct,
                    'turnover': stock_data.turnover,
                    'ranking_reason': stock_data.ranking_reason,
                    'seat_found_in': 'ä¹°æ–¹' if any(seat_name in seat.seat_name for seat in stock_data.buy_seats) else 'å–æ–¹'
                }
                activity_records.append(record)
            
            logger.info(f"âœ… å¸­ä½æ´»åŠ¨æœç´¢å®Œæˆ: {seat_name}, æ‰¾åˆ°{len(activity_records)}æ¡è®°å½•")
            return activity_records
            
        except Exception as e:
            logger.error(f"âŒ æœç´¢å¸­ä½æ´»åŠ¨å¤±è´¥: {e}")
            return []
    
    def analyze_seat_influence(self, seat_name: str, days: int = 30) -> Dict[str, Any]:
        """
        åˆ†æç‰¹å®šå¸­ä½çš„å¸‚åœºå½±å“åŠ›
        
        Args:
            seat_name: å¸­ä½åç§°
            days: åˆ†ææ—¶é—´èŒƒå›´
            
        Returns:
            å¸­ä½å½±å“åŠ›åˆ†æç»“æœ
        """
        try:
            # è·å–å¸­ä½æ´»åŠ¨è®°å½•
            activity_records = self.search_seat_activity(seat_name, days)
            
            if not activity_records:
                return {
                    'seat_name': seat_name,
                    'influence_score': 0,
                    'activity_count': 0,
                    'analysis_period': days,
                    'error': 'æœªæ‰¾åˆ°å¸­ä½æ´»åŠ¨è®°å½•'
                }
            
            # è®¡ç®—å½±å“åŠ›æŒ‡æ ‡
            total_activities = len(activity_records)
            buy_activities = len([r for r in activity_records if r['seat_found_in'] == 'ä¹°æ–¹'])
            sell_activities = total_activities - buy_activities
            
            # è®¡ç®—å¹³å‡æ¶¨è·Œå¹…
            avg_change_pct = sum(r['change_pct'] for r in activity_records) / total_activities
            
            # è®¡ç®—æˆåŠŸç‡ (æ¶¨è·Œç¬¦åˆä¹°å–æ–¹å‘çš„æ¯”ä¾‹)
            successful_activities = 0
            for record in activity_records:
                if record['seat_found_in'] == 'ä¹°æ–¹' and record['change_pct'] > 0:
                    successful_activities += 1
                elif record['seat_found_in'] == 'å–æ–¹' and record['change_pct'] < 0:
                    successful_activities += 1
            
            success_rate = successful_activities / total_activities if total_activities > 0 else 0
            
            # è®¡ç®—å½±å“åŠ›è¯„åˆ†
            base_score = 50
            activity_bonus = min(20, total_activities * 2)  # æ´»åŠ¨é¢‘ç‡åŠ åˆ†
            success_bonus = success_rate * 30  # æˆåŠŸç‡åŠ åˆ†
            influence_score = base_score + activity_bonus + success_bonus
            
            analysis_result = {
                'seat_name': seat_name,
                'influence_score': round(influence_score, 2),
                'activity_count': total_activities,
                'buy_activities': buy_activities,
                'sell_activities': sell_activities,
                'success_rate': round(success_rate * 100, 2),
                'average_change_pct': round(avg_change_pct, 2),
                'analysis_period': days,
                'activity_frequency': round(total_activities / days, 2),
                'recent_activities': activity_records[:5]  # æœ€è¿‘5æ¡è®°å½•
            }
            
            logger.info(f"âœ… å¸­ä½å½±å“åŠ›åˆ†æå®Œæˆ: {seat_name}, å½±å“åŠ›è¯„åˆ†: {influence_score:.2f}")
            return analysis_result
            
        except Exception as e:
            logger.error(f"âŒ å¸­ä½å½±å“åŠ›åˆ†æå¤±è´¥: {e}")
            return {
                'seat_name': seat_name,
                'error': str(e)
            }


# å…¨å±€é€‰è‚¡å¼•æ“å®ä¾‹
_stock_selector = None

def get_stock_selector(cache_enabled: bool = True) -> StockSelector:
    """è·å–å…¨å±€é€‰è‚¡å¼•æ“å®ä¾‹"""
    global _stock_selector
    if _stock_selector is None:
        _stock_selector = StockSelector(cache_enabled=cache_enabled)
    return _stock_selector