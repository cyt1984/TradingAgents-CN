#!/usr/bin/env python3
"""
æ•°æ®ç®¡ç†CLIå·¥å…·
ç”¨äºç®¡ç†æœ¬åœ°æŒä¹…åŒ–å­˜å‚¨çš„è‚¡ç¥¨æ•°æ®
"""

import argparse
import sys
from pathlib import Path
from datetime import datetime, timedelta
import pandas as pd
from typing import List, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from tradingagents.utils.logging_manager import get_logger
from tradingagents.dataflows.persistent_storage import get_persistent_manager
from tradingagents.dataflows.historical_data_manager import get_historical_manager
from tradingagents.dataflows.stock_master_manager import get_stock_master_manager

logger = get_logger('data_manager_cli')

class DataManagerCLI:
    """æ•°æ®ç®¡ç†CLIå·¥å…·ç±»"""
    
    def __init__(self):
        self.persistent_manager = get_persistent_manager()
        self.historical_manager = get_historical_manager()
        self.stock_master_manager = get_stock_master_manager()
    
    def show_status(self):
        """æ˜¾ç¤ºæ•°æ®å­˜å‚¨çŠ¶æ€"""
        print("ğŸ“Š æ•°æ®å­˜å‚¨çŠ¶æ€æŠ¥å‘Š")
        print("=" * 50)
        
        # è‚¡ç¥¨åŸºç¡€ä¿¡æ¯çŠ¶æ€
        stock_summary = self.stock_master_manager.get_market_summary()
        print(f"ğŸ“ˆ è‚¡ç¥¨åŸºç¡€ä¿¡æ¯:")
        print(f"   æ€»è‚¡ç¥¨æ•°: {stock_summary.get('total_stocks', 0)}")
        print(f"   æœ€åæ›´æ–°: {stock_summary.get('last_updated', 'æœªçŸ¥')}")
        print(f"   å¸‚åœºåˆ†å¸ƒ: {stock_summary.get('markets', {})}")
        
        # å†å²æ•°æ®çŠ¶æ€
        for freq in ["daily", "weekly", "monthly"]:
            summary = self.historical_manager.get_data_summary(freq)
            print(f"ğŸ“Š {freq.upper()}å†å²æ•°æ®:")
            print(f"   è‚¡ç¥¨æ•°: {summary.get('total_symbols', 0)}")
            print(f"   è®°å½•æ•°: {summary.get('total_records', 0)}")
            print(f"   æœ€æ—©æ—¥æœŸ: {summary.get('earliest_date', 'æœªçŸ¥')}")
            print(f"   æœ€æ–°æ—¥æœŸ: {summary.get('latest_date', 'æœªçŸ¥')}")
        
        # å­˜å‚¨ç©ºé—´ç»Ÿè®¡
        data_dir = Path("./data/persistent")
        if data_dir.exists():
            total_size = sum(f.stat().st_size for f in data_dir.rglob("*") if f.is_file())
            print(f"ğŸ’¾ å­˜å‚¨ç©ºé—´: {total_size / 1024 / 1024:.2f} MB")
    
    def list_stocks(self, market: Optional[str] = None, limit: int = 50):
        """åˆ—å‡ºè‚¡ç¥¨ä¿¡æ¯"""
        stocks = self.stock_master_manager.load_stock_list(market)
        if stocks is None or stocks.empty:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°è‚¡ç¥¨ä¿¡æ¯")
            return
        
        print(f"ğŸ“‹ è‚¡ç¥¨åˆ—è¡¨ ({len(stocks)} åªè‚¡ç¥¨)")
        print("-" * 80)
        
        display_cols = ['symbol', 'name', 'industry', 'market', 'list_date']
        available_cols = [col for col in display_cols if col in stocks.columns]
        
        display_df = stocks[available_cols].head(limit)
        print(display_df.to_string(index=False))
    
    def search_stocks(self, keyword: str, field: str = "name"):
        """æœç´¢è‚¡ç¥¨"""
        results = self.stock_master_manager.search_stocks(keyword, field)
        if results.empty:
            print(f"ğŸ” æ²¡æœ‰æ‰¾åˆ°åŒ…å« '{keyword}' çš„è‚¡ç¥¨")
            return
        
        print(f"ğŸ” æœç´¢ç»“æœ: æ‰¾åˆ° {len(results)} åªè‚¡ç¥¨")
        print("-" * 80)
        
        display_cols = ['symbol', 'name', 'industry', 'market']
        available_cols = [col for col in display_cols if col in results.columns]
        
        print(results[available_cols].to_string(index=False))
    
    def check_data_availability(self, symbol: str, frequency: str = "daily"):
        """æ£€æŸ¥æ•°æ®å¯ç”¨æ€§"""
        availability = self.historical_manager.get_data_availability(symbol, frequency)
        
        print(f"ğŸ“ˆ {symbol} æ•°æ®å¯ç”¨æ€§ ({frequency})")
        print("-" * 30)
        
        if availability["available"]:
            print(f"âœ… æ•°æ®å¯ç”¨")
            print(f"   æ—¥æœŸèŒƒå›´: {availability['start_date']} è‡³ {availability['end_date']}")
            print(f"   è®°å½•æ•°é‡: {availability['record_count']}")
            print(f"   æœ€åæ›´æ–°: {availability['last_updated']}")
        else:
            print("âŒ æ•°æ®ä¸å¯ç”¨")
    
    def export_stock_list(self, market: Optional[str] = None, 
                         format: str = "csv", output_path: Optional[str] = None):
        """å¯¼å‡ºè‚¡ç¥¨åˆ—è¡¨"""
        output_file = self.stock_master_manager.export_stock_list(
            market=market, format=format, output_path=output_path
        )
        
        if output_file:
            print(f"âœ… è‚¡ç¥¨åˆ—è¡¨å·²å¯¼å‡ºåˆ°: {output_file}")
        else:
            print("âŒ å¯¼å‡ºå¤±è´¥")
    
    def validate_data(self, symbol: str, frequency: str = "daily"):
        """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
        result = self.historical_manager.verify_data_integrity(symbol, frequency)
        
        print(f"ğŸ” {symbol} æ•°æ®éªŒè¯æŠ¥å‘Š ({frequency})")
        print("-" * 30)
        print(f"éªŒè¯ç»“æœ: {result['integrity_check']}")
        
        if result['issues']:
            print("å‘ç°çš„é—®é¢˜:")
            for issue in result['issues']:
                print(f"  - {issue}")
        else:
            print("âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡")
    
    def cleanup_old_data(self, days: int = 30):
        """æ¸…ç†æ—§æ•°æ®"""
        cutoff_date = datetime.now() - timedelta(days=days)
        
        print(f"ğŸ§¹ æ¸…ç† {days} å¤©å‰çš„æ•°æ®...")
        
        # æ¸…ç†å…ƒæ•°æ®æ–‡ä»¶
        metadata_files = list(Path("./data/persistent").rglob("*_metadata.json"))
        cleaned_count = 0
        
        for metadata_file in metadata_files:
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = pd.json.loads(f.read())
                    
                updated_at = pd.to_datetime(metadata.get('updated_at', ''))
                if updated_at < cutoff_date:
                    # åˆ é™¤å¯¹åº”çš„parquetæ–‡ä»¶
                    parquet_file = metadata_file.with_suffix('.parquet')
                    if parquet_file.exists():
                        parquet_file.unlink()
                    metadata_file.unlink()
                    cleaned_count += 1
                    
            except Exception as e:
                logger.warning(f"æ¸…ç†å¤±è´¥: {metadata_file} - {e}")
        
        print(f"âœ… æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {cleaned_count} ä¸ªæ—§æ•°æ®æ–‡ä»¶")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="TradingAgents æ•°æ®ç®¡ç†å·¥å…·")
    
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # çŠ¶æ€å‘½ä»¤
    status_parser = subparsers.add_parser('status', help='æ˜¾ç¤ºæ•°æ®å­˜å‚¨çŠ¶æ€')
    
    # åˆ—è¡¨å‘½ä»¤
    list_parser = subparsers.add_parser('list', help='åˆ—å‡ºè‚¡ç¥¨ä¿¡æ¯')
    list_parser.add_argument('--market', help='å¸‚åœºç±»å‹ (Aè‚¡/ç¾è‚¡/æ¸¯è‚¡)')
    list_parser.add_argument('--limit', type=int, default=50, help='æ˜¾ç¤ºæ•°é‡é™åˆ¶')
    
    # æœç´¢å‘½ä»¤
    search_parser = subparsers.add_parser('search', help='æœç´¢è‚¡ç¥¨')
    search_parser.add_argument('keyword', help='æœç´¢å…³é”®è¯')
    search_parser.add_argument('--field', default='name', 
                              choices=['name', 'symbol', 'industry'], 
                              help='æœç´¢å­—æ®µ')
    
    # æ£€æŸ¥å‘½ä»¤
    check_parser = subparsers.add_parser('check', help='æ£€æŸ¥æ•°æ®å¯ç”¨æ€§')
    check_parser.add_argument('symbol', help='è‚¡ç¥¨ä»£ç ')
    check_parser.add_argument('--frequency', default='daily', 
                             choices=['daily', 'weekly', 'monthly'],
                             help='æ•°æ®é¢‘ç‡')
    
    # å¯¼å‡ºå‘½ä»¤
    export_parser = subparsers.add_parser('export', help='å¯¼å‡ºè‚¡ç¥¨åˆ—è¡¨')
    export_parser.add_argument('--market', help='å¸‚åœºç±»å‹')
    export_parser.add_argument('--format', default='csv', 
                              choices=['csv', 'xlsx', 'json'],
                              help='å¯¼å‡ºæ ¼å¼')
    export_parser.add_argument('--output', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    
    # éªŒè¯å‘½ä»¤
    validate_parser = subparsers.add_parser('validate', help='éªŒè¯æ•°æ®å®Œæ•´æ€§')
    validate_parser.add_argument('symbol', help='è‚¡ç¥¨ä»£ç ')
    validate_parser.add_argument('--frequency', default='daily',
                                choices=['daily', 'weekly', 'monthly'],
                                help='æ•°æ®é¢‘ç‡')
    
    # æ¸…ç†å‘½ä»¤
    cleanup_parser = subparsers.add_parser('cleanup', help='æ¸…ç†æ—§æ•°æ®')
    cleanup_parser.add_argument('--days', type=int, default=30,
                               help='æ¸…ç†å¤šå°‘å¤©å‰çš„æ•°æ®')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    try:
        cli = DataManagerCLI()
        
        if args.command == 'status':
            cli.show_status()
        elif args.command == 'list':
            cli.list_stocks(args.market, args.limit)
        elif args.command == 'search':
            cli.search_stocks(args.keyword, args.field)
        elif args.command == 'check':
            cli.check_data_availability(args.symbol, args.frequency)
        elif args.command == 'export':
            cli.export_stock_list(args.market, args.format, args.output)
        elif args.command == 'validate':
            cli.validate_data(args.symbol, args.frequency)
        elif args.command == 'cleanup':
            cli.cleanup_old_data(args.days)
            
    except Exception as e:
        logger.error(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")

if __name__ == "__main__":
    main()