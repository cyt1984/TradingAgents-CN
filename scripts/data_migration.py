#!/usr/bin/env python3
"""
æ•°æ®è¿ç§»å·¥å…·
å°†ç°æœ‰çš„ç¼“å­˜æ•°æ®è¿ç§»åˆ°æ–°çš„æŒä¹…åŒ–å­˜å‚¨ç³»ç»Ÿ
"""

import os
import sys
import json
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any
import sqlite3
import argparse

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from tradingagents.utils.logging_manager import get_logger
from tradingagents.dataflows.persistent_storage import get_persistent_manager
from tradingagents.dataflows.historical_data_manager import get_historical_manager
from tradingagents.dataflows.stock_master_manager import get_stock_master_manager
from tradingagents.dataflows.enhanced_data_manager import EnhancedDataManager

logger = get_logger('data_migration')


class DataMigrationTool:
    """æ•°æ®è¿ç§»å·¥å…·ç±»"""
    
    def __init__(self):
        self.persistent_manager = get_persistent_manager()
        self.historical_manager = get_historical_manager()
        self.stock_master_manager = get_stock_master_manager()
        self.enhanced_data_manager = EnhancedDataManager()
        
        # ç¼“å­˜ç›®å½•è·¯å¾„
        self.cache_dirs = {
            'data_cache': project_root / 'tradingagents' / 'dataflows' / 'data_cache',
            'cache': project_root / 'cache',
            'redis_cache': project_root / 'data' / 'cache'
        }
        
    def discover_cache_files(self) -> Dict[str, List[Path]]:
        """å‘ç°æ‰€æœ‰ç¼“å­˜æ–‡ä»¶"""
        cache_files = {}
        
        for cache_name, cache_dir in self.cache_dirs.items():
            if cache_dir.exists():
                files = list(cache_dir.rglob("*.csv")) + \
                       list(cache_dir.rglob("*.json")) + \
                       list(cache_dir.rglob("*.pkl")) + \
                       list(cache_dir.rglob("*.parquet"))
                
                cache_files[cache_name] = files
                logger.info(f"ğŸ“ å‘ç° {cache_name}: {len(files)} ä¸ªæ–‡ä»¶")
        
        return cache_files
    
    def extract_symbols_from_filename(self, filename: str) -> List[str]:
        """ä»æ–‡ä»¶åä¸­æå–è‚¡ç¥¨ä»£ç """
        symbols = []
        
        # å¸¸è§çš„è‚¡ç¥¨ä»£ç æ¨¡å¼
        patterns = [
            r'(\d{6})',  # 6ä½æ•°å­— (Aè‚¡)
            r'(\d{4}\.HK)',  # æ¸¯è‚¡
            r'(\w+)-',  # ç¾è‚¡æ¨¡å¼
            r'([A-Z]{1,5})',  # ç¾è‚¡ä»£ç 
        ]
        
        import re
        
        # å°è¯•åŒ¹é…6ä½æ•°å­—
        matches = re.findall(r'\d{6}', filename)
        symbols.extend(matches)
        
        # å°è¯•åŒ¹é…æ¸¯è‚¡
        matches = re.findall(r'\d{4}\.HK', filename)
        symbols.extend(matches)
        
        # å°è¯•åŒ¹é…ç¾è‚¡
        matches = re.findall(r'([A-Z]{1,5})[^a-zA-Z0-9]', filename.upper())
        symbols.extend(matches)
        
        return list(set(symbols))  # å»é‡
    
    def migrate_stock_list(self) -> int:
        """è¿ç§»è‚¡ç¥¨åˆ—è¡¨æ•°æ®"""
        logger.info("ğŸ”„ å¼€å§‹è¿ç§»è‚¡ç¥¨åˆ—è¡¨æ•°æ®...")
        
        migrated_count = 0
        cache_files = self.discover_cache_files()
        
        stock_data = []
        
        # ä»å„ä¸ªç¼“å­˜ç›®å½•æ”¶é›†è‚¡ç¥¨ä¿¡æ¯
        for cache_name, files in cache_files.items():
            for file_path in files:
                try:
                    symbols = self.extract_symbols_from_filename(file_path.name)
                    
                    for symbol in symbols:
                        # è·å–è‚¡ç¥¨è¯¦ç»†ä¿¡æ¯
                        stock_info = self._get_stock_info_from_cache(symbol, file_path)
                        if stock_info:
                            stock_data.append(stock_info)
                        else:
                            # åˆ›å»ºåŸºç¡€ä¿¡æ¯
                            stock_data.append({
                                'symbol': symbol,
                                'name': f'è‚¡ç¥¨{symbol}',
                                'market': self._guess_market(symbol),
                                'source': 'migration'
                            })
                
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {file_path} - {e}")
        
        if stock_data:
            # å»é‡
            unique_stocks = {}
            for stock in stock_data:
                symbol = stock['symbol']
                if symbol not in unique_stocks:
                    unique_stocks[symbol] = stock
            
            stock_list = list(unique_stocks.values())
            self.stock_master_manager.save_stock_list(stock_list)
            migrated_count = len(stock_list)
            logger.info(f"âœ… è¿ç§»è‚¡ç¥¨åˆ—è¡¨å®Œæˆ: {migrated_count} åªè‚¡ç¥¨")
        
        return migrated_count
    
    def _get_stock_info_from_cache(self, symbol: str, file_path: Path) -> Optional[Dict[str, Any]]:
        """ä»ç¼“å­˜æ–‡ä»¶ä¸­æå–è‚¡ç¥¨ä¿¡æ¯"""
        try:
            if file_path.suffix == '.csv':
                df = pd.read_csv(file_path, nrows=1)
                if 'name' in df.columns:
                    return {
                        'symbol': symbol,
                        'name': df['name'].iloc[0] if not pd.isna(df['name'].iloc[0]) else f'è‚¡ç¥¨{symbol}'
                    }
            elif file_path.suffix == '.json':
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list) and data:
                        return data[0]
        except Exception:
            pass
        
        return None
    
    def _guess_market(self, symbol: str) -> str:
        """çŒœæµ‹è‚¡ç¥¨æ‰€å±å¸‚åœº"""
        if symbol.isdigit() and len(symbol) == 6:
            if symbol.startswith(('6', '5')):
                return 'Aè‚¡'
            elif symbol.startswith(('0', '3', '2')):
                return 'Aè‚¡'
        elif symbol.endswith('.HK'):
            return 'æ¸¯è‚¡'
        elif symbol.isalpha() and len(symbol) <= 5:
            return 'ç¾è‚¡'
        
        return 'æœªçŸ¥'
    
    def migrate_historical_data(self) -> int:
        """è¿ç§»å†å²ä»·æ ¼æ•°æ®"""
        logger.info("ğŸ”„ å¼€å§‹è¿ç§»å†å²ä»·æ ¼æ•°æ®...")
        
        migrated_count = 0
        cache_files = self.discover_cache_files()
        
        for cache_name, files in cache_files.items():
            for file_path in files:
                try:
                    # æå–è‚¡ç¥¨ä»£ç 
                    symbols = self.extract_symbols_from_filename(file_path.name)
                    
                    for symbol in symbols:
                        # å°è¯•åŠ è½½ä»·æ ¼æ•°æ®
                        price_data = self._load_price_data(file_path, symbol)
                        if price_data is not None and not price_data.empty:
                            # ç¡®å®šæ•°æ®é¢‘ç‡
                            frequency = self._detect_frequency(price_data)
                            
                            # ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨
                            self.historical_manager.save_historical_data(
                                symbol, price_data, frequency
                            )
                            migrated_count += 1
                            logger.info(f"âœ… è¿ç§»å†å²æ•°æ®: {symbol} ({frequency})")
                
                except Exception as e:
                    logger.error(f"âŒ è¿ç§»å†å²æ•°æ®å¤±è´¥: {file_path} - {e}")
        
        logger.info(f"âœ… è¿ç§»å†å²æ•°æ®å®Œæˆ: {migrated_count} ä¸ªæ–‡ä»¶")
        return migrated_count
    
    def _load_price_data(self, file_path: Path, symbol: str) -> Optional[pd.DataFrame]:
        """åŠ è½½ä»·æ ¼æ•°æ®"""
        try:
            if file_path.suffix == '.csv':
                df = pd.read_csv(file_path)
                if 'date' in df.columns or 'Date' in df.columns:
                    return df
            elif file_path.suffix == '.json':
                df = pd.read_json(file_path)
                if 'date' in df.columns or 'Date' in df.columns:
                    return df
            elif file_path.suffix == '.parquet':
                df = pd.read_parquet(file_path)
                if 'date' in df.columns or 'Date' in df.columns:
                    return df
        except Exception as e:
            logger.debug(f"æ— æ³•åŠ è½½ä»·æ ¼æ•°æ®: {file_path} - {e}")
        
        return None
    
    def _detect_frequency(self, df: pd.DataFrame) -> str:
        """æ£€æµ‹æ•°æ®é¢‘ç‡"""
        try:
            date_col = 'date' if 'date' in df.columns else 'Date'
            df[date_col] = pd.to_datetime(df[date_col])
            df = df.sort_values(date_col)
            
            # è®¡ç®—æ—¥æœŸé—´éš”
            intervals = df[date_col].diff().dt.days.dropna()
            avg_interval = intervals.mean()
            
            if avg_interval <= 2:
                return 'daily'
            elif avg_interval <= 8:
                return 'weekly'
            else:
                return 'monthly'
        except Exception:
            return 'daily'
    
    def migrate_financial_data(self) -> int:
        """è¿ç§»è´¢åŠ¡æ•°æ®"""
        logger.info("ğŸ”„ å¼€å§‹è¿ç§»è´¢åŠ¡æ•°æ®...")
        
        migrated_count = 0
        cache_files = self.discover_cache_files()
        
        for cache_name, files in cache_files.items():
            for file_path in files:
                if 'financial' in str(file_path).lower() or 'fundamental' in str(file_path).lower():
                    try:
                        symbols = self.extract_symbols_from_filename(file_path.name)
                        
                        for symbol in symbols:
                            financial_data = self._load_financial_data(file_path)
                            if financial_data is not None and not financial_data.empty:
                                # ç¡®å®šæŠ¥å‘Šç±»å‹
                                report_type = 'quarterly' if 'quarter' in str(file_path).lower() else 'annual'
                                
                                self.persistent_manager.save_financial_data(
                                    symbol, financial_data, report_type
                                )
                                migrated_count += 1
                                logger.info(f"âœ… è¿ç§»è´¢åŠ¡æ•°æ®: {symbol} ({report_type})")
                    
                    except Exception as e:
                        logger.error(f"âŒ è¿ç§»è´¢åŠ¡æ•°æ®å¤±è´¥: {file_path} - {e}")
        
        logger.info(f"âœ… è¿ç§»è´¢åŠ¡æ•°æ®å®Œæˆ: {migrated_count} ä¸ªæ–‡ä»¶")
        return migrated_count
    
    def _load_financial_data(self, file_path: Path) -> Optional[pd.DataFrame]:
        """åŠ è½½è´¢åŠ¡æ•°æ®"""
        try:
            if file_path.suffix == '.csv':
                return pd.read_csv(file_path)
            elif file_path.suffix == '.json':
                return pd.read_json(file_path)
            elif file_path.suffix == '.parquet':
                return pd.read_parquet(file_path)
        except Exception as e:
            logger.debug(f"æ— æ³•åŠ è½½è´¢åŠ¡æ•°æ®: {file_path} - {e}")
        
        return None
    
    def generate_migration_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆè¿ç§»æŠ¥å‘Š"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'source_directories': {},
            'migration_summary': {},
            'errors': []
        }
        
        # ç»Ÿè®¡æºç›®å½•
        cache_files = self.discover_cache_files()
        for cache_name, files in cache_files.items():
            total_size = sum(f.stat().st_size for f in files if f.exists())
            report['source_directories'][cache_name] = {
                'file_count': len(files),
                'total_size_mb': round(total_size / 1024 / 1024, 2)
            }
        
        # ç»Ÿè®¡è¿ç§»åçš„æ•°æ®
        report['migration_summary'] = {
            'stock_list_count': self.stock_master_manager.get_market_summary().get('total_stocks', 0),
            'historical_daily': self.historical_manager.get_data_summary('daily')['total_symbols'],
            'historical_weekly': self.historical_manager.get_data_summary('weekly')['total_symbols'],
            'historical_monthly': self.historical_manager.get_data_summary('monthly')['total_symbols']
        }
        
        return report
    
    def run_full_migration(self) -> Dict[str, int]:
        """è¿è¡Œå®Œæ•´è¿ç§»"""
        logger.info("ğŸš€ å¼€å§‹å®Œæ•´æ•°æ®è¿ç§»...")
        
        results = {
            'stock_list': 0,
            'historical': 0,
            'financial': 0,
            'errors': 0
        }
        
        try:
            results['stock_list'] = self.migrate_stock_list()
            results['historical'] = self.migrate_historical_data()
            results['financial'] = self.migrate_financial_data()
            
            # ç”ŸæˆæŠ¥å‘Š
            report = self.generate_migration_report()
            report_path = Path("./data/persistent") / f"migration_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            report_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… è¿ç§»å®Œæˆï¼ŒæŠ¥å‘Šå·²ä¿å­˜: {report_path}")
            
        except Exception as e:
            logger.error(f"âŒ è¿ç§»å¤±è´¥: {e}")
            results['errors'] += 1
        
        return results


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="TradingAgents æ•°æ®è¿ç§»å·¥å…·")
    parser.add_argument('--type', choices=['full', 'stocks', 'historical', 'financial'], 
                       default='full', help='è¿ç§»ç±»å‹')
    parser.add_argument('--source', default='./cache', help='æºç›®å½•')
    parser.add_argument('--report', action='store_true', help='åªç”ŸæˆæŠ¥å‘Š')
    
    args = parser.parse_args()
    
    try:
        migration_tool = DataMigrationTool()
        
        if args.report:
            report = migration_tool.generate_migration_report()
            print(json.dumps(report, ensure_ascii=False, indent=2))
            return
        
        if args.type == 'full':
            results = migration_tool.run_full_migration()
        elif args.type == 'stocks':
            results = {'stock_list': migration_tool.migrate_stock_list()}
        elif args.type == 'historical':
            results = {'historical': migration_tool.migrate_historical_data()}
        elif args.type == 'financial':
            results = {'financial': migration_tool.migrate_financial_data()}
        
        print("ğŸ‰ è¿ç§»ç»“æœ:")
        for key, value in results.items():
            print(f"  {key}: {value}")
    
    except KeyboardInterrupt:
        print("\nğŸ›‘ è¿ç§»è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")


if __name__ == "__main__":
    main()